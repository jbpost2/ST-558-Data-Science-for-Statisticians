---
title: "Modeling Concepts: Inference vs Prediction"
author: "Justin Post"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "css/ncsu.css", "css/ncsu-fonts.css", "css/mycss.css"]
    nature:
      beforeInit: ["js/ncsu-scale.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "partials/header.html"
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
library(plotly)
library(DT)
library(tidyverse)
options(dplyr.print_min = 5)
#opts_chunk$set(echo = FALSE)
```

layout: false

# What do we want to be able to do?

Data Science!

- Read in raw data and manipulate it
- Combine data sources
- Summarize data to glean insights
- Apply common analysis methods
- Communicate Effectively


---

# Modeling Ideas

What is a (statistical) model?

- A mathematical representation of some phenomenon on which you've observed data
- Form of the model can vary greatly!


---

# Modeling (log) Selling Price

- First a visual on motorcycle sales data

```{r, out.width='350px', fig.align='center', message = FALSE, warning = FALSE}
bike_data <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv")
bike_data <- bike_data |> 
  mutate(log_selling_price = log(selling_price), 
         log_km_driven = log(km_driven)) |>
  select(log_km_driven, log_selling_price, everything())
bike_data
```


---

# Modeling (log) Selling Price

- First a visual on motorcycle sales data

```{r, out.width='350px', fig.align='center'}
ggplot(bike_data, aes(x = log_km_driven, y = log_selling_price)) +
  geom_point() +
  geom_smooth(method = "lm")
```


---

# Modeling (log) Selling Price

- First a visual on motorcycle sales data

```{r, out.width='350px', fig.align='center'}
ggplot(bike_data, aes(x = log_km_driven, y = log_selling_price)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2))
```

---

# Modeling (log) Selling Price

- First a visual on motorcycle sales data

```{r, out.width='350px', fig.align='center'}
ggplot(bike_data, aes(x = log_km_driven, y = log_selling_price)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2) + I(x^3))
```


---

# Modeling (log) Selling Price

- First a visual on motorcycle sales data

```{r, out.width='350px', fig.align='center'}
ggplot(bike_data, aes(x = log_km_driven, y = log_selling_price)) +
  geom_point() +
  geom_smooth()
```


---

# Modeling (log) Selling Price

```{r, echo = FALSE}
library(tree)
tree_fit <- tree(log_selling_price ~ log_km_driven, data = bike_data)
preds <- predict(tree_fit, newdata  = data.frame(log_km_driven = seq(from = min(bike_data$log_km_driven), to = max(bike_data$log_km_driven), length = 500))) |> 
  as_tibble() |>
  rename("Tree_Predictions" = value) |>
  mutate(log_km_driven = seq(from = min(bike_data$log_km_driven), to = max(bike_data$log_km_driven), length = 500))
```

```{r, out.width='350px', fig.align='center'}
ggplot(bike_data, aes(x = log_km_driven, y = log_selling_price)) +
  geom_point() + 
  geom_line(data = preds, aes(x = log_km_driven, y = Tree_Predictions), color = "Blue", linewidth = 2)
```



---

# Modeling Ideas

What is a (statistical) model?

- A mathematical representation of some phenomenon on which you've observed data
- Form of the model can vary greatly!

**Statistical learning** - Inference, prediction/classification, and pattern finding

- Supervised learning - a variable (or variables) represents an **output** or **response** of interest

--

    + May model response and
        - Make **inference** on the model parameters  
        - **predict** a value or **classify** an observation

Our Goal: Understand what it means to be a good predictive model (not make inference)



---

# Training a Model

- Once a class of models is chosen, we must define some criteria to **fit** (or train) the model

**Simple Linear Regression (SLR) Model**

$$E(Y_i|x_i) = \beta_0+\beta_1x_i$$

---

# Training a Model

- Once a class of models is chosen, we must define some criteria to **fit** (or train) the model

**Simple Linear Regression (SLR) Model**

$$E(Y_i|x_i) = \beta_0+\beta_1x_i$$

- **Loss function** - Criteria used to fit or train a model

    - For a given **numeric** response value, $y_i$ and prediction, $\hat{y}_i$
    
    $$y_i - \hat{y}_i, (y_i-\hat{y}_i)^2, |y_i-\hat{y}_i|$$


---

# Training a Model

- Once a class of models is chosen, we must define some criteria to **fit** (or train) the model

**Simple Linear Regression (SLR) Model**

$$E(Y_i|x_i) = \beta_0+\beta_1x_i$$

- **Loss function** - Criteria used to fit or train a model

    - For a given **numeric** response value, $y_i$ and prediction, $\hat{y}_i$
    
    $$y_i - \hat{y}_i, (y_i-\hat{y}_i)^2, |y_i-\hat{y}_i|$$


- We try to optimize the loss over all the observations used for training

$$\sum_{i=1}^{n} (y_i-\hat{y}_i)^2~~~~~~~~~~~~~~~~~~~~ \sum_{i=1}^{n} |y_i-\hat{y}_i|$$



---

# Training (Fitting) the SLR Model

- Often use squared error loss (least squares regression)

- Nice solutions for our estimates exist!

$$\hat{\beta}_0 = \bar{y}-\bar{x}\hat{\beta}_1$$
$$\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$



---

# Training (Fitting) the SLR Model

- Often use squared error loss (least squares regression)

- Nice solutions for our estimates exist!

$$\hat{\beta}_0 = \bar{y}-\bar{x}\hat{\beta}_1$$
$$\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$


```{r}
y <- bike_data$log_selling_price
x <- bike_data$log_km_driven
b1_hat <- sum((x-mean(x))*(y-mean(y)))/sum((x-mean(x))^2)
b0_hat <- mean(y)-mean(x)*b1_hat
c(round(b0_hat, 4), round(b1_hat, 4))
```

- Now we can find a prediction! Denoted as $\hat{y}$


---

# Training (Fitting) the SLR Model in R

- Use `lm()` function to fit in R

- Utilizes `formula` notation: `y ~ x` -> `response ~ model terms`

```{r}
slr_fit <- lm(log_selling_price ~ log_km_driven, data = bike_data)
slr_fit
```


---

# Inference Using the SLR Model in R

- If we assume iid errors that are Normally distributed with the same variance, we can conduct inference!

    - Confidence intervals and hypothesis tests around the slope parameter
    
    - Use `summary()` (generic function!) on our fitted model

```{r}
summary(slr_fit)
```


---

# Inference Using the SLR Model in R

- If we assume iid errors that are Normally distributed with the same variance, we can conduct inference!

    - Can use `anova()` to get Analysis of Variance information

```{r}
anova(slr_fit)
```

---

# Inference Using the SLR Model in R

- If we assume iid errors that are Normally distributed with the same variance, we can conduct inference!

    - Residual diagnostics can be found via `plot()` on the fitted model

```{r, echo = FALSE, out.width='350px', fig.align='center'}
plot(slr_fit, which = 1, ask = FALSE)
```

---

# Inference Using the SLR Model in R

- If we assume iid errors that are Normally distributed with the same variance, we can conduct inference!

    - Residual diagnostics can be found via `plot()` on the fitted model

```{r, echo = FALSE, out.width='350px', fig.align='center'}
plot(slr_fit, which = 2, ask = FALSE)
```


---

# Prediction Using the SLR Model in R

- Can use the line for prediction with `predict()`!

    + Another generic function in R

```{r}
predict
predict.lm
```

---

# Prediction Using the SLR Model in R

- Can use the line for prediction with `predict()`!

    - Should supply fitted object and `newdata`
    
        - An optional data frame in which to look for variables with which to predict. If omitted, the fitted values are used.

```{r}
predict(slr_fit, newdata = data.frame(log_km_driven = c(log(1000), log(10000), log(100000))))
```

```{r}
exp(predict(slr_fit, newdata = data.frame(log_km_driven = c(log(1000), log(10000), log(100000)))))
```
---

# Quantifying How Well the Model Predicts

We use a **loss** function to fit the model. We use a **metric** to evaluate the model!

- Often use the same loss function for fitting and as the metric
- For a given **numeric** response value, $y_i$ and prediction, $\hat{y}_i$
$$(y_i-\hat{y}_i)^2, |y_i-\hat{y}_i|$$
- Incorporate all points via
$$\frac{1}{n}\sum_{i=1}^{n} (y_i-\hat{y}_i)^2, \frac{1}{n}\sum_{i=1}^{n} |y_i-\hat{y}_i|$$

---

# Metric Function

- For a numeric response, we commonly use squared error loss as our metric to evaluate a prediction
$$L(y_i,\hat{y}_i) = (y_i-\hat{y}_i)^2$$

- Use Root Mean Square Error as a **metric** across all observations
$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n} L(y_i, \hat{y}_i)} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2}$$


---

# Commonly Used Metrics

For prediction (numeric response)
- Mean Squared Error (MSE) or Root Mean Squared Error (RMSE)
- Mean Absolute Error (MAE or MAD - deviation)
$$L(y_i,\hat{y}_i) = |y_i-\hat{y}_i|$$
- [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss)
    + Doesn't penalize large mistakes as much as MSE



---

# Commonly Used Metrics

For prediction (numeric response)
- Mean Squared Error (MSE) or Root Mean Squared Error (RMSE)
- Mean Absolute Error (MAE or MAD - deviation)
$$L(y_i,\hat{y}_i) = |y_i-\hat{y}_i|$$
- [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss)
    + Doesn't penalize large mistakes as much as MSE

For classification (categorical response)
- Accuracy
- log-loss
- AUC
- F1 Score


---

# Evaluating our SLR Model

- We could find our metric for our SLR model using the training data

    + Called **training error**

```{r}
head(predict(slr_fit))
mean((bike_data$log_selling_price-predict(slr_fit))^2)
sqrt(mean((bike_data$log_selling_price-predict(slr_fit))^2))
```

- Doesn't tell us how well we do on data we haven't seen!


---

# Training vs Test Sets

Ideally we want our model to predict well for observations **it has yet to see**!

- For *multiple* linear regression models, our training MSE will always decrease as we add more variables to the model...

- We'll need an independent **test** set to predict on (more on this shortly!)


---

# Big Picture Modeling

Supervised Learning methods try to relate predictors to a response variable through a model

- Lots of common models

    - Regression models
    - Tree based methods
    - Naive Bayes
    - k Nearest Neighbors
    - ...

- For a set of predictor values, each will produce some prediction we can call $\hat{y}$ 

- Evaluate model via a metric

- Will use an independent test set or cross-validation to more accurately judge our model
