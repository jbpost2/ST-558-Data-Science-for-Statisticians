---
title: "Multiple Linear Regression Models"
author: "Justin Post"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "css/ncsu.css", "css/ncsu-fonts.css", "css/mycss.css"]
    nature:
      beforeInit: ["js/ncsu-scale.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "partials/header.html"
    self_contained: yes
editor_options: 
  chunk_output_type: console
---
  
```{r, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  fig.align = "center",
#fig.width = 11,
#fig.height = 5
  cache = FALSE
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(tidyverse)
library(knitr)
#library(reticulate)
#use_python("C:\\Users\\jbpost2\\AppData\\Local\\Programs\\Python\\Python310\\python.exe")
#use_python("C:\\python\\python.exe")
#use_python("C:\\ProgramData\\Anaconda3\\python.exe")
options(dplyr.print_min = 5)
#options(reticulate.repl.quiet = TRUE)
```


# Recap 

Given a model, we **fit** the model using data

- Must determine how well the model predicts on **new** data 
- Create a test set or use CV
- Judge effectiveness using a **metric** on predictions made from the model


---
  
# Regression Modeling Ideas
  
For a set of observations $y_1,...,y_n$, we may want to predict a future value

- Often use the sample mean to do so, $\bar{y}$ (an estimate of $E(Y)$)


---
  
# Regression Modeling Ideas
  
  For a set of observations $y_1,...,y_n$, we may want to predict a future value

- Often use the sample mean to do so, $\bar{y}$ (an estimate of $E(Y)$)

Now consider having pairs $(x_1,y_1), (x_2,y_2),...(x_n,y_n)$
  
```{r, echo = FALSE, out.width = "370px", fig.align='center'}
set.seed(1)
x <- seq(from = -11, to = 17, by = 0.1)
y <- 300 - x^2 -20*x + 1/10*x^3
yerr <- rep(y, 5) + rnorm(length(y)*5, mean = 3, sd = 100)
plot(x, y, type = "l", lwd = 5, col = "blue", ylim = c(-250, 600), main = "Below: Blue line, f(x), is the 'true' relationship between x and y")
lines(rep(x, 5), yerr, type = "p", cex = 0.4)
```

---
  
# Regression Modeling Ideas
  
Often use a linear (in the parameters) model for prediction

$$\mbox{SLR model: }E(Y|x) = \beta_0+\beta_1x$$
  
```{r, echo = FALSE, out.width = "370px", fig.align='center'}
dat <- data.frame(y,x)
plot(x, y, type = "l", lwd = 5, col = "blue", ylim = c(-250, 600), main = "Below: Blue line, f(x), is the 'true' relationship between x and y")
lines(rep(x, 5), yerr, type = "p", cex = 0.4)
fit <- lm(y~x, data = dat)
lines(y = predict(fit, dat[,"x", drop = FALSE]), x = dat$x, lwd = 3)
legend("topright", legend = c("SLR fit"), col = c("black"), lty = 1, lwd = 3)
```



---
  
# Regression Modeling Ideas
  
Can include more terms on the right hand side (RHS)

$$\mbox{Multiple Linear Regression Model: }E(Y|x) = \beta_0+\beta_1x+\beta_2x^2$$
  
```{r, echo = FALSE, out.width = "370px", fig.align='center'}
plot(x, y, type = "l", lwd = 5, col = "blue", ylim = c(-250, 600), main = "Below: Blue line, f(x), is the 'true' relationship between x and y")
lines(rep(x, 5), yerr, type = "p", cex = 0.4)
fit <- lm(y~x, data = dat)
lines(y = predict(fit, dat[,"x", drop = FALSE]), x = dat$x, lwd = 3)
fit2 <- lm(y~x+I(x^2), data = dat)
lines(y = predict(fit2, dat[,"x", drop = FALSE]), x = dat$x, lwd = 3, col = 'Green')
legend("topright", legend = c("SLR fit", "Quad fit"), col = c("black", "Green"), lty = 1, lwd = 3)
```


---
  
# Regression Modeling Ideas
  
Can include more terms on the right hand side (RHS)

$$\mbox{Multiple Linear Regression Model: }E(Y|x) = \beta_0+\beta_1x+\beta_2x^2+\beta_3x^3$$
  
```{r, echo = FALSE, out.width = "370px", fig.align='center'}
plot(x, y, type = "l", lwd = 5, col = "blue", ylim = c(-250, 600), main = "Below: Blue line, f(x), is the 'true' relationship between x and y")
lines(rep(x, 5), yerr, type = "p", cex = 0.4)
fit <- lm(y~x, data = dat)
lines(y = predict(fit, dat[,"x", drop = FALSE]), x = dat$x, lwd = 3)
fit2 <- lm(y~x+I(x^2), data = dat)
lines(y = predict(fit2, dat[,"x", drop = FALSE]), x = dat$x, lwd = 3, col = 'Green')
fit3 <- lm(y~x+I(x^2)+I(x^3), data = dat)
lines(y = predict(fit3, dat[,"x", drop = FALSE]), x = dat$x, lwd = 3, col = 'Brown')
legend("topright", legend = c("SLR fit", "Quad fit", "Cubic fit"), col = c("black", "Green", "Brown"), lty = 1, lwd = 3)
```

---
  
# Regression Modeling Ideas
  
- We model the mean response for a given $x$ value
- With multiple predictors or $x$'s, we do the same idea!

```{r, echo = FALSE, eval = TRUE, fig.align='center', out.width='500px'}
knitr::include_graphics("img/true_mlr.png")
```

```{r,webgl = TRUE, echo = FALSE, eval = FALSE}
library(rgl)
x1 <- seq(from = 0, to = 1, by = 0.01)
x2 <- seq(from = 0, to = 1, by = 0.01)
xy <- expand.grid(x1, x2)
names(xy) <- c("x1", "x2")
y <- 20 - 3*xy[,1] +5*xy[,2] -12*xy[,1]*xy[,2] + xy[,1]^2
obsx1x2 <- xy[sample(nrow(xy), size = 200, replace = TRUE),]
obsy <- 20-3*obsx1x2[,1]+5*obsx1x2[,2]-12*obsx1x2[,1]*obsx1x2[,2]+obsx1x2[,1]^2 + rnorm(200, sd = 2)
mlr_data <- data.frame(x1 = obsx1x2[,1], x2 = obsx1x2[,2], y = obsy)
# scatter plot with regression plane (rgl package)
plot3d(x = mlr_data$x1, y = mlr_data$x2, z = mlr_data$y,
					cex = 0.8, theta = 20, phi = 20, ticktype = "detailed",
                    xlab = "x1", ylab = "x2", zlab = "y",
					xlim = c(0,1), ylim = c(0,1), zlim = c(0, max(obsy)),
					main = "Data with 'true' relationship") 
#add surface
surface3d(x = x1, y = x2, z = y, color = "blue")
```


---

# Regression Modeling Ideas 

- Including a **main effect** for two predictors fits the best plane through the data

$$\mbox{Multiple Linear Regression Model: } E(Y|x_1,x_2) = \beta_0+\beta_1x_1+\beta_2x_2$$

```{r,webgl = TRUE, echo = FALSE, eval = FALSE}
plane_fit <- lm(y ~ x1 + x2, data = mlr_data)
# scatter plot with regression plane (rgl package)
plot3d(x = mlr_data$x1, y = mlr_data$x2, z = mlr_data$y,
					cex = 0.8, theta = 20, phi = 20, ticktype = "detailed",
                    xlab = "x1", ylab = "x2", zlab = "y",
					xlim = c(0,1), ylim = c(0,1), zlim = c(0, max(obsy)),
					main = "Data with best plane") 
#add surface
surface3d(x = x1, y = x2, z = predict(plane_fit, newdata = xy), color = "blue")
```

```{r, echo = FALSE, eval = TRUE, fig.align='center', out.width='400px'}
knitr::include_graphics("img/best_mlr_plane.png")
```



---

# Regression Modeling Ideas 

- Including **main effects** and an **interaction effect** allows for a more flexible surface

$$\mbox{Multiple Linear Regression Model: } E(Y|x_1,x_2) = \beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2$$

```{r,webgl = TRUE, echo = FALSE, eval = FALSE}
saddle_fit <- lm(y ~ x1*x2, data = mlr_data)
# scatter plot with regression plane (rgl package)
plot3d(x = mlr_data$x1, y = mlr_data$x2, z = mlr_data$y,
					cex = 0.8, theta = 20, phi = 20, ticktype = "detailed",
                    xlab = "x1", ylab = "x2", zlab = "y",
					xlim = c(0,1), ylim = c(0,1), zlim = c(0, max(obsy)),
					main = "Data with best saddle fit") 
#add surface
surface3d(x = x1, y = x2, z = predict(saddle_fit, newdata = xy), color = "blue")
```

```{r, echo = FALSE, eval = TRUE, fig.align='center', out.width='500px'}
knitr::include_graphics("img/best_mlr_saddle.png")
x1 <- seq(from = 0, to = 1, by = 0.01)
x2 <- seq(from = 0, to = 1, by = 0.01)
xy <- expand.grid(x1, x2)
names(xy) <- c("x1", "x2")
y <- 20 - 3*xy[,1] +5*xy[,2] -12*xy[,1]*xy[,2] + xy[,1]^2
obsx1x2 <- xy[sample(nrow(xy), size = 200, replace = TRUE),]
obsy <- 20-3*obsx1x2[,1]+5*obsx1x2[,2]-12*obsx1x2[,1]*obsx1x2[,2]+obsx1x2[,1]^2 + rnorm(200, sd = 2)
mlr_data <- data.frame(x1 = obsx1x2[,1], x2 = obsx1x2[,2], y = obsy)
saddle_fit <- lm(y ~ x1*x2, data = mlr_data)
```

---

# Regression Modeling Ideas 

- Including **main effects** and an **interaction effect** allows for a more flexible surface

- Interaction effects allow for the **effect** of one variable to depend on the value of another

- Model fit previously gives 
    
    + $\hat{y}$ = (`r round(saddle_fit$coef[1],3)`) + (`r  round(saddle_fit$coef[2],3)`)x1 + (`r  round(saddle_fit$coef[3],3)`)x2 + (`r  round(saddle_fit$coef[4], 3)`)x1x2
    
---

# Regression Modeling Ideas 

- Including **main effects** and an **interaction effect** allows for a more flexible surface

- Interaction effects allow for the **effect** of one variable to depend on the value of another

- Model fit previously gives 
    
    + $\hat{y}$ = (`r round(saddle_fit$coef[1],3)`) + (`r  round(saddle_fit$coef[2],3)`)x1 + (`r  round(saddle_fit$coef[3],3)`)x2 + (`r  round(saddle_fit$coef[4], 3)`)x1x2

    + For $x_1$ = 0, the slope on $x_2$ is (`r  round(saddle_fit$coef[3],3)`)`+0*` (`r  round(saddle_fit$coef[4],3)`) = `r  round(saddle_fit$coef[3],3)`


---

# Regression Modeling Ideas 

- Including **main effects** and an **interaction effect** allows for a more flexible surface

- Interaction effects allow for the **effect** of one variable to depend on the value of another

- Model fit previously gives 
    
    + $\hat{y}$ = (`r round(saddle_fit$coef[1],3)`) + (`r  round(saddle_fit$coef[2],3)`)x1 + (`r  round(saddle_fit$coef[3],3)`)x2 + (`r  round(saddle_fit$coef[4], 3)`)x1x2

    + For $x_1$ = 0, the slope on $x_2$ is (`r  round(saddle_fit$coef[3],3)`)`+0*` (`r  round(saddle_fit$coef[4],3)`) = `r  round(saddle_fit$coef[3],3)`

    + For $x_1$ = 0.5, the slope on $x_2$ is (`r  round(saddle_fit$coef[3],3)`)`+0.5*`(`r  round(saddle_fit$coef[4],3)`) = `r  round(saddle_fit$coef[3]+0.5*saddle_fit$coef[4],3)`
    
---

# Regression Modeling Ideas 

- Including **main effects** and an **interaction effect** allows for a more flexible surface

- Interaction effects allow for the **effect** of one variable to depend on the value of another

- Model fit previously gives 
    
    + $\hat{y}$ = (`r round(saddle_fit$coef[1],3)`) + (`r  round(saddle_fit$coef[2],3)`)x1 + (`r  round(saddle_fit$coef[3],3)`)x2 + (`r  round(saddle_fit$coef[4], 3)`)x1x2

    + For $x_1$ = 0, the slope on $x_2$ is (`r  round(saddle_fit$coef[3],3)`)`+0*` (`r  round(saddle_fit$coef[4],3)`) = `r  round(saddle_fit$coef[3],3)`

    + For $x_1$ = 0.5, the slope on $x_2$ is (`r  round(saddle_fit$coef[3],3)`)`+0.5*`(`r  round(saddle_fit$coef[4],3)`) = `r  round(saddle_fit$coef[3]+0.5*saddle_fit$coef[4],3)`

    + For $x_1$ = 1, the slope on $x_2$ is (`r  round(saddle_fit$coef[3],3)`)`+1*`(`r  round(saddle_fit$coef[4],3)`) = `r round(saddle_fit$coef[3]+1*saddle_fit$coef[4],3)`

- Similarly, the slope on $x_1$ depends on $x_2$!



---

# Regression Modeling Ideas 

- Including **main effects** and an **interaction effect** allows for a more flexible surface
- Can also include higher order polynomial terms

$$\mbox{Multiple Linear Regression Model: } E(Y|x_1,x_2) = \beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+\beta_4x_1^2$$

```{r,webgl = TRUE, echo = FALSE, eval = FALSE}
comp_fit <- lm(y ~ x1*x2 + I(x1^2), data = mlr_data)
# scatter plot with regression plane (rgl package)
plot3d(x = mlr_data$x1, y = mlr_data$x2, z = mlr_data$y,
					cex = 0.8, theta = 20, phi = 20, ticktype = "detailed",
                    xlab = "x1", ylab = "x2", zlab = "y",
					xlim = c(0,1), ylim = c(0,1), zlim = c(0, max(obsy)),
					main = "Data with complicated surface fit") 
#add surface
surface3d(x = x1, y = x2, z = predict(comp_fit, newdata = xy), color = "blue")
```

```{r, echo = FALSE, eval = TRUE, fig.align='center', out.width='500px'}
knitr::include_graphics("img/best_mlr_comp.png")
```


---

# Regression Modeling Ideas

Can also include categorical variables through **dummy** or **indicator** variables

- Categorical variable with value of $Success$ and $Failure$
- Define $x_2 = 0$ if variable is $Failure$
- Define $x_2 = 1$ if variable is $Success$


---

layout: false

# Regression Modeling Ideas

Can also include categorical variables through **dummy** or **indicator** variables

- Categorical variable with value of $Success$ and $Failure$
- Define $x_2 = 0$ if variable is $Failure$
- Define $x_2 = 1$ if variable is $Success$


```{r, echo = FALSE, out.width = "370px", fig.align='left'}
set.seed(1)
x1 <- seq(from = -11, to = 17, by = 0.1)
x2size <- 500
x2slopesize <- 3
y <- 300 - x1^2 -20*x1
yerr1 <- rep(y, 1) + rnorm(length(y)*1, mean = 3, sd = 100)
yerr2 <- rep(y*x2slopesize, 1) + rnorm(length(y)*1, mean = 3, sd = 100) + x2size
reg_data <- data.frame(y = c(yerr1, yerr2), x1 = rep(x1,2), x2 = c(rep(0, length(x1)), rep(1, length(x1))))
color_vec <- ifelse(reg_data$x2, "Blue", "Green")
plot(reg_data$x1, reg_data$y, type = "p", col = color_vec, main = "Plot of x vs y with Color by x2", 
     xlab = "x1", ylab = "y")
```


---

# Regression Modeling Ideas

- Define $x_2 = 0$ if variable is $Failure$
- Define $x_2 = 1$ if variable is $Success$

$$\mbox{Separate Intercept Model: }E(Y|x) = \beta_0+\beta_1x_1 + \beta_2x_2$$

```{r, echo = FALSE, out.width = "370px", fig.align='left'}
plot(reg_data$x1, reg_data$y, type = "p", col = color_vec, main = "Plot of x vs y with Color by x2", 
     xlab = "x1", ylab = "y")
fit <- lm(y~x1+x2, data = reg_data)
lines(y = predict(fit, reg_data[reg_data$x2 == 1, c("x1","x2")]), x = reg_data[reg_data$x2==1,'x1'], lwd = 3, col = "blue")
lines(y = predict(fit, reg_data[reg_data$x2 == 0, c("x1","x2")]), x = reg_data[reg_data$x2==0,'x1'], lwd = 3, col = "green")
legend("topright", legend = c("Fit when x2 = 1", "Fit when x2 = 0"), col = c("Blue", "Green"), lty = 1, lwd = 3)
```




---

# Regression Modeling Ideas

- Define $x_2 = 0$ if variable is $Failure$
- Define $x_2 = 1$ if variable is $Success$

$$\mbox{Separate Intercept and Slopes Model: }E(Y|x) = \beta_0+\beta_1x_1 + \beta_2x_2+\beta_3x_1x_2$$


```{r, echo = FALSE, out.width = "370px", fig.align='left'}
plot(reg_data$x1, reg_data$y, type = "p", col = color_vec, main = "Plot of x vs y with Color by x2", 
     xlab = "x1", ylab = "y")
fit2 <- lm(y~x1*x2, data = reg_data)
lines(y = predict(fit2, reg_data[reg_data$x2 == 1, c("x1","x2")]), x = reg_data[reg_data$x2==1,'x1'], lwd = 3, col = "blue")
lines(y = predict(fit2, reg_data[reg_data$x2 == 0, c("x1","x2")]), x = reg_data[reg_data$x2==0,'x1'], lwd = 3, col = "green")

legend("topright", legend = c("Fit when x2 = 1", "Fit when x2 = 0"), col = c("blue", "green"), lty = 1, lwd = 3)
```



---

# Regression Modeling Ideas

- Define $x_2 = 0$ if variable is $Failure$
- Define $x_2 = 1$ if variable is $Success$

$$\mbox{Separate Quadratics Model: }E(Y|x) = \beta_0+ \beta_1x_2+\beta_2x_1+\beta_3x_1x_2+ \beta_4x_1^2 +\beta_5x_1^2x_2$$

```{r, echo = FALSE, out.width = "370px", fig.align='left'}
plot(reg_data$x1, reg_data$y, type = "p", col = color_vec, main = "Plot of x vs y with Color by x2", 
     xlab = "x1", ylab = "y")
fit2 <- lm(y~x1*x2 + x2*I(x1^2), data = reg_data)
lines(y = predict(fit2, reg_data[reg_data$x2 == 1, c("x1","x2")]), x = reg_data[reg_data$x2==1,'x1'], lwd = 3, col = "blue")
lines(y = predict(fit2, reg_data[reg_data$x2 == 0, c("x1","x2")]), x = reg_data[reg_data$x2==0,'x1'], lwd = 3, col = "green")
legend("topright", legend = c("Fit when x2 = 1", "Fit when x2 = 0"), col = c("blue", "green"), lty = 1, lwd = 3)
```



---

# Regression Modeling Ideas

If your categorical variable has more than k>2 categories, define k-1 dummy variables
- Categorical variable with values of "Assistant", "Contractor", "Executive"
- Define $x_2 = 0$ if variable is $Executive$ or $Contractor$
- Define $x_2 = 1$ if variable is $Assistant$
- Define $x_3 = 0$ if variable is $Contractor$ or $Assistant$
- Define $x_3 = 1$ if variable is $Executive$

---

# Regression Modeling Ideas

If your categorical variable has more than k>2 categories, define k-1 dummy variables
- Categorical variable with values of "Assistant", "Contractor", "Executive"
- Define $x_2 = 0$ if variable is $Executive$ or $Contractor$
- Define $x_2 = 1$ if variable is $Assistant$
- Define $x_3 = 0$ if variable is $Contractor$ or $Assistant$
- Define $x_3 = 1$ if variable is $Executive$

$$\mbox{Separate Intercepts Model: }E(Y|x) = \beta_0+ \beta_1x_1+\beta_2x_2+\beta_3x_3$$
What is implied if $x_2$ and $x_3$ are both zero?



---

# Fitting an MLR Model

Big Idea: Trying to find the line, plane, saddle, etc. **of best fit** through points 

- How do we do the fit??

    + Usually minimize the sum of squared residuals (errors)
    

---

# Fitting an MLR Model

Big Idea: Trying to find the line, plane, saddle, etc. **of best fit** through points

- How do we do the fit??

    + Usually minimize the sum of squared residuals (errors)

- Residual = observed - predicted or $y_i-\hat{y}_i$

$$\min\limits_{\hat{\beta}'s}\sum_{i=1}^{n}(y_i-(\hat\beta_0+\hat\beta_1x_{1i}+...+\hat\beta_px_{pi}))^2$$
  
---
  
# Fitting an MLR Model
  
Big Idea: Trying to find the line, plane, saddle, etc. **of best fit** through points

- How do we do the fit??
  
  + Usually minimize the sum of squared residuals (errors)

- Residual = observed - predicted or $y_i-\hat{y}_i$
  
  $$\min\limits_{\hat{\beta}'s}\sum_{i=1}^{n}(y_i-(\hat\beta_0+\hat\beta_1x_{1i}+...+\hat\beta_px_{pi}))^2$$

- Closed-form results exist for easy calculation via software!


---

# Fitting a Linear Regression Model in R

- Use `lm()` and specify a `formula`: `LHS ~ RHS` 
    
    + `y ~` implies y is modelled by a linear function of the RHS
    
    + RHS consists of terms separated by `+` operators 

        - `y ~ x1 + x2` gives $E(Y|x_1, x_2) = \beta_0+\beta_1x_1+\beta_2x_2$

---

# Fitting a Linear Regression Model in R

- Use `lm()` and specify a `formula`: `LHS ~ RHS` 
    
    + `y ~` implies y is modelled by a linear function of the RHS
    
    + RHS consists of terms separated by `+` operators 

        - `y ~ x1 + x2` gives $E(Y|x_1, x_2) = \beta_0+\beta_1x_1+\beta_2x_2$

        - `:` for interactions, `y ~ x1 + x2 + x1:x2` gives $E(Y|x_1, x_2) = \beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2$
        
---

# Fitting a Linear Regression Model in R

- Use `lm()` and specify a `formula`: `LHS ~ RHS` 
    
    + `y ~` implies y is modelled by a linear function of the RHS
    
    + RHS consists of terms separated by `+` operators 

        - `y ~ x1 + x2` gives $E(Y|x_1, x_2) = \beta_0+\beta_1x_1+\beta_2x_2$

        - `:` for interactions, `y ~ x1 + x2 + x1*x2` gives $E(Y|x_1, x_2) = \beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3$
    
        - `*` denotes factor crossing: `a*b` is interpreted as `a + b + a:b`

        - `y ~ x - 1` removes the intercept term
        
---

# Fitting a Linear Regression Model in R

- Use `lm()` and specify a `formula`: `LHS ~ RHS` 
    
    + `y ~` implies y is modelled by a linear function of the RHS
    
    + RHS consists of terms separated by `+` operators 

        - `y ~ x1 + x2` gives $E(Y|x_1, x_2) = \beta_0+\beta_1x_1+\beta_2x_2$

        - `:` for interactions, `y ~ x1 + x2 + x1*x2` gives $E(Y|x_1, x_2) = \beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3$
    
        - `*` denotes factor crossing: `a*b` is interpreted as `a + b + a:b`

        - `y ~ x - 1` removes the intercept term

        - `I()` can be used to create arithmetic predictors
        
            + `y ~ a + I(b+c)` implies `b+c` is the sum of b and c
            
            + `y ~ x + I(x^2)` implies $E(Y|x_1) = \beta_0+\beta_1x_1+\beta_2x_1^2$


---

# Fitting MLR Models

- Let's read in our `bike_data` and fit some MLR models

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
bike_data <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv")
bike_data <- bike_data |>
  mutate(log_selling_price = log(selling_price),
         log_km_driven = log(km_driven)) |>
  select(log_km_driven, year, log_selling_price, owner, everything())
bike_data
```

---

# Fitting MLR Models

- Create models with the same slope but intercepts differing by a categorical variable

```{r}
owner_fits <- lm(log_selling_price ~ owner + log_km_driven, data = bike_data)
coef(owner_fits)
```

---

# Fitting MLR Models

- Create a data frame for plotting

```{r}
x_values <- seq(from = min(bike_data$log_km_driven), 
                to = max(bike_data$log_km_driven), 
                length = 2)
pred_df <- data.frame(log_km_driven = rep(x_values, 4), 
                      owner = c(rep("1st owner", 2),
                                rep("2nd owner", 2),
                                rep("3rd owner", 2),
                                rep("4th owner", 2)))
pred_df <- pred_df |> 
  mutate(predictions = predict(owner_fits, newdata = pred_df))
pred_df
```


---

# Fitting MLR Models

- Plot our different intercept models

```{r, fig.align='center', out.width = "360px"}
ggplot(bike_data, aes(x = log_km_driven, y = log_selling_price, color = owner)) +
  geom_point() + 
  geom_line(data = pred_df, aes(x = log_km_driven, y = predictions, color = owner))
```

---

# Fitting MLR Models

- Create models with the different slopes and intercepts 

```{r}
owner_fits_full <- lm(log_selling_price ~ owner*log_km_driven, data = bike_data)
coef(owner_fits_full)
```
---

# Fitting MLR Models

- Plot our different intercept models

```{r, fig.align='center', out.width = "360px"}
ggplot(bike_data, aes(x = log_km_driven, y = log_selling_price, color = owner)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

---
  
# Choosing an MLR Model
  
- Given a bunch of predictors, tons of models you could fit!  How to choose?

- Many variable selection methods exist...

- If you care mainly about prediction, just use *cross-validation* or training/test split!

    + Compare predictions using some metric!

    + We'll see how to use `tidymodels` to do this in a coherent way shortly!

---
      
# Recap
      
- Multiple Linear Regression models are a common model used for a numeric response
    
- Generally fit via minimizing the sum of squared residuals or errors
    
    + Could fit using sum of absolute deviation, or other metric
    
- Can include polynomial terms, interaction terms, and categorical variables
    
- Good metric to compare models with a continuous response is the RMSE
    