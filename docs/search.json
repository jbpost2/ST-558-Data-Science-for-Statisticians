[
  {
    "objectID": "Notes/58-Dockerizing_Shiny_Landing.html",
    "href": "Notes/58-Dockerizing_Shiny_Landing.html",
    "title": "Dockerizing a Shiny App",
    "section": "",
    "text": "The video below gives an example of dockerizing a shiny app!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Dockerizing a Shiny App"
    ]
  },
  {
    "objectID": "Notes/58-Dockerizing_Shiny_Landing.html#notes",
    "href": "Notes/58-Dockerizing_Shiny_Landing.html#notes",
    "title": "Dockerizing a Shiny App",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\napp.R file\nDockerfile\n\nThat’s it! That’s all we have :)",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Dockerizing a Shiny App"
    ]
  },
  {
    "objectID": "Notes/56-Docker_Landing.html",
    "href": "Notes/56-Docker_Landing.html",
    "title": "Docker Basics",
    "section": "",
    "text": "The video below discusses the idea of containerization and the software Docker.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Docker Basics"
    ]
  },
  {
    "objectID": "Notes/56-Docker_Landing.html#notes",
    "href": "Notes/56-Docker_Landing.html#notes",
    "title": "Docker Basics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Docker Basics"
    ]
  },
  {
    "objectID": "Notes/55.5-Week10.html",
    "href": "Notes/55.5-Week10.html",
    "title": "Week 10 Overview",
    "section": "",
    "text": "More coming soon!",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Week 10 Overview"
    ]
  },
  {
    "objectID": "Notes/53-Regression_Classification_Trees.html",
    "href": "Notes/53-Regression_Classification_Trees.html",
    "title": "Regression & Classification Trees",
    "section": "",
    "text": "We’ve looked to two very common models:\nThese models are great ways to try and understand relationships between variables. One thing that can be a drawback (or benefit, depending) when using these models is that they are highly structured. Recall, in the MLR case we are essentially fitting some high dimensional plane to our data. If our data doesn’t follow this type of structure, the models fit can do a poor job predicting in some cases.\nInstead of these structured models, we can use models that are more flexible. Let’s talk about one such type of model, the regression/classification tree!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Regression & Classification Trees"
    ]
  },
  {
    "objectID": "Notes/53-Regression_Classification_Trees.html#tree-based-methods",
    "href": "Notes/53-Regression_Classification_Trees.html#tree-based-methods",
    "title": "Regression & Classification Trees",
    "section": "Tree Based Methods",
    "text": "Tree Based Methods\nTree based methods are very flexible. They attempt to split up predictor space into regions. On each region, a different prediction can then be made. Adjacent regions need not have predictions close to each other!\nRecall that we have two separate tasks we could do in a supervised learning situation, regression or classification. Depending on the situation, we can create a regression or classification tree!\n\nClassification tree if the goal is to classify (predict) group membership\n\nUsually use most prevalent class in region as the prediction\n\nRegression tree if the goal is to predict a continuous response\n\nUsually use mean of observations in region as the prediction\n\n\nThese models are very easy for people to interpret! For instance, consider the tree below relating a predictor (speed) to stopping distance (dist).\n\nlibrary(tree) #rpart is also often used\n\nWarning: package 'tree' was built under R version 4.4.3\n\nfitTree &lt;- tree(dist ~ speed, data = cars) #default splitting is deviance\nplot(fitTree)\ntext(fitTree)\n\n\n\n\n\n\n\n\nWe can compare this to the simple linear regression fit to see the increased flexibility of a regression tree model.\n\nlibrary(tidyverse)\nggplot(cars, aes(x = speed, y = dist)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, size = 2) + \n  geom_segment(x = 0, xend = 9.5, y = 10.67, yend = 10.67, col = \"Orange\", size = 2) +\n  geom_segment(x = 9.5, xend = 12.5, y = 23.22, yend = 23.22, col = \"Orange\", size = 2) +\n  geom_segment(x = 12.5, xend = 17.5, y = 39.75, yend = 39.75, col = \"Orange\", size = 2) +\n  geom_segment(x = 17.5, xend = 23.5, y = 55.71, yend = 55.71, col = \"Orange\", size = 2) +\n  geom_segment(x = 23.5, xend = max(cars$speed), y = 92, yend = 92, col = \"Orange\", size = 2)\n\n\n\n\n\n\n\n\n\nHow Is a Regression Tree Fit?\nRecall: Once we’ve chosen our model form, we need to fit the model to data. Generally, we can write the fitting process as the minimization of some loss function over the training data. How do we pick our splits of the predictor space in this case?\n\nFit using recursive binary splitting - a greedy algorithm\nFor every possible value of each predictor, we find the squared error loss based on splitting our data around that point. We then try to minimize that\n\nConsider having one variable xx. For a given observed value, call it ss, we can think of having two regions (recall || is read as ‘given’): R1(s)={x|x&lt;s} and R2(s)={x|x≥s}R_1(s) = \\{x|x &lt; s\\}\\mbox{ and }R_2(s) = \\{x|x \\geq s\\}\nWe seek the value of ss that minimize the equation ∑all x in R1(s)(yi−y‾R1)2+∑all x in R2(s)(yi−y‾R2)2\\sum_{\\mbox{all }x\\mbox{ in }R_1(s)}(y_i-\\bar{y}_{R_1})^2+\\sum_{\\mbox{all }x\\mbox{ in }R_2(s)}(y_i-\\bar{y}_{R_2})^2\nWritten more mathematically, we could say we want minimize mins∑i:xi∈R1(s)(yi−y‾R1)2+∑i:xi∈R2(s)(yi−y‾R2)2min_{s} \\sum_{i:x_i\\in R_1(s)}(y_i-\\bar{y}_{R_1})^2+\\sum_{i:x_i\\in R_2(s)}(y_i-\\bar{y}_{R_2})^2\n\n\nLet’s visualize this idea! Consider that basic cars data set that has a response of dist (stopping distance) and a predictor of speed. Let’s find the value of the loss functions for different splits of our speed variable.\n\nggplot(cars, aes(x = speed, y = dist)) + \n  geom_point()\n\n\n\n\n\n\n\n\nLet’s first try a split at speed = 7. The sum of squared errors based on this split is 2.766546^{4}.\n\n\n\n\n\n\n\n\n\nAgain, this is found by taking all the points in the first region, finding the residual (from the mean, represented by the blue line here), squaring those, and summing the values. Then we repeat for the 2nd region. The sum of those two values is then the sum of squared errors (SSE) if we were to use this split.\nIs that the smallest it could be? Likely not! Let’s try some other splits and see what SSE they give.\n\n\n\n\n\n\n\n\n\n\nWe would try this for all possible splits (across each predictor) and choose the split that minimizes the sum of squared errors as our first split. It turns out that speed = 17.5 is the optimal splitting point for this data set.\n\n\n\n\n\n\n\n\n\n\n\nNext, we’d go down the first branch of that split to that ‘node’. This node has all the observations corresponding to that branch. Now we repeat this process there!\n\n\n\n\n\n\n\n\n\n\n\nHere the best split on the lower portion is 12.5.\nLikewise, we go down the second branch to the other node and repeat the process.\nGenerally, we grow a `large’ tree (many nodes)\nTrees can then be pruned back so as to not overfit the data (pruned back using some criterion like cost-complexity pruning)\nGenerally, we can choose number of nodes/splits using the training/test set or cross-validation!\n\n\n\nFitting Regression Trees with tidymodels\n\nRecall the Bike data and log_selling_price as our response\n\n\nset.seed(10)\nlibrary(tidyverse)\nlibrary(tidymodels)\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv\") |&gt;\n  mutate(log_selling_price = log(selling_price)) |&gt;\n  select(-selling_price)\n#save creation of new variables for now!\nbike_split &lt;- initial_split(bike_data, prop = 0.7)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_train\n\n# A tibble: 742 × 7\n   name     year seller_type owner km_driven ex_showroom_price log_selling_price\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 Bajaj …  2012 Individual  1st …     50000             54299             10.3 \n 2 Honda …  2015 Individual  1st …      7672             54605             10.6 \n 3 Bajaj …  2005 Individual  1st …     21885                NA              9.80\n 4 Hero H…  2017 Individual  1st …     27000                NA             10.5 \n 5 Royal …  2013 Individual  1st …     49000                NA             11.4 \n 6 Bajaj …  2008 Individual  1st …     19500                NA             10.3 \n 7 Hero C…  2014 Individual  1st …     38000                NA             10.5 \n 8 Bajaj …  2009 Individual  1st …     16000                NA              9.90\n 9 Hero H…  2008 Individual  3rd …     65000                NA             10.1 \n10 Bajaj …  2019 Individual  1st …      7600                NA             12.2 \n# ℹ 732 more rows\n\nbike_CV_folds &lt;- vfold_cv(bike_train, 10)\n\nWe can fit a regression tree model in a very similar way to how we fit our MLR models!\n\nCreate our Recipe for Data Preprocessing\nFirst, let’s create our recipe.\n\ntree_rec &lt;- recipe(log_selling_price ~ ., data = bike_train) |&gt;\n  update_role(name, new_role = \"ID\") |&gt;\n  step_log(km_driven) |&gt;\n  step_rm(ex_showroom_price) |&gt;\n  step_dummy(owner, seller_type) |&gt;\n  step_normalize(all_numeric(), -all_outcomes())\ntree_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: km_driven\n\n\n• Variables removed: ex_showroom_price\n\n\n• Dummy variables from: owner and seller_type\n\n\n• Centering and scaling for: all_numeric() and -all_outcomes()\n\n\nNote: We don’t need to include interaction terms in our tree based models! An interaction would imply that the effect of, say, log_km_driven depends on the year the bike was manufactured (and vice-versa). The tree structure inherently includes this type of relationship! For instance, suppose we first split on log_km_driven &gt; 10. On the branch where log_km_driven &gt; 10 we then split on year &lt; 1990. Suppose those are our only two splits. We can see that the effect of year is different depending on our log_km_driven! For one side of the log_km_driven split we don’t include year at all (hence it doesn’t have an effect when considering those values of log_km_driven) and on the other side of that split we change our prediction based on year. This is exactly the idea of an interaction!\n\n\nDefine our Model and Engine\nNext, let’s define our model. The info page here can be used to determine the right function to call and the possible engines to use for the fit.\nIn this case, decision_tree() with rpart as the engine will do the trick. If we click on the link for this model we can see that there are three tuning parameters we can consider:\n\ntree_depth: Tree Depth (type: integer, default: 30L)\nmin_n: Minimal Node Size (type: integer, default: 2L)\ncost_complexity: Cost-Complexity Parameter (type: double, default: 0.01)\n\nIf we want to use CV to choose one of these, we can set its value to tune() when creating the model. Let’s use tree_depth and cost_complexity as our tuning parameters and set our min_n to 20.\nIn the case of decision_tree() we also need to tell tidymodels whether we are doing a regression task vs a classification task. This is done via set_mode().\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n\n\nCreate our Workflow\nNow we use workflow() to create an object to use in our fitting processes.\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_mod)\n\n\n\nUse CV to Select our Tuning Parameters\nNow we can use tune_grid() on our bike_CV_folds object. We just need to create a tuning grid to fit our models with. If we don’t specify one, the dials package tries to figure it out for us:\n\ntemp &lt;- tree_wkf |&gt; \n  tune_grid(resamples = bike_CV_folds)\ntemp |&gt; \n  collect_metrics()\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1        7.50e- 6         12 rmse    standard   0.510    10  0.0175 Preprocess…\n 2        7.50e- 6         12 rsq     standard   0.467    10  0.0207 Preprocess…\n 3        2.80e- 8          8 rmse    standard   0.509    10  0.0174 Preprocess…\n 4        2.80e- 8          8 rsq     standard   0.468    10  0.0205 Preprocess…\n 5        1.66e- 3         15 rmse    standard   0.501    10  0.0158 Preprocess…\n 6        1.66e- 3         15 rsq     standard   0.479    10  0.0182 Preprocess…\n 7        5.79e-10          2 rmse    standard   0.538    10  0.0196 Preprocess…\n 8        5.79e-10          2 rsq     standard   0.396    10  0.0209 Preprocess…\n 9        4.05e- 9          3 rmse    standard   0.510    10  0.0181 Preprocess…\n10        4.05e- 9          3 rsq     standard   0.456    10  0.0193 Preprocess…\n11        1.24e- 3          6 rmse    standard   0.503    10  0.0158 Preprocess…\n12        1.24e- 3          6 rsq     standard   0.477    10  0.0193 Preprocess…\n13        5.68e- 2          5 rmse    standard   0.543    10  0.0202 Preprocess…\n14        5.68e- 2          5 rsq     standard   0.382    10  0.0165 Preprocess…\n15        1.82e- 6         12 rmse    standard   0.510    10  0.0175 Preprocess…\n16        1.82e- 6         12 rsq     standard   0.467    10  0.0207 Preprocess…\n17        5.23e- 8          9 rmse    standard   0.509    10  0.0175 Preprocess…\n18        5.23e- 8          9 rsq     standard   0.468    10  0.0208 Preprocess…\n19        5.47e- 5          9 rmse    standard   0.509    10  0.0175 Preprocess…\n20        5.47e- 5          9 rsq     standard   0.468    10  0.0208 Preprocess…\n\n\nWe can see that the cost_complexity parameter and tree_depth parameters are randomly varied and results are returned.\nIf we want to set the number of the values ourselves, we can use grid_regular() instead. By specifying a vector or levels we can say how many of each tuning parameter we want. grid_regular() then finds all combinations of the values of each (here 10*5 = 50 combinations).\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\nNow we use tune_grid() with this grid specified.\n\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = bike_CV_folds,\n            grid = tree_grid)\ntree_fits\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [667/75]&gt; Fold01 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [667/75]&gt; Fold02 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [668/74]&gt; Fold03 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [668/74]&gt; Fold04 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [668/74]&gt; Fold05 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [668/74]&gt; Fold06 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [668/74]&gt; Fold07 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [668/74]&gt; Fold08 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [668/74]&gt; Fold09 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [668/74]&gt; Fold10 &lt;tibble [100 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nLooking at the tree_fits object isn’t super useful. It has all the info but we need to pull it out. As we see above, we can use collect_metrics() to combine the metrics across the folds.\n\ntree_fits |&gt;\n  collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001          1 rmse    standard   0.561    10  0.0190 Preprocess…\n 2    0.0000000001          1 rsq     standard   0.338    10  0.0176 Preprocess…\n 3    0.000000001           1 rmse    standard   0.561    10  0.0190 Preprocess…\n 4    0.000000001           1 rsq     standard   0.338    10  0.0176 Preprocess…\n 5    0.00000001            1 rmse    standard   0.561    10  0.0190 Preprocess…\n 6    0.00000001            1 rsq     standard   0.338    10  0.0176 Preprocess…\n 7    0.0000001             1 rmse    standard   0.561    10  0.0190 Preprocess…\n 8    0.0000001             1 rsq     standard   0.338    10  0.0176 Preprocess…\n 9    0.000001              1 rmse    standard   0.561    10  0.0190 Preprocess…\n10    0.000001              1 rsq     standard   0.338    10  0.0176 Preprocess…\n# ℹ 90 more rows\n\n\nAs done in the tutorial, we can plot these to gain some insight:\n\ntree_fits %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\nIdeally, we probably want to sort this by the smallest rmse value. Let’s also filter down to just looking at rmse.\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001          4 rmse    standard   0.496    10  0.0163 Preprocess…\n 2    0.000000001           4 rmse    standard   0.496    10  0.0163 Preprocess…\n 3    0.00000001            4 rmse    standard   0.496    10  0.0163 Preprocess…\n 4    0.0000001             4 rmse    standard   0.496    10  0.0163 Preprocess…\n 5    0.000001              4 rmse    standard   0.496    10  0.0163 Preprocess…\n 6    0.00001               4 rmse    standard   0.496    10  0.0163 Preprocess…\n 7    0.0001                4 rmse    standard   0.496    10  0.0163 Preprocess…\n 8    0.001                 4 rmse    standard   0.496    10  0.0162 Preprocess…\n 9    0.01                  4 rmse    standard   0.498    10  0.0176 Preprocess…\n10    0.01                  8 rmse    standard   0.498    10  0.0176 Preprocess…\n# ℹ 40 more rows\n\n\nThe function select_best() can be used to grab the best model’s tuning parameter values. We also should specify which metric!\n\ntree_best_params &lt;- select_best(tree_fits, metric = \"rmse\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          4 Preprocessor1_Model11\n\n\n(After this initial phase, we might also want to fit a finer grid of tuning parameter values near the current ‘best’ ones!) Now we can finalize our model on the training set by fitting this chosen model via finalize_workflow().\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\nNow that we’ve set up how to fit the final model, let’s do it via last_fit() on the bike_split object.\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(bike_split)\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [742/319]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nThis object has information about how the final fitted model (fit on the entire training data set) performs on the test set. We can see the metrics more clearly using collect_metrics().\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.565 Preprocessor1_Model1\n2 rsq     standard       0.445 Preprocessor1_Model1\n\n\nAs done in the tutorial, we could pull out this fit and learn more about it.\n\ntree_final_model &lt;- extract_workflow(tree_final_fit) \ntree_final_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 742 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 742 352.956900 10.721500  \n   2) year&lt; 0.1604381 354 118.136200 10.304490  \n     4) year&lt; -0.9711688 99  42.945090  9.955315  \n       8) km_driven&gt;=-0.07668815 80  24.027690  9.812273  \n        16) year&lt; -1.423811 52  11.224130  9.659656 *\n        17) year&gt;=-1.423811 28   9.343018 10.095710 *\n       9) km_driven&lt; -0.07668815 19  10.388380 10.557600 *\n     5) year&gt;=-0.9711688 255  58.435060 10.440050  \n      10) km_driven&gt;=-0.1109097 205  33.439120 10.355390  \n        20) year&lt; -0.518526 80  12.341380 10.225880 *\n        21) year&gt;=-0.518526 125  18.897350 10.438270 *\n      11) km_driven&lt; -0.1109097 50  17.502590 10.787150  \n        22) year&lt; -0.2922047 16   6.876996 10.503040 *\n        23) year&gt;=-0.2922047 34   8.726354 10.920850 *\n   3) year&gt;=0.1604381 388 117.094000 11.101970  \n     6) year&lt; 0.6130808 149  29.735310 10.800920  \n      12) km_driven&gt;=-0.5047722 121  20.838220 10.751970  \n        24) owner_X2nd.owner&gt;=1.178863 15   1.468796 10.451540 *\n        25) owner_X2nd.owner&lt; 1.178863 106  17.823910 10.794490 *\n      13) km_driven&lt; -0.5047722 28   7.354383 11.012450  \n        26) km_driven&lt; -1.218805 10   1.952084 10.880130 *\n        27) km_driven&gt;=-1.218805 18   5.129964 11.085950 *\n     7) year&gt;=0.6130808 239  65.435960 11.289650  \n      14) km_driven&gt;=-1.236697 179  35.914190 11.164620  \n        28) km_driven&gt;=0.1002281 52   8.612033 10.998670 *\n        29) km_driven&lt; 0.1002281 127  25.283800 11.232560 *\n      15) km_driven&lt; -1.236697 60  18.374510 11.662680  \n        30) km_driven&gt;=-2.026019 38   8.605414 11.552500 *\n        31) km_driven&lt; -2.026019 22   8.511043 11.852980 *\n\n\nPlotting is definitely the better way to view this!\n\ntree_final_model %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nComparing to Our LASSO and MLR Fits\nRecall, we fit MLR models and LASSO models to this data set. We can pull those back up to determine which model did the best overall on the test set. Then we can get an overall ‘best’ model and fit that model to the entire data set!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Regression & Classification Trees"
    ]
  },
  {
    "objectID": "Notes/53-Regression_Classification_Trees.html#classification-trees",
    "href": "Notes/53-Regression_Classification_Trees.html#classification-trees",
    "title": "Regression & Classification Trees",
    "section": "Classification Trees",
    "text": "Classification Trees\nClassification trees are very similar to regression trees except, of course, our response is a categorical variable. This means that we don’t use the same loss functions nor metrics, but we still split the predictor space up into regions. We then can make our prediction based on which bin an observation ends up in. Most often, we use the most prevalent class in a bin as our prediction.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Regression & Classification Trees"
    ]
  },
  {
    "objectID": "Notes/53-Regression_Classification_Trees.html#recap-and-pros-cons",
    "href": "Notes/53-Regression_Classification_Trees.html#recap-and-pros-cons",
    "title": "Regression & Classification Trees",
    "section": "Recap and Pros & Cons",
    "text": "Recap and Pros & Cons\n\nTrees are a nonlinear model that can be more flexible than linear models.\n\nPros:\n\nSimple to understand and easy to interpret output\n\nPredictors don’t need to be scaled. Unlike algorithms like the LASSO, having all the predictors on different scales makes no difference in the choosing of regions.\nNo statistical assumptions necessary to get the fit (although this is true for least squares regression as well)\nBuilt in variable selection based on the algorithm!\n\nCons:\n\nSmall changes in data can vastly change tree\n\nThe lack of ‘sharing’ information with nearby data points makes this algorithm more variable. Given a new data set from the same situation, the splits we get for the tree can differ quite a bit! That isn’t ideal as we’d like to have stable results across data sets collected on the same population.\n\nNo optimal algorithm for choosing splits exists.\n\nWe saw the use of a greedy algorithm to select our regions in the regression tree case. This is a greedy algorithm because it is only looking one step ahead to find the best split. There might be a split at this step that creates a great future split. However, we may never find it because we only ever look at the best thing we can do at the current split!\n\nNeed to prune or use CV to determine the model.\n\nWith MLR models, CV isn’t used at all. However, here we really need to prune the tree and/or use CV to figure out the optimal size of the tree to build!\n\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Regression & Classification Trees"
    ]
  },
  {
    "objectID": "Notes/51.5-Week9.html",
    "href": "Notes/51.5-Week9.html",
    "title": "Week 9 Overview",
    "section": "",
    "text": "More coming soon!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Week 9 Overview"
    ]
  },
  {
    "objectID": "Notes/50-LASSO_Models_Selecting_Models_CV_Test_Set.html",
    "href": "Notes/50-LASSO_Models_Selecting_Models_CV_Test_Set.html",
    "title": "LASSO Models",
    "section": "",
    "text": "We’ve seen that using a training/test split can give us a way to judge our model’s effectiveness on data it wasn’t trained on.\nAlternatively, we’ve seen that cross-validation can be used to judge our model’s effectiveness without doing the training/test split. We might do this if we have a smaller data set.\nA third option is to use both CV and a training/test split. We might do this if we had a number of different classes or families of models we were fitting. We could use CV to choose the best from each family solely on the training data. Then we could compare only the best models from each class on the test set.\nFor instance, we might have the following families of models:\nThe latter three model types all have tuning parameters that must be selected when fitting the model. CV on the training set is often used to select those tuning parameters! We can also use CV on the training set to determine the best MLR model (or use some other model selection technique).\nUsing this process, we would get a best model of each type (or family) using the training set only. Then we can compare the best model from each family of models on the test set to select an overall best model!\nLet’s introduce the LASSO model and then use both CV (on the training set) to select the tuning parameter. We’ll similarly compare some competing MLR models using CV on the training set. Once we have our best LASSO and best MLR model, we’ll then go to the test set to determine an overall best model.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "LASSO Models"
    ]
  },
  {
    "objectID": "Notes/50-LASSO_Models_Selecting_Models_CV_Test_Set.html#fitting-models-on-our-bike_data",
    "href": "Notes/50-LASSO_Models_Selecting_Models_CV_Test_Set.html#fitting-models-on-our-bike_data",
    "title": "LASSO Models",
    "section": "Fitting Models on our bike_data",
    "text": "Fitting Models on our bike_data\nFirst, a training/test split on our bike_data:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nset.seed(10)\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv\")\nbike_data &lt;- bike_data |&gt; \n  mutate(log_selling_price = log(selling_price), \n         log_km_driven = log(km_driven),\n         owners = ifelse(owner == \"1st owner\", \"single\", \"multiple\")) |&gt;\n  select(log_km_driven, log_selling_price, everything())\n#use tidymodel functions for splitting the data\nbike_split &lt;- initial_split(bike_data, prop = 0.7)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\n\nNow we’ll proceed with finding our best LASSO model and best MLR model on just the bike_train data.\n\nSelecting the Best MLR Model Using CV\nAgain, it isn’t necessary to use CV to select an MLR model (there are tons of ways to choose between candidate MLR models - AIC, BIC, backward selection, Mallow’s Cp, …). However, CV is a perfectly valid way to choose between MLR models.\nLet’s compare the following models\n\nMLR model 1: log_selling_price ~ log_km_driven + owners + year (fits different intercept based on owners being “single” or “multiple” and slope terms for log_km_driven and year)\nMLR model 2: log_selling_price ~ log_km_driven + owners + log_km_driven*owners (fits different SLR models between log_selling_price and log_km_driven (different slopes and intercepts) for each setting of owners)\nMLR model 3: log_selling_price ~ (log_km_driven + owners + year)^2 (fits different a model with all pairwise interactions and main effects - essentially two separate MLR models for each setting of owners using log_km_driven and year as predictors)\n\nWe want to just use the training data here and 10 fold CV. First, let’s create our CV folds:\n\n#create folds\nbike_CV_folds &lt;- vfold_cv(bike_train, 10)\n\nRemember to use tidymodels we want to set up our model (and engine) and create our recipes. Those go into a workflow that can then be fit on the above folds.\nLet’s define our basic linear model:\n\n#set up how we'll fit our linear model\nMLR_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nNow define our recipes for the three models. Note: we created the log_km_driven variable earlier. This is ok because this isn’t a learned transform. That is, there is no danger of training/test set data leakage issues since we know how to do the natural log transform, regardless of the data.\n\n#define our MLR models\nMLR_recipe1 &lt;- recipe(log_selling_price ~ log_km_driven + owners + year, \n                      data = bike_train) |&gt;\n  step_dummy(owners)\nMLR_recipe2 &lt;- recipe(log_selling_price ~ log_km_driven + owners,\n                      data = bike_train) |&gt;\n  step_dummy(owners) |&gt;\n  step_interact(~log_km_driven:starts_with(\"owner\"))\n\nMLR_recipe3 &lt;- recipe(log_selling_price ~ log_km_driven + owners + year,\n                      data = bike_train) |&gt;\n  step_dummy(owners) |&gt;\n  step_interact(~log_km_driven:starts_with(\"owner\") + log_km_driven:year + starts_with(\"owner\"):year)\n\nNow we create our workflows for each model:\n\nMLR_wkf1 &lt;- workflow() |&gt;\n  add_recipe(MLR_recipe1) |&gt;\n  add_model(MLR_spec)\n\nMLR_wkf2 &lt;- workflow() |&gt;\n  add_recipe(MLR_recipe2) |&gt;\n  add_model(MLR_spec)\n\nMLR_wkf3 &lt;- workflow() |&gt;\n  add_recipe(MLR_recipe3) |&gt;\n  add_model(MLR_spec)\n\nLet’s fit these models to our CV folds and see how they perform!\n\nMLR_fit1 &lt;-  MLR_wkf1 |&gt;\n  fit_resamples(bike_CV_folds)\n\nMLR_fit2 &lt;- MLR_wkf2 |&gt;\n  fit_resamples(bike_CV_folds) \n\nMLR_fit3 &lt;- MLR_wkf3 |&gt;\n  fit_resamples(bike_CV_folds)\n\nCombine the metrics across the folds and create a final data frame with the results\n\nrbind(MLR_fit1 |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\"),\n      MLR_fit2 |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\"),\n      MLR_fit3 |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\")) |&gt; \n  mutate(Model = c(\"Model 1\", \"Model 2\", \"Model 3\")) |&gt;\n  select(Model, mean, n, std_err)\n\n# A tibble: 3 × 4\n  Model    mean     n std_err\n  &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Model 1 0.494    10  0.0200\n2 Model 2 0.570    10  0.0158\n3 Model 3 0.484    10  0.0180\n\n\nBased on RMSE, we see that the last model is the best MLR model of the three we fit! Note again, we’ve chosen between the three models from the family of models (MLR models) using just CV on the training data.\nLet’s refit that on the entire training set.\n\nMLR_final &lt;-  MLR_wkf3 |&gt;\n  fit(bike_train)\ntidy(MLR_final)\n\n# A tibble: 7 × 5\n  term                            estimate std.error statistic       p.value\n  &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)                    -74.2      87.0        -0.853 0.394        \n2 log_km_driven                    3.42      8.58        0.398 0.691        \n3 year                             0.0442    0.0433      1.02  0.308        \n4 owners_single                 -136.       22.4        -6.09  0.00000000183\n5 log_km_driven_x_owners_single    0.222     0.0696      3.19  0.00148      \n6 log_km_driven_x_year            -0.00190   0.00427    -0.445 0.657        \n7 owners_single_x_year             0.0665    0.0110      6.02  0.00000000273\n\n\n\n\nFit a LASSO Model Using CV\nA similar model to the MLR model that involves a tuning parameter is the LASSO model.\n\nLeast Angle Subset and Selection Operator or LASSO\n\nSimilar to Least Squares but a penalty is placed on the sum of the absolute values of the regression coefficients\nα\\alpha (&gt;0) is called a tuning parameter\n\nminβ′s∑i=1n(yi−(β0+β1x1i+...+βpxpi))2+α∑j=1p|βj|\\min\\limits_{\\beta's}\\sum_{i=1}^{n}(y_i-(\\beta_0+\\beta_1x_{1i}+...+\\beta_px_{pi}))^2 + \\alpha\\sum_{j=1}^{p}|\\beta_j|\n\nSets coefficients to 0 as you ‘shrink’ (have a larger and larger α\\alpha)!\n\n\n\n\n\n\n\n\n\n\n\n\nWhen choosing the tuning parameter (α\\alpha), we are really considering a family of models for a given set of predictors!\n\nConsider the MLR model with different intercepts based on owners that includes both log_km_driven and year as predictors. The plot here gives the coefficient profiles (estimates) as a function of α\\alpha\n\n\n\n\n\n\n\n\n\n\n\nTo read this plot, note that each line represents the value of one of the ‘slope’ coefficients in the model. All the way on the right we have the unconstrained solution (the usual MLR solution). Compare the values there to\n\nlm(log_selling_price ~ log_km_driven + year + owners, data = bike_train)$coef\n\n  (Intercept) log_km_driven          year  ownerssingle \n-134.57077580   -0.24380816    0.07339603   -0.08397016 \n\n\nAs we move to the left on the graph (as α\\alpha increases) we see that our coefficients are shrunk towards 0, eventually being set to 0!\nWe want to choose which level of shrinkage is appropriate. That is, which value of α\\alpha gives us the best model for predicting! A perfect case for using CV!\n\nCreate our Recipe\nLet’s use 10 fold CV to choose our α\\alpha value (i.e. figure out our best LASSO model with these three predictors). As we are penalizing the magnitude of the slope estimates, LASSO models should really be fit on standardized predictors. Let’s include that step in our recipe!\n\n#set up how we'll fit our LASSO model\n#code modified from https://juliasilge.com/blog/lasso-the-office/\nLASSO_recipe &lt;- recipe(log_selling_price ~ log_km_driven + owners + year, \n                      data = bike_train) |&gt;\n  step_dummy(owners) |&gt;\n  step_normalize(log_km_driven, year)\n\n\n\nCreate a Model Instance with tune()\nGreat, now we need to specify a LASSO model. Recall this page allows us to find a type of model to fit along with how to specify it and the engine used to fit the model.\nIn this case, we still want linear_reg() but we want to use the ‘glmnet’ engine. We also want to specify the penalty parameter (corresponds to a form of α\\alpha we mentioned above). “glmnet” actually allows us to fit a more complicated model (the elastic net) so there is a second tuning parameter to deal with (called mixture).\n\nWe set mixture = 1 to turn this into a LASSO model (rather than an elastic net model)\nWe set penalty = tune() to tell tidymodels we are going to use a resampling method to choose this parameter\n\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nSweet - now we create our workflow.\n\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(LASSO_recipe) |&gt;\n  add_model(LASSO_spec)\nLASSO_wkf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nFit the Model with tune_grid() and grid_regular()\nWe saw how to fit a workflow to a set of CV folds with fit_resample(). Since we have a tuning parameter here, we don’t want to use that function. Instead, we use tune_grid(). This function allows us to fit the model to CV folds but specify the set of tuning parameters to consider.\n\nThis implies we are actually doing a bunch of model fits on the CV folds! One for each tuning parameter we specify.\nIn the tune_grid() function we can specify the values of the tuning parameter with the grid = argument.\ngrid_regular() is a function that can be used to choose a grid of reasonable values\n\n\n#A warning will occur for one value of the tuning parameter, safe to ignore\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = bike_CV_folds,\n            grid = grid_regular(penalty(), levels = 200)) \n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nLASSO_grid\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [667/75]&gt; Fold01 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n 2 &lt;split [667/75]&gt; Fold02 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n 3 &lt;split [668/74]&gt; Fold03 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n 4 &lt;split [668/74]&gt; Fold04 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n 5 &lt;split [668/74]&gt; Fold05 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n 6 &lt;split [668/74]&gt; Fold06 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n 7 &lt;split [668/74]&gt; Fold07 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n 8 &lt;split [668/74]&gt; Fold08 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n 9 &lt;split [668/74]&gt; Fold09 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n10 &lt;split [668/74]&gt; Fold10 &lt;tibble [400 × 5]&gt; &lt;tibble [1 × 3]&gt;\n\nThere were issues with some computations:\n\n  - Warning(s) x10: A correlation computation is required, but `estimate` is constant...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nThis may look like it only has one fit per fold but the list columns hide the fact that we actually have fit 200 separate LASSO models (one for each tuning parameter specified by grid_regular()). Notice this:\n\nLASSO_grid[1, \".metrics\"][[1]]\n\n[[1]]\n# A tibble: 400 × 5\n    penalty .metric .estimator .estimate .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard       0.475 Preprocessor1_Model001\n 2 1.12e-10 rmse    standard       0.475 Preprocessor1_Model002\n 3 1.26e-10 rmse    standard       0.475 Preprocessor1_Model003\n 4 1.41e-10 rmse    standard       0.475 Preprocessor1_Model004\n 5 1.59e-10 rmse    standard       0.475 Preprocessor1_Model005\n 6 1.78e-10 rmse    standard       0.475 Preprocessor1_Model006\n 7 2.00e-10 rmse    standard       0.475 Preprocessor1_Model007\n 8 2.25e-10 rmse    standard       0.475 Preprocessor1_Model008\n 9 2.52e-10 rmse    standard       0.475 Preprocessor1_Model009\n10 2.83e-10 rmse    standard       0.475 Preprocessor1_Model010\n# ℹ 390 more rows\n\n\nThis is actually a tibble with 400 metrics (200 rmse values and 200 rsq values).\nOf course, we want to have these metrics computed across the folds for each of our 200 values of the tuning parameter. We use collect_metrics() as before!\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model009\n10 2.83e-10 rmse    standard   0.494    10  0.0199 Preprocessor1_Model010\n# ℹ 190 more rows\n\n\nOk, but it is tough to see the values there. Let’s plot it instead.\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis plot is essentially reversed from the plots we looked at previously. For small values of the penalty (left side of the graph) we have no shrinkage. As our penalty gets bigger we have to shrink our coefficients more (further right on the graph has more shrinkage). Having little to no shrinkage is associated with the lower RMSE values based on our CV results!\n\n\nPull Out the ‘Best’ Model with select_best() and finalize_workflow()\nWe can get the tuning parameter corresponding to the best RMSE value and determine which coefficients that model has using select_best() and finalize_workflow().\n\nlowest_rmse &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\nlowest_rmse\n\n# A tibble: 1 × 2\n  penalty .config               \n    &lt;dbl&gt; &lt;chr&gt;                 \n1  0.0174 Preprocessor1_Model165\n\n\nNow fit that ‘best’ LASSO model on the entire training set. finalize_workflow() tells R to finish our training with a specific setting of the terms we set to tune() in our model definition. We can supply the result from the previous code chunk to get the best model.\n\nLASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.0174263338600965\n  mixture = 1\n\nComputational engine: glmnet \n\n#fit it to the entire training set to see the model fit\nLASSO_final &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  fit(bike_train)\ntidy(LASSO_final)\n\n# A tibble: 4 × 3\n  term          estimate penalty\n  &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    10.7     0.0174\n2 log_km_driven  -0.233   0.0174\n3 year            0.307   0.0174\n4 owners_single  -0.0151  0.0174\n\n\n\n\n\nComparing Chosen Models on the Test Set\nNow we have our best model from each ‘family’ of models (one MLR and one LASSO model). Let’s take them to the test set and see how they perform on this data!\n\nWe can use last_fit() on the bike_split object as we did in the previous section of notes\nThis uses the training set transformations on the test set and does predictions\n\n\nMLR_wkf3 |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.546 Preprocessor1_Model1\n2 rsq     standard       0.483 Preprocessor1_Model1\n\n\n\nLASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.557 Preprocessor1_Model1\n2 rsq     standard       0.473 Preprocessor1_Model1\n\n\nWe see that the MLR model outperforms the LASSO model! This would be our overall best model.\n\nJust to relate this to how we found this previously, note that we can do this ourselves in the usual manner. That is, use predict() and rmse_vec() from yardstick:\n\n\nMLR_final |&gt;\n  predict(bike_test) |&gt;\n  pull() |&gt;\n  rmse_vec(truth = bike_test$log_selling_price)\n\n[1] 0.5460833\n\nLASSO_final |&gt;\n  predict(bike_test) |&gt;\n  pull() |&gt;\n  rmse_vec(truth = bike_test$log_selling_price)\n\n[1] 0.5565673\n\n\n\nAs MLR_final and LASSO_final both have class workflow, using predict() actually uses predict.workflow(). This means it does the appropriate training set transformations prior to predicting on the test set! From the documentation for predict.workflow():\n\n\nThis is the predict() method for a fit workflow object. The nice thing about predicting from a workflow is that it will:\n\n\n\nPreprocess new_data using the preprocessing method specified when the workflow was created and fit. This is accomplished using hardhat::forge(), which will apply any formula preprocessing or call recipes::bake() if a recipe was supplied.\n\n\n\n\nCall parsnip::predict.model_fit() for you using the underlying fit parsnip model.\n\n\n\n\nFinal Step\nNow that we have an overall best model from our set of best models :) We would now refit the best model on the full dataset for future use. Again, we want to apply the transformations laid out previously. However, any transforms that depend on the data should now be based on the full data, not just the training data. By using tidymodels this is taken care of for us! We simply fit() the model with the full data set.\n\nfinal_model &lt;- MLR_wkf3 |&gt;\n  fit(bike_data) \ntidy(final_model)\n\n# A tibble: 7 × 5\n  term                            estimate std.error statistic       p.value\n  &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)                   -127.       79.6        -1.60  0.110        \n2 log_km_driven                    6.56      7.82        0.839 0.402        \n3 year                             0.0709    0.0396      1.79  0.0738       \n4 owners_single                 -126.       20.8        -6.07  0.00000000179\n5 log_km_driven_x_owners_single    0.281     0.0616      4.57  0.00000555   \n6 log_km_driven_x_year            -0.00348   0.00389    -0.895 0.371        \n7 owners_single_x_year             0.0613    0.0103      5.97  0.00000000333\n\n\nIf we want the final model fit in the usual lm form, we can use extract_fit_parsnip()\n\nalmost_usual_fit &lt;- extract_fit_parsnip(final_model)\nusual_fit &lt;- almost_usual_fit$fit\nsummary(usual_fit)\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.82178 -0.34620 -0.05771  0.26085  2.50456 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   -1.274e+02  7.964e+01  -1.600   0.1099    \nlog_km_driven                  6.557e+00  7.817e+00   0.839   0.4018    \nyear                           7.089e-02  3.961e-02   1.790   0.0738 .  \nowners_single                 -1.263e+02  2.081e+01  -6.069 1.79e-09 ***\nlog_km_driven_x_owners_single  2.811e-01  6.157e-02   4.566 5.55e-06 ***\nlog_km_driven_x_year          -3.482e-03  3.889e-03  -0.895   0.3708    \nowners_single_x_year           6.127e-02  1.027e-02   5.965 3.33e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5001 on 1054 degrees of freedom\nMultiple R-squared:  0.5081,    Adjusted R-squared:  0.5053 \nF-statistic: 181.4 on 6 and 1054 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWrap-up\n\nIf you are only considering one type of model, you can use just a training/test set or just use k-fold CV to select the best version of that model\nWhen you have multiple types of models to choose from, we usually use both!\n\nWhen we use the test set too much, we may have ‘data leakage’\nEssentially we end up training our models to the test set by using it too much\nUsing CV with a training/test set helps us avoid this!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "LASSO Models"
    ]
  },
  {
    "objectID": "Notes/50-LASSO_Models_Selecting_Models_CV_Test_Set.html#recap",
    "href": "Notes/50-LASSO_Models_Selecting_Models_CV_Test_Set.html#recap",
    "title": "LASSO Models",
    "section": "Recap",
    "text": "Recap\nCross-validation gives a way to use more of the data while still seeing how the model does on test data\n\nCommonly 5 fold or 10 fold is done\nOnce a best model is chosen, model is refit on entire data set\n\nWe can use CV with or without a training/test split, depending on how much data we have and whether or not we have tuning parameters!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "LASSO Models"
    ]
  },
  {
    "objectID": "Notes/48-Modeling_With_tidymodels_Landing.html",
    "href": "Notes/48-Modeling_With_tidymodels_Landing.html",
    "title": "Modeling with tidymodels",
    "section": "",
    "text": "The video below discusses the tidymodel package and its approach to modeling. This package is a replacement for the caret package that has been widely used previously.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Modeling with `tidymodels`"
    ]
  },
  {
    "objectID": "Notes/48-Modeling_With_tidymodels_Landing.html#notes",
    "href": "Notes/48-Modeling_With_tidymodels_Landing.html#notes",
    "title": "Modeling with tidymodels",
    "section": "Notes",
    "text": "Notes\nNote: The code on slides 15 & 16 of the video differs slightly from the code in the notes below. An older version of the tidymodels package was used in the video and that code no longer works. The code in the slides attached is the way to do this now.\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Modeling with `tidymodels`"
    ]
  },
  {
    "objectID": "Notes/46-Cross_Validation.html",
    "href": "Notes/46-Cross_Validation.html",
    "title": "Cross-Validation",
    "section": "",
    "text": "We’ve discussed the idea of supervised learning. In this paradigm we have a response or target variable we are trying to predict.\nRecall our example of fitting an MLR model from the last section of notes. There we first split the data into a training and test set. We fit the model on the training set. Then we looked at the RMSE on the test set to judge how well the models did.\nRMSE=1ntest∑i=1ntest(yi−ŷi)2RMSE = \\sqrt{\\frac{1}{n_{test}}\\sum_{i=1}^{n_{test}}(y_i-\\hat{y}_i)^2}\nwhere yiy_i is the actual response from the test set and ŷi\\hat{y}_i is the prediction for that response value using the model.\nNote: The terms metric and loss function are often used interchangeably. They are slightly different.\nWe could fit our MLR model using a different loss function (say minimizing the absolute deviations). We could also evaluate our model using a different metric (say the average of the absolute deviations).",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Cross-Validation"
    ]
  },
  {
    "objectID": "Notes/46-Cross_Validation.html#cross-validation-video-introduction",
    "href": "Notes/46-Cross_Validation.html#cross-validation-video-introduction",
    "title": "Cross-Validation",
    "section": "Cross Validation Video Introduction",
    "text": "Cross Validation Video Introduction\nAn alternative to the training test split is to use cross-validation. Check out the video below for a quick introduction to cross-validation. Please pop this video out and watch it in the full panopto player!\n\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Cross-Validation"
    ]
  },
  {
    "objectID": "Notes/46-Cross_Validation.html#cv-without-a-trainingtest-set-split",
    "href": "Notes/46-Cross_Validation.html#cv-without-a-trainingtest-set-split",
    "title": "Cross-Validation",
    "section": "CV without a Training/Test Set Split",
    "text": "CV without a Training/Test Set Split\nSometimes we don’t have a lot of data so we don’t want to split our data into a training and test set. In this setting, CV is often used by itself to select a final model.\nConsider our data set about the selling price of motorcycles:\n\nlibrary(tidyverse)\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv\")\nbike_data &lt;- bike_data |&gt;\n  mutate(log_selling_price = log(selling_price),\n         log_km_driven = log(km_driven)) |&gt;\n  select(log_km_driven, year, log_selling_price, everything())\nbike_data\n\n# A tibble: 1,061 × 9\n   log_km_driven  year log_selling_price name    selling_price seller_type owner\n           &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n 1          5.86  2019             12.1  Royal …        175000 Individual  1st …\n 2          8.64  2017             10.7  Honda …         45000 Individual  1st …\n 3          9.39  2018             11.9  Royal …        150000 Individual  1st …\n 4         10.0   2015             11.1  Yamaha…         65000 Individual  1st …\n 5          9.95  2011              9.90 Yamaha…         20000 Individual  2nd …\n 6         11.0   2010              9.80 Honda …         18000 Individual  1st …\n 7          9.74  2018             11.3  Honda …         78500 Individual  1st …\n 8         10.6   2008             12.1  Royal …        180000 Individual  2nd …\n 9         10.4   2010             10.3  Hero H…         30000 Individual  1st …\n10         10.6   2016             10.8  Bajaj …         50000 Individual  1st …\n# ℹ 1,051 more rows\n# ℹ 2 more variables: km_driven &lt;dbl&gt;, ex_showroom_price &lt;dbl&gt;\n\n\n\nLet’s consider our three linear regression models for the log_selling_price\n\nModel 1: log_selling_price = intercept + slope*year + Error\\mbox{Model 1: log_selling_price = intercept + slope*year + Error}\nModel 2: log_selling_price = intercept + slope*log_km_driven + Error\\mbox{Model 2: log_selling_price = intercept + slope*log_km_driven + Error}\nModel 3: log_selling_price = intercept + slope*log_km_driven + slope*year + Error\\mbox{Model 3: log_selling_price = intercept + slope*log_km_driven + slope*year + Error}\n\nWe can use CV error to choose between these models without doing a training/test split.\nWe’ll see how to do this within the tidymodels framework shortly. For now, let’s do it by hand so we can make sure we understand the process CV uses!\n\n\nImplementing 10 fold Cross-Validation Ourselves\nLet’s start with dividing the data into separate (distinct) folds.\n\nSplit the data into 10 separate folds (subsets)\n\nWe can do this by randomly reordering our observations and taking the first ten percent to be the first fold, 2nd ten percent to be the second fold, and so on.\n\n\n\nnrow(bike_data)\n\n[1] 1061\n\nsize_fold &lt;- floor(nrow(bike_data)/10)\n\n\n\n\nEach fold will have 106 observations. There will be an extra observation in this case, we’ll lump that into our last fold.\n\n\nLet’s get our folds by using sample() to reorder the indices.\n\n\nThen we’ll use a for loop to cycle through the pieces and save the folds in a list\n\n\n\n\nset.seed(8)\nrandom_indices &lt;- sample(1:nrow(bike_data), size = nrow(bike_data), replace = FALSE)\n#see the random reordering\nhead(random_indices)\n\n[1] 591  12 631 899 867  86\n\n#create a list to save our folds in\nfolds &lt;- list()\n#now cycle through our random indices vector and take the appropriate observations to each fold\nfor(i in 1:10){\n  if (i &lt; 10) {\n    fold_index &lt;- seq(from = (i-1)*size_fold +1, to = i*size_fold, by = 1)\n    folds[[i]] &lt;- bike_data[random_indices[fold_index], ]\n  } else {\n    fold_index &lt;- seq(from = (i-1)*size_fold +1, to = length(random_indices), by = 1)\n    folds[[i]] &lt;- bike_data[random_indices[fold_index], ]\n  }\n}\nfolds[[1]]\n\n# A tibble: 106 × 9\n   log_km_driven  year log_selling_price name    selling_price seller_type owner\n           &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n 1         10.1   2014             10.3  Honda …         30000 Individual  1st …\n 2          9.21  2016             10.2  Honda …         28000 Individual  2nd …\n 3         11.5   2007              9.21 Hero H…         10000 Individual  1st …\n 4         10.7   2015             10.3  Honda …         30000 Individual  2nd …\n 5          9.40  2014             11.6  Royal …        110000 Individual  1st …\n 6         10.8   2009             10.1  Hero H…         25000 Individual  1st …\n 7          9.62  2018             10.5  Hero H…         35000 Individual  1st …\n 8         10.3   2016             11.2  Bajaj …         70000 Individual  1st …\n 9          9.90  2015             10.8  Yamaha…         50000 Individual  1st …\n10          9.71  2017             11.8  KTM 25…        135000 Individual  1st …\n# ℹ 96 more rows\n# ℹ 2 more variables: km_driven &lt;dbl&gt;, ex_showroom_price &lt;dbl&gt;\n\nfolds[[2]]\n\n# A tibble: 106 × 9\n   log_km_driven  year log_selling_price name    selling_price seller_type owner\n           &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n 1         10.7   2013             10.2  Bajaj …         28000 Individual  1st …\n 2          9.21  2018             10.7  Honda …         46000 Individual  1st …\n 3          9.68  2016             11.0  Suzuki…         60000 Individual  1st …\n 4          9.38  2017             11.6  Honda …        110000 Individual  1st …\n 5          8.92  2018             11.1  Hero S…         65000 Individual  1st …\n 6          8.99  2018             11.4  Bajaj …         87000 Individual  1st …\n 7         11.0   2010              9.90 Bajaj …         20000 Individual  1st …\n 8         10.8   2010              9.90 Hero  …         20000 Individual  1st …\n 9          9.85  2016             10.5  TVS Ju…         35000 Individual  1st …\n10         10.6   2017             11.0  Bajaj …         60000 Individual  2nd …\n# ℹ 96 more rows\n# ℹ 2 more variables: km_driven &lt;dbl&gt;, ex_showroom_price &lt;dbl&gt;\n\n\n\n\n\nLet’s put this process into our own function for splitting our data up!\n\n\nThis function will take in a data set and a number of folds (num_folds) and returns a list with the folds of the data.\n\n\n\n\nget_cv_splits &lt;- function(data, num_folds){\n  #get fold size\n  size_fold &lt;- floor(nrow(data)/num_folds)\n  #get random indices to subset the data with\n  random_indices &lt;- sample(1:nrow(data), size = nrow(data), replace = FALSE)\n  #create a list to save our folds in\n  folds &lt;- list()\n  #now cycle through our random indices vector and take the appropriate observations to each fold\n  for(i in 1:num_folds){\n    if (i &lt; num_folds) {\n      fold_index &lt;- seq(from = (i-1)*size_fold +1, to = i*size_fold, by = 1)\n      folds[[i]] &lt;- data[random_indices[fold_index], ]\n    } else {\n      fold_index &lt;- seq(from = (i-1)*size_fold +1, to = length(random_indices), by = 1)\n      folds[[i]] &lt;- data[random_indices[fold_index], ]\n    }\n  }\n  return(folds)\n}\nfolds &lt;- get_cv_splits(bike_data, 10)\n\n\nNow we can fit the data on nine of the folds and test on the tenth. Then switch which fold is left out and repeat the process. We’ll use MSE rather than RMSE as our metric for now.\n\n\n#set the test fold number\ntest_number &lt;- 10\n#pull out the testing fold\ntest &lt;- folds[[test_number]]\ntest\n\n# A tibble: 107 × 9\n   log_km_driven  year log_selling_price name    selling_price seller_type owner\n           &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n 1         10.3   2015              10.1 Honda …         25000 Individual  1st …\n 2         10.4   2016              10.5 Hero H…         35000 Individual  1st …\n 3         10.1   2018              11.0 Yamaha…         60000 Individual  1st …\n 4         10.4   2013              10.6 Bajaj …         40000 Individual  1st …\n 5          7.57  2017              12.0 Mahind…        165000 Individual  1st …\n 6          9.90  2019              11.4 TVS Ap…         90000 Individual  1st …\n 7          8.52  2017              11.1 Bajaj …         65000 Individual  1st …\n 8          9.17  2018              11.5 TVS Ap…         95000 Individual  1st …\n 9          8.61  2016              10.5 Hero G…         38000 Dealer      1st …\n10         10.1   2015              10.7 Yamaha…         45000 Individual  1st …\n# ℹ 97 more rows\n# ℹ 2 more variables: km_driven &lt;dbl&gt;, ex_showroom_price &lt;dbl&gt;\n\n#remove the test fold from the list\nfolds[[test_number]] &lt;- NULL\n#note that folds is only length 9 now\nlength(folds)\n\n[1] 9\n\n\n\n\n\nRecall the purrr:reduce() function. This allows us to iteratively apply a function across an object. Here we’ll use reduce() with rbind() to combine the tibbles saved in the different folds.\n\n\n\n\n#combine the other folds into train set\ntrain &lt;- purrr::reduce(folds, rbind)\n#the other nine folds are now in train\ntrain\n\n# A tibble: 954 × 9\n   log_km_driven  year log_selling_price name    selling_price seller_type owner\n           &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n 1         11.2   2012              10.7 Hero H…         45000 Individual  1st …\n 2         10.4   2014              10.5 Hero I…         35000 Individual  1st …\n 3          9.62  2017              11.8 Bajaj …        138000 Individual  1st …\n 4          8.85  2017              11.9 Royal …        150000 Individual  1st …\n 5         10.6   2010              10.5 Bajaj …         35000 Individual  1st …\n 6          8.85  2018              11.2 Honda …         70000 Individual  1st …\n 7          8.07  2019              11.4 Hero X…         87000 Individual  1st …\n 8          9.80  2017              11.2 Yamaha…         75000 Individual  1st …\n 9          9.85  2014              11.5 Honda …        100000 Individual  1st …\n10          9.96  2018              11.3 Bajaj …         80000 Individual  1st …\n# ℹ 944 more rows\n# ℹ 2 more variables: km_driven &lt;dbl&gt;, ex_showroom_price &lt;dbl&gt;\n\n\n\n\n\nNow we can fit our model and check the metric on the test set\n\n\n\n\nfit &lt;- lm(log_selling_price ~ log_km_driven, data = train)\n#get test error (MSE - square RMSE)\n(yardstick::rmse_vec(test[[\"log_selling_price\"]], \n                     predict(fit, newdata = test)))^2\n\n[1] 0.3318796\n\n\n\n\n\nOk, let’s wrap that into a function for ease! Let y be a string for the response, x a character vector of predictors, and test_number the fold to test on.\n\n\n\n\nfind_test_metric &lt;- function(y, x, folds, test_number){\n  #pull out the testing fold\n  test &lt;- folds[[test_number]]\n  #remove the test fold from the list\n  folds[[test_number]] &lt;- NULL\n  #combine the other folds into train set\n  train &lt;- purrr::reduce(folds, rbind)\n  #fit our model. reformulate allows us to take strings and turn that into a formula\n  fit &lt;- lm(reformulate(termlabels = x, response = y), data = train)\n  #get test error\n  (yardstick::rmse_vec(test[[y]], predict(fit, newdata = test)))^2\n}\n\n\n\n\nWe can use the purrr::map() function to apply this function to 1:10. This will use each fold as the test set once.\n\n\n\n\nfolds &lt;- get_cv_splits(bike_data, 10)\npurrr::map(1:10, find_test_metric, \n           y = \"log_selling_price\", \n           x = \"log_km_driven\", \n           folds = folds)\n\n[[1]]\n[1] 0.2829525\n\n[[2]]\n[1] 0.3260931\n\n[[3]]\n[1] 0.4689752\n\n[[4]]\n[1] 0.3097491\n\n[[5]]\n[1] 0.3006277\n\n[[6]]\n[1] 0.3152691\n\n[[7]]\n[1] 0.4214135\n\n[[8]]\n[1] 0.2991941\n\n[[9]]\n[1] 0.3496436\n\n[[10]]\n[1] 0.4744565\n\n\n\nNow we combine these into one value! We’ll average the MSE values across the folds and then take the square root of that to obtain the RMSE (averaging square roots makes less sense, which is why we found MSE first)\n\nWe can again use reduce() to combine all of these values into one number! We average it by dividing by the number of folds and finally find the square root to make it the RMSE!\n\n\n\nsum_mse &lt;- purrr::map(1:10, \n                      find_test_metric, \n                      y = \"log_selling_price\", \n                      x = \"log_km_driven\", \n                      folds = folds) |&gt;\n  reduce(.f = sum)\nsqrt(sum_mse/10)\n\n[1] 0.5956823\n\n\n\n\n\nLet’s put it into a function and make it generic for the number of folds we have.\n\n\n\n\nfind_cv &lt;- function(y, x, folds, num_folds){\n  sum_mse &lt;- purrr::map(1:num_folds, \n                        find_test_metric, \n                        y = y, \n                        x =x, \n                        folds = folds) |&gt;\n    reduce(.f = sum)\n  return(sqrt(sum_mse/num_folds))\n}\nfind_cv(\"log_selling_price\", \"log_km_driven\", folds, num_folds = 10)\n\n[1] 0.5956823\n\n\n\n\nCompare Different Models Using Our CV Functions\nNow let’s compare our three separate models via 10 fold CV! We can use the same folds with different models.\n\nFirst our model with just year as a predictor:\n\n\nfolds &lt;- get_cv_splits(bike_data, 10)\ncv_reg_1 &lt;- find_cv(\"log_selling_price\", \"year\", folds, num_folds = 10)\n\n\nNow for our model with just log_km_driven:\n\n\ncv_reg_2 &lt;- find_cv(\"log_selling_price\", \"log_km_driven\", folds, num_folds = 10)\n\n\nAnd for the model with both of these (main) effects:\n\n\ncv_reg_3 &lt;- find_cv(\"log_selling_price\", c(\"year\", \"log_km_driven\"), folds, num_folds = 10)\n\nWe can then use these CV scores to compare across models and choose the ‘best’ one!\n\nc(\"year\" = cv_reg_1, \"log_km_driven\" = cv_reg_2, \"both\" = cv_reg_3)\n\n         year log_km_driven          both \n    0.5508152     0.5956502     0.5135431 \n\n\nWe see that the model that has both predictors has the lowest cross-validation RMSE!\nWe would now fit this ‘best’ model on the full data set!\n\nbest_fit &lt;- lm(log_selling_price ~ year + log_km_driven, data = bike_data)\nbest_fit\n\n\nCall:\nlm(formula = log_selling_price ~ year + log_km_driven, data = bike_data)\n\nCoefficients:\n  (Intercept)           year  log_km_driven  \n   -148.79329        0.08034       -0.22686",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Cross-Validation"
    ]
  },
  {
    "objectID": "Notes/46-Cross_Validation.html#cv-recap",
    "href": "Notes/46-Cross_Validation.html#cv-recap",
    "title": "Cross-Validation",
    "section": "CV Recap",
    "text": "CV Recap\nNice! We’ve done a comparison of models on how well they predict on data they are not trained on without splitting our data into a train and test set! This is very useful when we don’t have a lot of data.\nAgain, we would choose our best model (model 3 here) and refit the model on the full data set! We’ll see how to do this in the tidymodels framework shortly\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Cross-Validation"
    ]
  },
  {
    "objectID": "Notes/44-Modeling_Concepts_Inference_Prediction_Landing.html",
    "href": "Notes/44-Modeling_Concepts_Inference_Prediction_Landing.html",
    "title": "Modeling Concepts",
    "section": "",
    "text": "The video below discusses the paradigm of statistical learning.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Modeling Concepts"
    ]
  },
  {
    "objectID": "Notes/44-Modeling_Concepts_Inference_Prediction_Landing.html#notes",
    "href": "Notes/44-Modeling_Concepts_Inference_Prediction_Landing.html#notes",
    "title": "Modeling Concepts",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Modeling Concepts"
    ]
  },
  {
    "objectID": "Notes/43-RShiny_Articles_II.html",
    "href": "Notes/43-RShiny_Articles_II.html",
    "title": "Useful R Shiny Articles II",
    "section": "",
    "text": "Lastly, we should cover the idea of isolating reactive inputs so that our code won’t react to them!\nPlease read the following article:\n\nStop reactions with isolate()\n\nThen head back here for the next content!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Useful R Shiny Articles II"
    ]
  },
  {
    "objectID": "Notes/41-RShiny_Articles_I.html",
    "href": "Notes/41-RShiny_Articles_I.html",
    "title": "Useful R Shiny Articles I",
    "section": "",
    "text": "Sharing your app is really important. We’ve talked about the basics of this but should discuss it in more detail.\nPlease read the following two articles:\n\nGetting started with shinyapps.io\nSharing apps to run locally\n\nThen head back here for the next content!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Useful R Shiny Articles I"
    ]
  },
  {
    "objectID": "Notes/39-RShiny_Dynamic_UI_Landing.html",
    "href": "Notes/39-RShiny_Dynamic_UI_Landing.html",
    "title": "Dynamic User Interfaces",
    "section": "",
    "text": "The video below dives into how to make your shiny app appearance more dynamic. That is, how can we change which widgets show? How do we update a widget? How do we hide part of our app?\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Dynamic User Interfaces"
    ]
  },
  {
    "objectID": "Notes/39-RShiny_Dynamic_UI_Landing.html#notes",
    "href": "Notes/39-RShiny_Dynamic_UI_Landing.html#notes",
    "title": "Dynamic User Interfaces",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Dynamic User Interfaces"
    ]
  },
  {
    "objectID": "Notes/37-RShiny_Reactivity.html",
    "href": "Notes/37-RShiny_Reactivity.html",
    "title": "Reactivity",
    "section": "",
    "text": "The video below goes deeper into the important concept of ‘reactivity’ or the re-evaluation of R code. This is the cornerstone of what makes a shiny app a shiny app!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Reactivity"
    ]
  },
  {
    "objectID": "Notes/37-RShiny_Reactivity.html#notes",
    "href": "Notes/37-RShiny_Reactivity.html#notes",
    "title": "Reactivity",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Reactivity"
    ]
  },
  {
    "objectID": "Notes/35-RShiny_Connecting_UI_Server_Landing.html",
    "href": "Notes/35-RShiny_Connecting_UI_Server_Landing.html",
    "title": "Connecting the UI and Server",
    "section": "",
    "text": "The video below discusses how to get the user interface and server to ‘talk’ in a shiny app.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Connecting the UI and Server"
    ]
  },
  {
    "objectID": "Notes/35-RShiny_Connecting_UI_Server_Landing.html#notes",
    "href": "Notes/35-RShiny_Connecting_UI_Server_Landing.html#notes",
    "title": "Connecting the UI and Server",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Connecting the UI and Server"
    ]
  },
  {
    "objectID": "Notes/33-RShiny_Introduction_Landing.html",
    "href": "Notes/33-RShiny_Introduction_Landing.html",
    "title": "Introduction to R Shiny",
    "section": "",
    "text": "The video below introduces the shiny package. This package allows for us to make interactive web-based applications that can run R code dynamically! These can be great ways to allow users to explore your data, model, etc.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Introduction to R Shiny"
    ]
  },
  {
    "objectID": "Notes/33-RShiny_Introduction_Landing.html#notes",
    "href": "Notes/33-RShiny_Introduction_Landing.html#notes",
    "title": "Introduction to R Shiny",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Introduction to R Shiny"
    ]
  },
  {
    "objectID": "Notes/32-Advanced_Function_Writing_Landing.html",
    "href": "Notes/32-Advanced_Function_Writing_Landing.html",
    "title": "Advanced Function Writing",
    "section": "",
    "text": "The video below goes further into function writing. We discuss important topics that allow you to create functions that can be used in chaining and other fun things!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Advanced Function Writing"
    ]
  },
  {
    "objectID": "Notes/32-Advanced_Function_Writing_Landing.html#notes",
    "href": "Notes/32-Advanced_Function_Writing_Landing.html#notes",
    "title": "Advanced Function Writing",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Advanced Function Writing"
    ]
  },
  {
    "objectID": "Notes/30-Apply_Family_Landing.html",
    "href": "Notes/30-Apply_Family_Landing.html",
    "title": "Apply Family of Functions",
    "section": "",
    "text": "The video below discusses the apply family of functions. This is a set of Base R functions that allow us to ‘apply’ functions to different types of objects. These functions are often able to replace loops and make code more readable (and more efficient generally).\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Apply Family of Functions"
    ]
  },
  {
    "objectID": "Notes/30-Apply_Family_Landing.html#notes",
    "href": "Notes/30-Apply_Family_Landing.html#notes",
    "title": "Apply Family of Functions",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Apply Family of Functions"
    ]
  },
  {
    "objectID": "Notes/29.1-Week6.html",
    "href": "Notes/29.1-Week6.html",
    "title": "Week 6 Overview",
    "section": "",
    "text": "Welcome to week 6! Please see Moodle for your required videos.\nThis week we’ll start off with a recap, and then introduce the apply() functions from Base R, and the (mostly) equivalent tidyverse options from the purrr package. We will then get into advanced function writing and querying APIs.",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Week 6 Overview"
    ]
  },
  {
    "objectID": "Notes/29.1-Week6.html#week-6-additional-readingslearning-materials",
    "href": "Notes/29.1-Week6.html#week-6-additional-readingslearning-materials",
    "title": "Week 6 Overview",
    "section": "Week 6 Additional Readings/Learning Materials",
    "text": "Week 6 Additional Readings/Learning Materials\n\napply() family\nAdvanced R: Chapter 9.7\n\n\npurrr\nR for Data Science: Chapter 26.3.3\n\n\nAdvanced Function Writing\nR for Data Science: Chapter 19\nUpon completion of this week, students will be able to:\n\nquery APIs to return appropriate data (CO 2)\n\ndefine the term API\nexplain the common syntax often used for APIs\n\nhave a base understanding of the common apply() family of functions\n\nhave a working understanding on how to use common apply functions\nunderstand when to use which apply function\n\nhave a base understanding of the purrr package\n\nhave a working understanding on how to use common map functions\nunderstand when it may be useful to use functions from the purrr pacakge vs Base R\n\nImprove function writing skills\n\nhave a working understanding of how to leverage … into our function writing\nhow to use stops in function writing\nand more!",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Week 6 Overview"
    ]
  },
  {
    "objectID": "Notes/28-Numeric_Variable_Graphs_Landing.html",
    "href": "Notes/28-Numeric_Variable_Graphs_Landing.html",
    "title": "Numeric Variable Graphs",
    "section": "",
    "text": "The video below discusses how to create graphs for numeric variables using ggplot2.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Numeric Variable Graphs"
    ]
  },
  {
    "objectID": "Notes/28-Numeric_Variable_Graphs_Landing.html#notes",
    "href": "Notes/28-Numeric_Variable_Graphs_Landing.html#notes",
    "title": "Numeric Variable Graphs",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Numeric Variable Graphs"
    ]
  },
  {
    "objectID": "Notes/26-Barplots_ggplot_Basics_Landing.html",
    "href": "Notes/26-Barplots_ggplot_Basics_Landing.html",
    "title": "Barplots & ggplot2 Basics",
    "section": "",
    "text": "The video below discusses the course goals.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Barplots & `ggplot2` Basics"
    ]
  },
  {
    "objectID": "Notes/26-Barplots_ggplot_Basics_Landing.html#notes",
    "href": "Notes/26-Barplots_ggplot_Basics_Landing.html#notes",
    "title": "Barplots & ggplot2 Basics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Barplots & `ggplot2` Basics"
    ]
  },
  {
    "objectID": "Notes/24-EDA_Concepts_Landing.html",
    "href": "Notes/24-EDA_Concepts_Landing.html",
    "title": "EDA Concepts",
    "section": "",
    "text": "The video below discusses concepts around exploratory data analysis (EDA). This is a vital step that you’ll do almost anytime you have data! It is a bit of an art form and takes some subject matter expertise to do really well but we’ll talk about the most common steps.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "EDA Concepts"
    ]
  },
  {
    "objectID": "Notes/24-EDA_Concepts_Landing.html#notes",
    "href": "Notes/24-EDA_Concepts_Landing.html#notes",
    "title": "EDA Concepts",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "EDA Concepts"
    ]
  },
  {
    "objectID": "Notes/22-SQL_Style_Joins_Landing.html",
    "href": "Notes/22-SQL_Style_Joins_Landing.html",
    "title": "SQL Joins",
    "section": "",
    "text": "The video below discusses SQL style joins. Recall that when we have a database we often have multiple tables (or data frames) that are linked by some column or columns. When that is the case, we may want to combine the tables into one. That is the idea of a join!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "SQL Joins"
    ]
  },
  {
    "objectID": "Notes/22-SQL_Style_Joins_Landing.html#notes",
    "href": "Notes/22-SQL_Style_Joins_Landing.html#notes",
    "title": "SQL Joins",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "SQL Joins"
    ]
  },
  {
    "objectID": "Notes/20-Manipulating_Data_with_tidyr.html",
    "href": "Notes/20-Manipulating_Data_with_tidyr.html",
    "title": "Manipulating Data with tidyr",
    "section": "",
    "text": "We now have a good handle on common actions we want to take on our data frames. However, we’ve been treating our data as though it is already in long format where each row consists of one observation and each column one variable.\nThis isn’t always the case! Sometimes we have wide format data where we may have more than one observation in a given row.\nYou might see wide data if you deal with pivot tables in excel. It is often a nicer way to display data, but almost all of the plotting, summarizing, and modeling we do in statistics expects data to be in long form. Luckily, the tidyr package gives us nice functionality for switching between these two forms!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Manipulating Data with `tidyr`"
    ]
  },
  {
    "objectID": "Notes/20-Manipulating_Data_with_tidyr.html#tidyr-package",
    "href": "Notes/20-Manipulating_Data_with_tidyr.html#tidyr-package",
    "title": "Manipulating Data with tidyr",
    "section": "tidyr Package",
    "text": "tidyr Package\nThis pacakge allows us to easily manipulate data via\n\npivot_longer() - lengthens data by increasing the number of rows and decreasing the number of columns\n\nMost important as analysis methods often prefer this form\n\npivot_wider() - widens data by increasing the number of columns and decreasing the number of rows\n\nWe’ll also look at a couple other functions from tidyr that can be useful.\n\npivot_longer()\nConsider the data set called cityTemps.txt available via the URL below.\n\nlibrary(readr)\ntemps_data &lt;- read_table(file = \"https://www4.stat.ncsu.edu/~online/datasets/cityTemps.txt\") \n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  city = col_character(),\n  sun = col_double(),\n  mon = col_double(),\n  tue = col_double(),\n  wed = col_double(),\n  thr = col_double(),\n  fri = col_double(),\n  sat = col_double()\n)\n\ntemps_data\n\n# A tibble: 6 × 8\n  city        sun   mon   tue   wed   thr   fri   sat\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 atlanta      81    87    83    79    88    91    94\n2 baltimore    73    75    70    78    73    75    79\n3 charlotte    82    80    75    82    83    88    93\n4 denver       72    71    67    68    72    71    58\n5 ellington    51    42    47    52    55    56    59\n6 frankfort    70    70    72    70    74    74    79\n\n\nThis data is in wide format as more than one observation on a city is in each row.\n\nSwitch to ‘Long’ form with pivot_longer(). Checking the help, the major arguments are:\n\ncols = columns to pivot to longer format (cols = 2:8)\n\nnames_to = new name(s) for columns created (names_to = \"day\")\n\nvalues_to = new name(s) for data values (values_to = \"temp\")\n\n\n\nlibrary(tidyr)\ntemps_data |&gt;\n  pivot_longer(cols = 2:8, \n               names_to = \"day\", \n               values_to = \"temp\")\n\n# A tibble: 42 × 3\n   city      day    temp\n   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n 1 atlanta   sun      81\n 2 atlanta   mon      87\n 3 atlanta   tue      83\n 4 atlanta   wed      79\n 5 atlanta   thr      88\n 6 atlanta   fri      91\n 7 atlanta   sat      94\n 8 baltimore sun      73\n 9 baltimore mon      75\n10 baltimore tue      70\n# ℹ 32 more rows\n\n\nThat’s better! Now each row has one observation in it. Recall we had a lot of functionality for selecting columns within the tidyverse. That holds here as well!\n\ntemps_data |&gt;\n  pivot_longer(cols = sun:sat, \n               names_to = \"day\", \n               values_to = \"temp\")\n\n# A tibble: 42 × 3\n   city      day    temp\n   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n 1 atlanta   sun      81\n 2 atlanta   mon      87\n 3 atlanta   tue      83\n 4 atlanta   wed      79\n 5 atlanta   thr      88\n 6 atlanta   fri      91\n 7 atlanta   sat      94\n 8 baltimore sun      73\n 9 baltimore mon      75\n10 baltimore tue      70\n# ℹ 32 more rows\n\n\n\n\npivot_wider()\nOccasionally we’ll want to make our data wider for display purposes. We can make this switch to ‘Wide’ form with pivot_wider(). There are two major arguments we must specify:\n\nnames_from = column(s) to get the names used in the output columns\nvalues_from = column(s) to get the cell values from\n\nLet’s consider our batting data set from the dplyr notes.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(Lahman)\n\nWarning: package 'Lahman' was built under R version 4.4.2\n\nbatting_tbl &lt;- as_tibble(Batting)\nbatting_tbl\n\n# A tibble: 113,799 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n 2 aardsda01   2006     1 CHN    NL       45     2     0     0     0     0     0\n 3 aardsda01   2007     1 CHA    AL       25     0     0     0     0     0     0\n 4 aardsda01   2008     1 BOS    AL       47     1     0     0     0     0     0\n 5 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n 6 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n 7 aardsda01   2012     1 NYA    AL        1     0     0     0     0     0     0\n 8 aardsda01   2013     1 NYN    NL       43     0     0     0     0     0     0\n 9 aardsda01   2015     1 ATL    NL       33     1     0     0     0     0     0\n10 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n# ℹ 113,789 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\nWe may want to get just the data for one team (say the Pirates) and display each players number of hits across the years 2018 to 2020.\n\nLet’s subset the data to get just the pirates (teamID == \"PIT\")\nThen we’ll select only their hits and year columns (playerID, H, and yearID)\nThen we need to pivot that data set wider so that we have the year across the top (names_from), the players as the rows, and the entries as the hits (values_from)\n\n\nbatting_tbl |&gt;\n  filter(yearID %in% 2018:2020, teamID == \"PIT\") |&gt;\n  select(playerID, yearID, H) |&gt;\n  pivot_wider(names_from = yearID, values_from = \"H\")\n\n# A tibble: 96 × 4\n   playerID  `2019` `2020` `2018`\n   &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 agrazda01      2     NA     NA\n 2 alforan01     NA      3     NA\n 3 anderta01     NA     NA      0\n 4 archech01      4     NA      2\n 5 baronst01      2     NA     NA\n 6 bashlty01     NA      0     NA\n 7 belljo02     146     44    131\n 8 bostich01     NA     NA      0\n 9 braulst01     14      0      3\n10 brubajt01     NA      0     NA\n# ℹ 86 more rows\n\n\nGreat! You can see that missing values are filled in for those that didn’t play in a given year. Let’s subset this to remove any rows with missing values (so we only get players that played for the pirates in all three years).\nThe tidyr function drop_na() does this exact thing for us!\n\nbatting_tbl |&gt;\n  filter(yearID %in% 2018:2020, teamID == \"PIT\") |&gt;\n  select(playerID, yearID, H) |&gt;\n  pivot_wider(names_from = yearID, values_from = \"H\") |&gt;\n  drop_na()\n\n# A tibble: 17 × 4\n   playerID  `2019` `2020` `2018`\n   &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 belljo02     146     44    131\n 2 braulst01     14      0      3\n 3 burdini01      0      0      0\n 4 crickky01      0      0      0\n 5 felizmi01      0      0      0\n 6 fraziad01    154     48     88\n 7 holmecl01      0      0      0\n 8 kelake01       0      0      0\n 9 moranco01    129     44    115\n10 musgrjo01      8      0      5\n11 neverdo01      0      0      0\n12 newmake01    152     35     19\n13 osunajo01     69     16     24\n14 polangr01     37     24    117\n15 rodriri05      0      0      0\n16 stallja01     50     31      8\n17 willitr01      6      0      5\n\n\nLet’s also remove those with 0 hits:\n\nbatting_tbl |&gt;\n  filter(yearID %in% 2018:2020, teamID == \"PIT\", H &gt; 0) |&gt;\n  select(playerID, yearID, H) |&gt;\n  pivot_wider(names_from = yearID, values_from = \"H\") |&gt;\n  drop_na()\n\n# A tibble: 7 × 4\n  playerID  `2019` `2020` `2018`\n  &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1 belljo02     146     44    131\n2 fraziad01    154     48     88\n3 moranco01    129     44    115\n4 newmake01    152     35     19\n5 osunajo01     69     16     24\n6 polangr01     37     24    117\n7 stallja01     50     31      8\n\n\nThe column names 2018, 2019, and 2020 are non-standard names as they start with a number. That’s not ideal so let’s rename those too!\n\nbatting_tbl |&gt;\n  filter(yearID %in% 2018:2020, teamID == \"PIT\", H &gt; 0) |&gt;\n  select(playerID, yearID, H) |&gt;\n  pivot_wider(names_from = yearID, values_from = \"H\") |&gt;\n  drop_na() |&gt;\n  rename('year2018' = `2018`,\n         'year2019' = `2019`,\n         'year2020' = `2020`)\n\n# A tibble: 7 × 4\n  playerID  year2019 year2020 year2018\n  &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n1 belljo02       146       44      131\n2 fraziad01      154       48       88\n3 moranco01      129       44      115\n4 newmake01      152       35       19\n5 osunajo01       69       16       24\n6 polangr01       37       24      117\n7 stallja01       50       31        8\n\n\nWould be better with actual player names (we’ll learn about how to combine this data set with another one that has their actual names soon!)\n\nbatting_tbl |&gt;\n  filter(yearID %in% 2018:2020, teamID == \"PIT\", H &gt; 0) |&gt;\n  select(playerID, yearID, H) |&gt;\n  pivot_wider(names_from = yearID, values_from = \"H\") |&gt;\n  drop_na() |&gt;\n  rename('year2018' = `2018`,\n         'year2019' = `2019`,\n         'year2020' = `2020`) |&gt;\n  dplyr::inner_join(select(People, playerID, nameFirst, nameLast)) |&gt;\n  select(nameFirst, nameLast, everything())\n\nJoining with `by = join_by(playerID)`\n\n\n# A tibble: 7 × 6\n  nameFirst nameLast  playerID  year2019 year2020 year2018\n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n1 Josh      Bell      belljo02       146       44      131\n2 Adam      Frazier   fraziad01      154       48       88\n3 Colin     Moran     moranco01      129       44      115\n4 Kevin     Newman    newmake01      152       35       19\n5 Jose      Osuna     osunajo01       69       16       24\n6 Gregory   Polanco   polangr01       37       24      117\n7 Jacob     Stallings stallja01       50       31        8\n\n\n\n\nColumn Manipulations with tidyr\n\nSeparate a column using separate_wider_delim() (a few other variants exist as well)\nCombine two columns with unite()\n\n\n\n\n\n\n\n\n\n\n\nConsider data set on air pollution in Chicago\n\n\nchicago_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/Chicago.csv\")\n\nRows: 1461 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): city, date, season\ndbl (8): X, death, temp, dewpoint, pm10, o3, time, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nchicago_data\n\n# A tibble: 1,461 × 11\n       X city  date      death  temp dewpoint  pm10    o3  time season  year\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  3654 chic  1/1/1997    137  36      37.5  13.1   5.66  3654 winter  1997\n 2  3655 chic  1/2/1997    123  45      47.2  41.9   5.53  3655 winter  1997\n 3  3656 chic  1/3/1997    127  40      38    27.0   6.29  3656 winter  1997\n 4  3657 chic  1/4/1997    146  51.5    45.5  25.1   7.54  3657 winter  1997\n 5  3658 chic  1/5/1997    102  27      11.2  15.3  20.8   3658 winter  1997\n 6  3659 chic  1/6/1997    127  17       5.75  9.36 14.9   3659 winter  1997\n 7  3660 chic  1/7/1997    116  16       7    20.2  11.9   3660 winter  1997\n 8  3661 chic  1/8/1997    118  19      17.8  33.1   8.68  3661 winter  1997\n 9  3662 chic  1/9/1997    148  26      24    12.1  13.4   3662 winter  1997\n10  3663 chic  1/10/1997   121  16       5.38 24.8  10.4   3663 winter  1997\n# ℹ 1,451 more rows\n\n\n\nAlthough we saw that we should treat date variables as date objects (say from lubridate), we could manually separate out the dates we see here. We can notice that the month comes first followed by a /, then the day, a /, and the year.\n\nWe can split on the delimiter /\nThe arguments to give separate_wider_delim() are:\n\ncols = the columns we want\ndelim = the delimiter\nnames = new names for the split variables\ncols_remove - binary, whether to remove the original column or not\n\n\n\n\nchicago_data |&gt;\n  separate_wider_delim(cols = date, \n                       delim = \"/\", \n                       names = c(\"Month\", \"Day\", \"Year\"), \n                       cols_remove = FALSE)\n\n# A tibble: 1,461 × 14\n       X city  Month Day   Year  date     death  temp dewpoint  pm10    o3  time\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  3654 chic  1     1     1997  1/1/1997   137  36      37.5  13.1   5.66  3654\n 2  3655 chic  1     2     1997  1/2/1997   123  45      47.2  41.9   5.53  3655\n 3  3656 chic  1     3     1997  1/3/1997   127  40      38    27.0   6.29  3656\n 4  3657 chic  1     4     1997  1/4/1997   146  51.5    45.5  25.1   7.54  3657\n 5  3658 chic  1     5     1997  1/5/1997   102  27      11.2  15.3  20.8   3658\n 6  3659 chic  1     6     1997  1/6/1997   127  17       5.75  9.36 14.9   3659\n 7  3660 chic  1     7     1997  1/7/1997   116  16       7    20.2  11.9   3660\n 8  3661 chic  1     8     1997  1/8/1997   118  19      17.8  33.1   8.68  3661\n 9  3662 chic  1     9     1997  1/9/1997   148  26      24    12.1  13.4   3662\n10  3663 chic  1     10    1997  1/10/19…   121  16       5.38 24.8  10.4   3663\n# ℹ 1,451 more rows\n# ℹ 2 more variables: season &lt;chr&gt;, year &lt;dbl&gt;\n\n\nNice! These are character strings so we might want to turn them into numbers but, again, we’d really want to use date type data for these anyway.\n\nunite() allows us to combine two columns into one\n\nPerhaps we want a new column with the date and the season together (for display purposes)\nWe just pass unite() the name of the new column (col =), the columns we want to combine, and the separator to use (sep =)\n\n\n\nchicago_data |&gt;\n  unite(col = \"season_date\", season, date, sep = \": \") |&gt;\n  select(season_date, everything())\n\n# A tibble: 1,461 × 10\n   season_date           X city  death  temp dewpoint  pm10    o3  time  year\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 winter: 1/1/1997   3654 chic    137  36      37.5  13.1   5.66  3654  1997\n 2 winter: 1/2/1997   3655 chic    123  45      47.2  41.9   5.53  3655  1997\n 3 winter: 1/3/1997   3656 chic    127  40      38    27.0   6.29  3656  1997\n 4 winter: 1/4/1997   3657 chic    146  51.5    45.5  25.1   7.54  3657  1997\n 5 winter: 1/5/1997   3658 chic    102  27      11.2  15.3  20.8   3658  1997\n 6 winter: 1/6/1997   3659 chic    127  17       5.75  9.36 14.9   3659  1997\n 7 winter: 1/7/1997   3660 chic    116  16       7    20.2  11.9   3660  1997\n 8 winter: 1/8/1997   3661 chic    118  19      17.8  33.1   8.68  3661  1997\n 9 winter: 1/9/1997   3662 chic    148  26      24    12.1  13.4   3662  1997\n10 winter: 1/10/1997  3663 chic    121  16       5.38 24.8  10.4   3663  1997\n# ℹ 1,451 more rows\n\n\n\n\nRecap!\n\npivot_wider() & pivot_longer() great for reshaping data\nunite() & separate_wider_*() nice for dealing with columns\ntidyr Cheat Sheet\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Manipulating Data with `tidyr`"
    ]
  },
  {
    "objectID": "Notes/18-Reading_Excel_Data.html",
    "href": "Notes/18-Reading_Excel_Data.html",
    "title": "Reading Excel Data",
    "section": "",
    "text": "Another really common type of data is Excel data. This is data that has a file extension of .xls or .xlsx. We often want to pull data from different sheets within these files.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Reading Excel Data"
    ]
  },
  {
    "objectID": "Notes/18-Reading_Excel_Data.html#readxl-package",
    "href": "Notes/18-Reading_Excel_Data.html#readxl-package",
    "title": "Reading Excel Data",
    "section": "readxl Package",
    "text": "readxl Package\nThe readxl package is part of the tidyverse (not loaded by default) that has functionality for reading in this type of data!\nHowever, these types of files cannot be pulled from a URL. Instead, we’ll need to download the files and provide a path to them.\n\nDownload the dry beans data set available at: https://www4.stat.ncsu.edu/~online/datasets/Dry_Bean_Dataset.xlsx\nStore it in your R project folder, a datasets folder within there, or the folder with your .qmd file in it.\nLet’s read it into R!\n\nIf the file exists in your .qmd file’s directory, we can read it in via:\n\nlibrary(readxl)\ndry_bean_data &lt;- read_excel(\"data/Dry_Bean_Dataset.xlsx\")\ndry_bean_data\n\n# A tibble: 13,611 × 17\n    Area Perimeter MajorAxisLength MinorAxisLength AspectRatio Eccentricity\n   &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 28395      610.            208.            174.        1.20        0.550\n 2 28734      638.            201.            183.        1.10        0.412\n 3 29380      624.            213.            176.        1.21        0.563\n 4 30008      646.            211.            183.        1.15        0.499\n 5 30140      620.            202.            190.        1.06        0.334\n 6 30279      635.            213.            182.        1.17        0.520\n 7 30477      670.            211.            184.        1.15        0.489\n 8 30519      630.            213.            183.        1.17        0.514\n 9 30685      636.            214.            183.        1.17        0.514\n10 30834      632.            217.            181.        1.20        0.554\n# ℹ 13,601 more rows\n# ℹ 11 more variables: ConvexArea &lt;dbl&gt;, EquivDiameter &lt;dbl&gt;, Extent &lt;dbl&gt;,\n#   Solidity &lt;dbl&gt;, Roundness &lt;dbl&gt;, Compactness &lt;dbl&gt;, ShapeFactor1 &lt;dbl&gt;,\n#   ShapeFactor2 &lt;dbl&gt;, ShapeFactor3 &lt;dbl&gt;, ShapeFactor4 &lt;dbl&gt;, Class &lt;chr&gt;\n\n\nGreat! Easy enough. If the file was in one folder up from your .qmd file, you could read it in via\n\ndry_bean_data &lt;- read_excel(\"../Dry_Bean_Dataset.xlsx\")\n\nIf the file had been in a folder called datasets located one folder up from your .qmd file, you could read it in via\n\ndry_bean_data &lt;- read_excel(\"../datasets/Dry_Bean_Dataset.xlsx\")\n\nNote: If you switch to have your chunk output in your console, the working directory used during the interactive modifying and submitting of code from your .qmd file will use your usual working directory for your R session. This can be annoying! When you render it will use the .qmd file’s location as the working directory.\n\nReading From a Particular Sheet\nWe might want to programmatically look at the sheets available in the excel document. This can be done with the excel_sheets() function.\n\nexcel_sheets(\"data/Dry_Bean_Dataset.xlsx\")\n\n[1] \"Dry_Beans_Dataset\" \"Citation_Request\" \n\n\nWe can pull in data from a specific sheet with the name or via integers (or NULL for 1st)\n\ncitation_dry_bean_data &lt;- read_excel(\"data/Dry_Bean_Dataset.xlsx\", \n                            sheet = excel_sheets(\"data/Dry_Bean_Dataset.xlsx\")[2])\ncitation_dry_bean_data\n\n# A tibble: 0 × 1\n# ℹ 1 variable:\n#   Citation Request :\nKOKLU, M. and OZKAN, I.A., (2020), “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture, 174, 105507. DOI: https://doi.org/10.1016/j.compag.2020.105507 &lt;lgl&gt;\n\n\nNotice that didn’t read in correctly! There is only one entry there (the 1st cell, 1st column) and it is currently being treated as the column name. Similar to the read_csv() function we can use col_names = FALSE here (thanks coherent ecosystem!!).\n\ncitation_dry_bean_data &lt;- read_excel(\"data/Dry_Bean_Dataset.xlsx\", \n                            sheet = excel_sheets(\"data/Dry_Bean_Dataset.xlsx\")[2],\n                            col_names = FALSE)\n\nNew names:\n• `` -&gt; `...1`\n\ncitation_dry_bean_data\n\n# A tibble: 1 × 1\n  ...1                                                                          \n  &lt;chr&gt;                                                                         \n1 \"Citation Request :\\r\\nKOKLU, M. and OZKAN, I.A., (2020), “Multiclass Classif…\n\n\nWe can see there are some special characters in there (like line break). If we use cat() it will print that out nicely.\n\ncat(dplyr::pull(citation_dry_bean_data, 1))\n\nCitation Request :\nKOKLU, M. and OZKAN, I.A., (2020), “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture, 174, 105507. DOI: https://doi.org/10.1016/j.compag.2020.105507\n\n\n\n\nReading Only Specific Cells\nOccasionally, we might want to read only some cells on a particular sheet. This can be done by specifying the range argument!\n\nCells must be in a contiguous range\n\n\ndry_bean_range &lt;- read_excel(\"data/Dry_Bean_Dataset.xlsx\", \n                   range = cell_cols(\"A:B\")\n                   )\ndry_bean_range\n\n# A tibble: 13,611 × 2\n    Area Perimeter\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1 28395      610.\n 2 28734      638.\n 3 29380      624.\n 4 30008      646.\n 5 30140      620.\n 6 30279      635.\n 7 30477      670.\n 8 30519      630.\n 9 30685      636.\n10 30834      632.\n# ℹ 13,601 more rows\n\n\n\n\nRecap!\nThe read_xl package provides nice functionality for reading in excel type data.\n\nAs it is part of the tidyverse it reads the data into a tibble\nFunctionality to read in from different sheets or to read in particular ranges of data\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Reading Excel Data"
    ]
  },
  {
    "objectID": "Notes/16-Tidyverse_Essentials.html",
    "href": "Notes/16-Tidyverse_Essentials.html",
    "title": "Tidyverse Essentials",
    "section": "",
    "text": "One of the big impediments to learning R in the past was the vast ecosystem of packages.\nAlong came the tidyverse collection of packages! While not the most efficient method for programming, the tidyverse provides a coherent ecosystem for almost all common data tasks! That is,",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Tidyverse Essentials"
    ]
  },
  {
    "objectID": "Notes/16-Tidyverse_Essentials.html#tidyverse-syntax",
    "href": "Notes/16-Tidyverse_Essentials.html#tidyverse-syntax",
    "title": "Tidyverse Essentials",
    "section": "tidyverse Syntax",
    "text": "tidyverse Syntax\nAs the tidyverse is mostly concerned with the analysis and manipulation of data, the main data object used is a special version of a data frame called a tibble.\n\niris_tbl &lt;- dplyr::as_tibble(iris)\nclass(iris_tbl)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nYou can see the classes of a tibble include a data frame. When R functions do method dispatch, they look through the class list from first to last. If there is a method for tbl_df it uses that, if not, it looks for a method for a tbl. If that doesn’t exist, it uses a method for data.frames.\n\nstr(iris_tbl)\n\ntibble [150 × 5] (S3: tbl_df/tbl/data.frame)\n $ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe see the structure looks very similar to that of a data.frame.\n\niris_tbl\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\nHowever, we can see the default print() method for a tibble (which is used when you just type an R object into the console) is not the same. We get fancy printing that is more useful for us and doesn’t clog up our output space. We get information on the number of observations, the columns, and see only the first few rows/columns.\nAlmost all of the tidyverse functions are built to work on a tibble. That is, they usually take in a tibble and output a tibble.\n\n(Almost) all functions have similar syntax!\nfunction_name(tibble, other_arg, ...)\nMakes them perfect for chaining!\n\ntibble |&gt;\n    function(other_arg, ...) |&gt;\n    ...\nNote: you’ll often see the chain from the magrittr package used (%&gt;%). Due to the popularity of this operator, R implemented its own pipe recently (|&gt;). At this point, the functionality is almost the same so we’ll use the Base R pipe since it doesn’t require a package load.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Tidyverse Essentials"
    ]
  },
  {
    "objectID": "Notes/16-Tidyverse_Essentials.html#tidyverse-packages",
    "href": "Notes/16-Tidyverse_Essentials.html#tidyverse-packages",
    "title": "Tidyverse Essentials",
    "section": "tidyverse Packages",
    "text": "tidyverse Packages\nThe tidyverse consists of a large number of packages. However, library(tidyverse) loads only the eight core packages (which sometimes load other packages of course). Those are (from their website):\n\nggplot2 - ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details\ndplyr - dplyr provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges\ntidyr - tidyr provides a set of functions that help you get to tidy data. Tidy data is data with a consistent form: in brief, every variable goes in a column, and every column is a variable\nreadr - readr provides a fast and friendly way to read rectangular data (like csv, tsv, and fwf). It is designed to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes\npurrr - purrr enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. Once you master the basic concepts, purrr allows you to replace many for loops with code that is easier to write and more expressive\ntibble - tibble is a modern re-imagining of the data frame, keeping what time has proven to be effective, and throwing out what it has not. Tibbles are data.frames that are lazy and surly: they do less and complain more forcing you to confront problems earlier, typically leading to cleaner, more expressive code\nstringr - stringr provides a cohesive set of functions designed to make working with strings as easy as possible. It is built on top of stringi, which uses the ICU C library to provide fast, correct implementations of common string manipulations\nforcats - forcats provides a suite of useful tools that solve common problems with factors. R uses factors to handle categorical variables, variables that have a fixed and known set of possible values\n\nWe’ll spend a good bit of time on ggplot2, dplry, tidyr, and readr. tibble will get used implicitly along the way",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Tidyverse Essentials"
    ]
  },
  {
    "objectID": "Notes/16-Tidyverse_Essentials.html#recap",
    "href": "Notes/16-Tidyverse_Essentials.html#recap",
    "title": "Tidyverse Essentials",
    "section": "Recap!",
    "text": "Recap!\ntidyverse provides a coherent ecosystem for almost all common data tasks!\n\nWorks on tibbles (special data frames)\n(Almost) all packages have functions with the same syntax\nA plethora of help documentation and vignettes exist\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Tidyverse Essentials"
    ]
  },
  {
    "objectID": "Notes/10_5-Week2_3_em.html",
    "href": "Notes/10_5-Week2_3_em.html",
    "title": "Week 3 Overview",
    "section": "",
    "text": "This wraps up the content for week 2. Now we require some practice! You should head back to our Moodle site to check out your homework assignment for this week.\nWe are now ready to really look at bringing data in and how to handle that data more seamlessly. We’ll look at working within the tidyverse. This is a suite of packages that all work together and allow you to read in data and do most common data manipulations.\nThis week we’ll also see how we can use the tidyr package to change the format of a dataset (long to wide), see how we can connect R to a database, learn about SQL style joins",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Week 3 Overview"
    ]
  },
  {
    "objectID": "Notes/10_5-Week2_3_em.html#week-3-additional-readingslearning-materials",
    "href": "Notes/10_5-Week2_3_em.html#week-3-additional-readingslearning-materials",
    "title": "Week 3 Overview",
    "section": "Week 3 Additional Readings/Learning Materials",
    "text": "Week 3 Additional Readings/Learning Materials\n\nManipulating Data\n\nChapters 3, 4, and 5 of R 4 Data Science\n(Optional) R Packages book\n(Optional) List of CRAN approved packages\n(Optional) List of useful R packages\n\n\n\nReading Data\n\nChapter 7 of R 4 Data Science\n(Optional) SQL syntax\n\n\n\nSQL Joins\n\nChapter 19 of R 4 Data Science\n\n\n\nReading Data\n\nread delimited data, SAS data files, SPSS files, and other file types into R (CO 2)\n\ndescribe the term delimiter\nread comma separated value files into R using the readr package\nexplain how the read_ functions determine column types\ndescribe the readxl package and its functions\ncompare and contrast tibbles and data frames\n\nwrite a stored data set to a file using different delimiters (CO 2)\n\n\n\nManipulating Data\n\nuse logical statements and indexing vectors to subset common data objects using common functions such as [, subset, or dplyr::filter functions (CO 2, 3, 4)\nlist favorable things to look for in an R package (CO 2, 3)\n\ndescribe the general purpose of the tidyverse package\noutline the difference between require and library\nexplicitly use functions from a particular package using the :: operator\ndiscuss the idea of masking of R functions and objects\n\ndescribe the uses of and program with functions from the dplyr package(CO 2, 3, 4)\n\nexplain the benefits of using the dplyr package over base R methods\nprogram with the arrange, filter, select, and rename functions from the dplyr package\noptimize selecting variables from a data frame using the select function’s options (such as starts_with)\ncombine functions in the dplry package to subset and summarize a data set in R\ndescribe the uses of and program with the mutate, group_by, and summarise functions in the dplyr R package\ncombine functions in the dplry package to subset and summarize a data set in R\n\nprogram using the chain of commands or chaining/piping operators (CO 1, 4)\n\n\n\nOther ways to connect R to data\n\nexplain the general process of connecting R to a database, connect R to a database, and request data (CO 2)\n\ndefine the term SQL and RDBMS\ncompare terminology between statistics and SQL (tables vs data sets, etc.)\nextract SQL code from dplyr commands\nwrite very basic SQL code to select and merge data\ndescribe why the collect function is required when using R to query a database\ndetermine the appropriate type of join to extract information of interest from given tables\n\nquery APIs to return appropriate data (CO 2)\n\ndefine the term API\nexplain the common syntax often used for APIs\n\n\n\n\nOther Data Manipulations\n\nutilize the tidyr package to manipulate data (CO 2)\n\na. change data between wide to long formats\nb. split or combine columns using the tidyr package\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Week 3 Overview"
    ]
  },
  {
    "objectID": "Notes/13-Control_Flow_Vectorized_Functions.html",
    "href": "Notes/13-Control_Flow_Vectorized_Functions.html",
    "title": "Control Flow: Vectorized Functions",
    "section": "",
    "text": "In the spirit of loops, vectorized functions give us a way to execute code on an entire ‘vector’ at once (although we can be a bit more general than just vectors). This tends to speed up computation in comparison to basic loops in R!\nThis is because loops are inefficient in R. R is an interpreted language. This means that it does a lot of the work of figuring out what to do for you. (Think about function dispatch - it looks at the type of object and figures out which version of plot() or summary() to use.) This process tends to slow R down in comparison to a vectorized operation where it still runs a loop under the hood but a vector should have all the same type of elements in it. This means it can avoid figuring the same thing out repeatedly!\n\n\nThere are some ‘built-in’ vectorized functions that are quite useful to apply to a 2D type object:\n\ncolMeans(), rowMeans()\ncolSums(), rowSums()\ncolSds(), colVars(), colMedians() (must install the matrixStats package to get these)\n\nLet’s go back to our batting dataset from the previous note set.\n\nlibrary(Lahman)\nmy_batting &lt;- Batting[, c(\"playerID\", \"teamID\", \"G\", \"AB\", \"R\", \"H\", \"X2B\", \"X3B\", \"HR\")]\nhead(my_batting)\n\n   playerID teamID  G AB R H X2B X3B HR\n1 aardsda01    SFN 11  0 0 0   0   0  0\n2 aardsda01    CHN 45  2 0 0   0   0  0\n3 aardsda01    CHA 25  0 0 0   0   0  0\n4 aardsda01    BOS 47  1 0 0   0   0  0\n5 aardsda01    SEA 73  0 0 0   0   0  0\n6 aardsda01    SEA 53  0 0 0   0   0  0\n\n\nWe can apply the colMeans() function easily!\n\ncolMeans(my_batting[, 3:9])\n\n         G         AB          R          H        X2B        X3B         HR \n 50.384213 137.415513  18.243218  35.841071   6.140151   1.209975   2.874296 \n\n\nIf we install the matrixStats package (download the files from the internet), we can then use the colMedians() function to obtain the column medians in a quick fashion.\n\n#install.packages(\"matrixStats\") #only run this once on your machine!\nlibrary(matrixStats)\n\nWarning: package 'matrixStats' was built under R version 4.4.3\n\ncolMedians(my_batting[, 3:9])\n\nError in colMedians(my_batting[, 3:9]): Argument 'x' must be a matrix or a vector\n\n\nAh, this package requires the object passed to be a matrix or vector (homogenous). Although our data frame we pass is homogenous, the function doesn’t have a check for that. No worries, we can convert to a matrix using as.matrix() (similar to the is. family of functions there is an as. family of functions (read as ‘as dot’)).\n\ncolMedians(as.matrix(my_batting[, 3:9]))\n\n  G  AB   R   H X2B X3B  HR \n 34  44   4   8   1   0   0 \n\n\nLet’s compare the speed of this code to the speed of a for loop!\n\nThe microbenchmark package allows for easy recording of computing time.\nWe just wrap the code we want to benchmark in the microbenchmark() function.\nThis repeatedly executes the code and reports summary stats on how long it took\n\nHere we will grab all the numeric columns from the data\nSome columns contain NA or missing values. We’ll add na.rm = TRUE to both function calls to ignore those values (this is where the for loop actually struggles in this case!)\n\n\n\n#install.packages(\"microbenchmark\") #run only once on your machine!\nlibrary(microbenchmark)\nmy_numeric_batting &lt;- Batting[, 6:22] #get all numeric columns\nvectorized_results &lt;- microbenchmark(\n  colMeans(my_numeric_batting, na.rm = TRUE)\n)\n\nloop_results &lt;- microbenchmark(\n  for(i in 1:17){\n    mean(my_numeric_batting[, i], na.rm = TRUE)\n  }\n)\n\n\nCompare computational time\n\n\nvectorized_results\n\nUnit: milliseconds\n                                       expr    min      lq     mean median\n colMeans(my_numeric_batting, na.rm = TRUE) 4.3092 5.04975 7.508202 5.3844\n      uq     max neval\n 7.23035 25.8668   100\n\n\n\nloop_results\n\nUnit: milliseconds\n                                                                expr     min\n for (i in 1:17) {     mean(my_numeric_batting[, i], na.rm = TRUE) } 14.3022\n       lq     mean   median       uq     max neval\n 17.44525 21.04644 19.51715 23.63965 52.2441   100\n\n\n\n\n\nWe saw the limitation of using standard if/then/else logic for manipulating a data set. The ifelse() function is a vectorized form of if/then/else logic.\nLet’s revisit our example that used the airquality dataset. We wanted to code a wind category variable:\n\nhigh wind days (15mph \\(\\leq\\) wind)\n\nwindy days (10mph \\(\\leq\\) wind &lt; 15mph)\n\nlightwind days (6mph \\(\\leq\\) wind &lt; 10mph)\n\ncalm days (wind \\(\\leq\\) 6mph)\n\nThe syntax for ifelse is:\n\nifelse(vector_condition, if_true_do_this, if_false_do_this)\n\na vector is returned!\n\nifelse(airquality$Wind &gt;= 15, \"HighWind\", \"Not HighWind\")\n\n  [1] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n  [6] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\"\n [11] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [16] \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\"\n [21] \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\" \"HighWind\"    \n [26] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [31] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\"\n [36] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [41] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [46] \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\"\n [51] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [56] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [61] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [66] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [71] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [76] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [81] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [86] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [91] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [96] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[101] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[106] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[111] \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\"\n[116] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[121] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[126] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\"\n[131] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"HighWind\"    \n[136] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[141] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[146] \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\"\n[151] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n\n\nWe can use a second call to ifelse() to assign what to do in the FALSE condition!\n\nifelse(airquality$Wind &gt;= 15, \"HighWind\",\n          ifelse(airquality$Wind &gt;= 10, \"Windy\",\n                 ifelse(airquality$Wind &gt;= 6, \"LightWind\", \n                        ifelse(airquality$Wind &gt;= 0, \"Calm\", \"Error\"))))\n\n  [1] \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"     \"Windy\"    \n  [7] \"LightWind\" \"Windy\"     \"HighWind\"  \"LightWind\" \"LightWind\" \"LightWind\"\n [13] \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"     \"Windy\"     \"HighWind\" \n [19] \"Windy\"     \"LightWind\" \"LightWind\" \"HighWind\"  \"LightWind\" \"Windy\"    \n [25] \"HighWind\"  \"Windy\"     \"LightWind\" \"Windy\"     \"Windy\"     \"Calm\"     \n [31] \"LightWind\" \"LightWind\" \"LightWind\" \"HighWind\"  \"LightWind\" \"LightWind\"\n [37] \"Windy\"     \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"    \n [43] \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"     \"HighWind\" \n [49] \"LightWind\" \"Windy\"     \"Windy\"     \"LightWind\" \"Calm\"      \"Calm\"     \n [55] \"LightWind\" \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"    \n [61] \"LightWind\" \"Calm\"      \"LightWind\" \"LightWind\" \"Windy\"     \"Calm\"     \n [67] \"Windy\"     \"Calm\"      \"LightWind\" \"Calm\"      \"LightWind\" \"LightWind\"\n [73] \"Windy\"     \"Windy\"     \"Windy\"     \"Windy\"     \"LightWind\" \"Windy\"    \n [79] \"LightWind\" \"Calm\"      \"Windy\"     \"LightWind\" \"LightWind\" \"Windy\"    \n [85] \"LightWind\" \"LightWind\" \"LightWind\" \"Windy\"     \"LightWind\" \"LightWind\"\n [91] \"LightWind\" \"LightWind\" \"LightWind\" \"Windy\"     \"LightWind\" \"LightWind\"\n [97] \"LightWind\" \"Calm\"      \"Calm\"      \"Windy\"     \"LightWind\" \"LightWind\"\n[103] \"Windy\"     \"Windy\"     \"Windy\"     \"LightWind\" \"Windy\"     \"Windy\"    \n[109] \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"HighWind\"  \"Windy\"    \n[115] \"Windy\"     \"LightWind\" \"Calm\"      \"LightWind\" \"Calm\"      \"LightWind\"\n[121] \"Calm\"      \"LightWind\" \"LightWind\" \"LightWind\" \"Calm\"      \"Calm\"     \n[127] \"Calm\"      \"LightWind\" \"HighWind\"  \"Windy\"     \"Windy\"     \"Windy\"    \n[133] \"LightWind\" \"Windy\"     \"HighWind\"  \"LightWind\" \"Windy\"     \"Windy\"    \n[139] \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"     \"LightWind\" \"Windy\"    \n[145] \"LightWind\" \"Windy\"     \"Windy\"     \"HighWind\"  \"LightWind\" \"Windy\"    \n[151] \"Windy\"     \"LightWind\" \"Windy\"    \n\n\nWhoa that was pretty easy! Nice.\nLet’s compare this to using a for loop speed-wise.\n\nloopTime&lt;-microbenchmark(\n  for (i in seq_len(nrow(airquality))){\n    if(airquality$Wind[i] &gt;= 15){\n       \"HighWind\"\n    } else if (airquality$Wind[i] &gt;= 10){\n      \"Windy\"\n    } else if (airquality$Wind[i] &gt;= 6){\n      \"LightWind\"\n    } else if (airquality$Wind[i] &gt;= 0){\n      \"Calm\"\n    } else{\n      \"Error\"\n    }\n  }\n, unit = \"us\")\n\n\nvectorTime &lt;- microbenchmark(\n  ifelse(airquality$Wind &gt;= 15, \"HighWind\",\n         ifelse(airquality$Wind &gt;= 10, \"Windy\",\n                ifelse(airquality$Wind &gt;= 6, \"LightWind\", \n                       ifelse(airquality$Wind &gt;= 0, \"Calm\", \"Error\"))))\n)\n\nCompare!\n\nloopTime\n\nUnit: microseconds\n                                                                                                                                                                                                                                                                                                                                 expr\n for (i in seq_len(nrow(airquality))) {     if (airquality$Wind[i] &gt;= 15) {         \"HighWind\"     }     else if (airquality$Wind[i] &gt;= 10) {         \"Windy\"     }     else if (airquality$Wind[i] &gt;= 6) {         \"LightWind\"     }     else if (airquality$Wind[i] &gt;= 0) {         \"Calm\"     }     else {         \"Error\"     } }\n  min      lq     mean median      uq    max neval\n 3170 3373.35 3975.474 3644.3 4060.85 7969.5   100\n\nvectorTime\n\nUnit: microseconds\n                                                                                                                                                                                  expr\n ifelse(airquality$Wind &gt;= 15, \"HighWind\", ifelse(airquality$Wind &gt;=      10, \"Windy\", ifelse(airquality$Wind &gt;= 6, \"LightWind\", ifelse(airquality$Wind &gt;=      0, \"Calm\", \"Error\"))))\n  min     lq    mean median     uq   max neval\n 94.3 101.65 117.668 105.75 114.65 498.8   100\n\n\nNote: There is an if_else() function from the dplry package. This has more restrictions than ifelse() but otherwise is pretty similar.\n\n\n\n\nLoops are slower in R\nUse vectorized functions if possible\nCommon vectorized functions\n\ncolMeans(), rowMeans()\ncolSums(), rowSums()\nmatrixStats::colSds(), matrixStats::colVars(), matrixStats::colMedians()\nifelse() or dplyr::if_else()\napply family (covered soon)\npurrr package (covered in a bit)\n\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Control Flow: Vectorized Functions"
    ]
  },
  {
    "objectID": "Notes/13-Control_Flow_Vectorized_Functions.html#vectorized-functions",
    "href": "Notes/13-Control_Flow_Vectorized_Functions.html#vectorized-functions",
    "title": "Control Flow: Vectorized Functions",
    "section": "",
    "text": "In the spirit of loops, vectorized functions give us a way to execute code on an entire ‘vector’ at once (although we can be a bit more general than just vectors). This tends to speed up computation in comparison to basic loops in R!\nThis is because loops are inefficient in R. R is an interpreted language. This means that it does a lot of the work of figuring out what to do for you. (Think about function dispatch - it looks at the type of object and figures out which version of plot() or summary() to use.) This process tends to slow R down in comparison to a vectorized operation where it still runs a loop under the hood but a vector should have all the same type of elements in it. This means it can avoid figuring the same thing out repeatedly!\n\n\nThere are some ‘built-in’ vectorized functions that are quite useful to apply to a 2D type object:\n\ncolMeans(), rowMeans()\ncolSums(), rowSums()\ncolSds(), colVars(), colMedians() (must install the matrixStats package to get these)\n\nLet’s go back to our batting dataset from the previous note set.\n\nlibrary(Lahman)\nmy_batting &lt;- Batting[, c(\"playerID\", \"teamID\", \"G\", \"AB\", \"R\", \"H\", \"X2B\", \"X3B\", \"HR\")]\nhead(my_batting)\n\n   playerID teamID  G AB R H X2B X3B HR\n1 aardsda01    SFN 11  0 0 0   0   0  0\n2 aardsda01    CHN 45  2 0 0   0   0  0\n3 aardsda01    CHA 25  0 0 0   0   0  0\n4 aardsda01    BOS 47  1 0 0   0   0  0\n5 aardsda01    SEA 73  0 0 0   0   0  0\n6 aardsda01    SEA 53  0 0 0   0   0  0\n\n\nWe can apply the colMeans() function easily!\n\ncolMeans(my_batting[, 3:9])\n\n         G         AB          R          H        X2B        X3B         HR \n 50.384213 137.415513  18.243218  35.841071   6.140151   1.209975   2.874296 \n\n\nIf we install the matrixStats package (download the files from the internet), we can then use the colMedians() function to obtain the column medians in a quick fashion.\n\n#install.packages(\"matrixStats\") #only run this once on your machine!\nlibrary(matrixStats)\n\nWarning: package 'matrixStats' was built under R version 4.4.3\n\ncolMedians(my_batting[, 3:9])\n\nError in colMedians(my_batting[, 3:9]): Argument 'x' must be a matrix or a vector\n\n\nAh, this package requires the object passed to be a matrix or vector (homogenous). Although our data frame we pass is homogenous, the function doesn’t have a check for that. No worries, we can convert to a matrix using as.matrix() (similar to the is. family of functions there is an as. family of functions (read as ‘as dot’)).\n\ncolMedians(as.matrix(my_batting[, 3:9]))\n\n  G  AB   R   H X2B X3B  HR \n 34  44   4   8   1   0   0 \n\n\nLet’s compare the speed of this code to the speed of a for loop!\n\nThe microbenchmark package allows for easy recording of computing time.\nWe just wrap the code we want to benchmark in the microbenchmark() function.\nThis repeatedly executes the code and reports summary stats on how long it took\n\nHere we will grab all the numeric columns from the data\nSome columns contain NA or missing values. We’ll add na.rm = TRUE to both function calls to ignore those values (this is where the for loop actually struggles in this case!)\n\n\n\n#install.packages(\"microbenchmark\") #run only once on your machine!\nlibrary(microbenchmark)\nmy_numeric_batting &lt;- Batting[, 6:22] #get all numeric columns\nvectorized_results &lt;- microbenchmark(\n  colMeans(my_numeric_batting, na.rm = TRUE)\n)\n\nloop_results &lt;- microbenchmark(\n  for(i in 1:17){\n    mean(my_numeric_batting[, i], na.rm = TRUE)\n  }\n)\n\n\nCompare computational time\n\n\nvectorized_results\n\nUnit: milliseconds\n                                       expr    min      lq     mean median\n colMeans(my_numeric_batting, na.rm = TRUE) 4.3092 5.04975 7.508202 5.3844\n      uq     max neval\n 7.23035 25.8668   100\n\n\n\nloop_results\n\nUnit: milliseconds\n                                                                expr     min\n for (i in 1:17) {     mean(my_numeric_batting[, i], na.rm = TRUE) } 14.3022\n       lq     mean   median       uq     max neval\n 17.44525 21.04644 19.51715 23.63965 52.2441   100\n\n\n\n\n\nWe saw the limitation of using standard if/then/else logic for manipulating a data set. The ifelse() function is a vectorized form of if/then/else logic.\nLet’s revisit our example that used the airquality dataset. We wanted to code a wind category variable:\n\nhigh wind days (15mph \\(\\leq\\) wind)\n\nwindy days (10mph \\(\\leq\\) wind &lt; 15mph)\n\nlightwind days (6mph \\(\\leq\\) wind &lt; 10mph)\n\ncalm days (wind \\(\\leq\\) 6mph)\n\nThe syntax for ifelse is:\n\nifelse(vector_condition, if_true_do_this, if_false_do_this)\n\na vector is returned!\n\nifelse(airquality$Wind &gt;= 15, \"HighWind\", \"Not HighWind\")\n\n  [1] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n  [6] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\"\n [11] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [16] \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\"\n [21] \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\" \"HighWind\"    \n [26] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [31] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\"\n [36] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [41] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [46] \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\"\n [51] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [56] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [61] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [66] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [71] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [76] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [81] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [86] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [91] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n [96] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[101] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[106] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[111] \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\"\n[116] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[121] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[126] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\"\n[131] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"HighWind\"    \n[136] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[141] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n[146] \"Not HighWind\" \"Not HighWind\" \"HighWind\"     \"Not HighWind\" \"Not HighWind\"\n[151] \"Not HighWind\" \"Not HighWind\" \"Not HighWind\"\n\n\nWe can use a second call to ifelse() to assign what to do in the FALSE condition!\n\nifelse(airquality$Wind &gt;= 15, \"HighWind\",\n          ifelse(airquality$Wind &gt;= 10, \"Windy\",\n                 ifelse(airquality$Wind &gt;= 6, \"LightWind\", \n                        ifelse(airquality$Wind &gt;= 0, \"Calm\", \"Error\"))))\n\n  [1] \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"     \"Windy\"    \n  [7] \"LightWind\" \"Windy\"     \"HighWind\"  \"LightWind\" \"LightWind\" \"LightWind\"\n [13] \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"     \"Windy\"     \"HighWind\" \n [19] \"Windy\"     \"LightWind\" \"LightWind\" \"HighWind\"  \"LightWind\" \"Windy\"    \n [25] \"HighWind\"  \"Windy\"     \"LightWind\" \"Windy\"     \"Windy\"     \"Calm\"     \n [31] \"LightWind\" \"LightWind\" \"LightWind\" \"HighWind\"  \"LightWind\" \"LightWind\"\n [37] \"Windy\"     \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"    \n [43] \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"     \"HighWind\" \n [49] \"LightWind\" \"Windy\"     \"Windy\"     \"LightWind\" \"Calm\"      \"Calm\"     \n [55] \"LightWind\" \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"    \n [61] \"LightWind\" \"Calm\"      \"LightWind\" \"LightWind\" \"Windy\"     \"Calm\"     \n [67] \"Windy\"     \"Calm\"      \"LightWind\" \"Calm\"      \"LightWind\" \"LightWind\"\n [73] \"Windy\"     \"Windy\"     \"Windy\"     \"Windy\"     \"LightWind\" \"Windy\"    \n [79] \"LightWind\" \"Calm\"      \"Windy\"     \"LightWind\" \"LightWind\" \"Windy\"    \n [85] \"LightWind\" \"LightWind\" \"LightWind\" \"Windy\"     \"LightWind\" \"LightWind\"\n [91] \"LightWind\" \"LightWind\" \"LightWind\" \"Windy\"     \"LightWind\" \"LightWind\"\n [97] \"LightWind\" \"Calm\"      \"Calm\"      \"Windy\"     \"LightWind\" \"LightWind\"\n[103] \"Windy\"     \"Windy\"     \"Windy\"     \"LightWind\" \"Windy\"     \"Windy\"    \n[109] \"LightWind\" \"LightWind\" \"Windy\"     \"Windy\"     \"HighWind\"  \"Windy\"    \n[115] \"Windy\"     \"LightWind\" \"Calm\"      \"LightWind\" \"Calm\"      \"LightWind\"\n[121] \"Calm\"      \"LightWind\" \"LightWind\" \"LightWind\" \"Calm\"      \"Calm\"     \n[127] \"Calm\"      \"LightWind\" \"HighWind\"  \"Windy\"     \"Windy\"     \"Windy\"    \n[133] \"LightWind\" \"Windy\"     \"HighWind\"  \"LightWind\" \"Windy\"     \"Windy\"    \n[139] \"LightWind\" \"Windy\"     \"Windy\"     \"Windy\"     \"LightWind\" \"Windy\"    \n[145] \"LightWind\" \"Windy\"     \"Windy\"     \"HighWind\"  \"LightWind\" \"Windy\"    \n[151] \"Windy\"     \"LightWind\" \"Windy\"    \n\n\nWhoa that was pretty easy! Nice.\nLet’s compare this to using a for loop speed-wise.\n\nloopTime&lt;-microbenchmark(\n  for (i in seq_len(nrow(airquality))){\n    if(airquality$Wind[i] &gt;= 15){\n       \"HighWind\"\n    } else if (airquality$Wind[i] &gt;= 10){\n      \"Windy\"\n    } else if (airquality$Wind[i] &gt;= 6){\n      \"LightWind\"\n    } else if (airquality$Wind[i] &gt;= 0){\n      \"Calm\"\n    } else{\n      \"Error\"\n    }\n  }\n, unit = \"us\")\n\n\nvectorTime &lt;- microbenchmark(\n  ifelse(airquality$Wind &gt;= 15, \"HighWind\",\n         ifelse(airquality$Wind &gt;= 10, \"Windy\",\n                ifelse(airquality$Wind &gt;= 6, \"LightWind\", \n                       ifelse(airquality$Wind &gt;= 0, \"Calm\", \"Error\"))))\n)\n\nCompare!\n\nloopTime\n\nUnit: microseconds\n                                                                                                                                                                                                                                                                                                                                 expr\n for (i in seq_len(nrow(airquality))) {     if (airquality$Wind[i] &gt;= 15) {         \"HighWind\"     }     else if (airquality$Wind[i] &gt;= 10) {         \"Windy\"     }     else if (airquality$Wind[i] &gt;= 6) {         \"LightWind\"     }     else if (airquality$Wind[i] &gt;= 0) {         \"Calm\"     }     else {         \"Error\"     } }\n  min      lq     mean median      uq    max neval\n 3170 3373.35 3975.474 3644.3 4060.85 7969.5   100\n\nvectorTime\n\nUnit: microseconds\n                                                                                                                                                                                  expr\n ifelse(airquality$Wind &gt;= 15, \"HighWind\", ifelse(airquality$Wind &gt;=      10, \"Windy\", ifelse(airquality$Wind &gt;= 6, \"LightWind\", ifelse(airquality$Wind &gt;=      0, \"Calm\", \"Error\"))))\n  min     lq    mean median     uq   max neval\n 94.3 101.65 117.668 105.75 114.65 498.8   100\n\n\nNote: There is an if_else() function from the dplry package. This has more restrictions than ifelse() but otherwise is pretty similar.\n\n\n\n\nLoops are slower in R\nUse vectorized functions if possible\nCommon vectorized functions\n\ncolMeans(), rowMeans()\ncolSums(), rowSums()\nmatrixStats::colSds(), matrixStats::colVars(), matrixStats::colMedians()\nifelse() or dplyr::if_else()\napply family (covered soon)\npurrr package (covered in a bit)\n\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Control Flow: Vectorized Functions"
    ]
  },
  {
    "objectID": "Notes/11-Control_Flow_Logicals_if_then_else.html",
    "href": "Notes/11-Control_Flow_Logicals_if_then_else.html",
    "title": "Control Flow: Logicals & if/then/else",
    "section": "",
    "text": "A logical statement is a comparison of two quantities. It will resolve as TRUE or FALSE (note the all caps).\n\nTo compare to things in R, we can use standard operators\n\n== equality check (although this isn’t always the best choice!)\n!= not equal to\n&gt;=, &gt;, &lt;, &lt;= operators\n\n\n\n#Strings must be exactly the same to be equivalent\n\"hi\" == \"hi\"\n\n[1] TRUE\n\n\"hi\" == \" hi\"\n\n[1] FALSE\n\n\n\n4 &gt;= 1\n\n[1] TRUE\n\n4 != 1\n\n[1] TRUE\n\nsqrt(3)^2  == 3\n\n[1] FALSE\n\n\nThat last one should be true! The issue is the loss of precision with taking the square root of 3. Instead of using == we can use the near() function from the dplyr package (you may need to install this package, install.packages(\"dplyr\")). To call a function directly from a package we can use ::\n\ndplyr::near(sqrt(3)^2, 3)\n\n[1] TRUE\n\n\nThat’s more like it!\n\n\nIn addition to the standard operators, R has a family of is. (read as “is dot”) functions. These allow you to check a lot of things about an R object or value!\n\nis.numeric(\"Word\")\n\n[1] FALSE\n\nis.numeric(c(10, 12, 20))\n\n[1] TRUE\n\n\n\nis.character(c(10, 12, 20))\n\n[1] FALSE\n\nis.character(c(\"10\", \"12\"))\n\n[1] TRUE\n\nis.na(c(1:2, NA, 3))\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nThis last one is important!\n\nFirst, note that R applies the is.na() function element-wise to the vector. This is not common behavior.\nSecond, NA is the missing value indicator in R. When we start to read in data we need to check for missing values. More on that later.\nNA differs from NULL which is the undefined value in R\n\n\n\n\nCreating logical statements can be useful for subsetting data. We’ll see how to do this in a streamlined fashion later, but for now, let’s use Base R functionality to do some subsetting.\nRecall the iris data set. This has measurements on flowers.\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nAs R does comparisons element-wise, we can compare the Species column to a value. This returns a vector of TRUE and FALSE values (a logical vector).\n\niris$Species == 'setosa'\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nIf we index an object with a logical vector, it returns the values where a TRUE occurred and doesn’t return values where a FALSE occurred!\n\niris[iris$Species == \"setosa\", ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n11          5.4         3.7          1.5         0.2  setosa\n12          4.8         3.4          1.6         0.2  setosa\n13          4.8         3.0          1.4         0.1  setosa\n14          4.3         3.0          1.1         0.1  setosa\n15          5.8         4.0          1.2         0.2  setosa\n16          5.7         4.4          1.5         0.4  setosa\n17          5.4         3.9          1.3         0.4  setosa\n18          5.1         3.5          1.4         0.3  setosa\n19          5.7         3.8          1.7         0.3  setosa\n20          5.1         3.8          1.5         0.3  setosa\n21          5.4         3.4          1.7         0.2  setosa\n22          5.1         3.7          1.5         0.4  setosa\n23          4.6         3.6          1.0         0.2  setosa\n24          5.1         3.3          1.7         0.5  setosa\n25          4.8         3.4          1.9         0.2  setosa\n26          5.0         3.0          1.6         0.2  setosa\n27          5.0         3.4          1.6         0.4  setosa\n28          5.2         3.5          1.5         0.2  setosa\n29          5.2         3.4          1.4         0.2  setosa\n30          4.7         3.2          1.6         0.2  setosa\n31          4.8         3.1          1.6         0.2  setosa\n32          5.4         3.4          1.5         0.4  setosa\n33          5.2         4.1          1.5         0.1  setosa\n34          5.5         4.2          1.4         0.2  setosa\n35          4.9         3.1          1.5         0.2  setosa\n36          5.0         3.2          1.2         0.2  setosa\n37          5.5         3.5          1.3         0.2  setosa\n38          4.9         3.6          1.4         0.1  setosa\n39          4.4         3.0          1.3         0.2  setosa\n40          5.1         3.4          1.5         0.2  setosa\n41          5.0         3.5          1.3         0.3  setosa\n42          4.5         2.3          1.3         0.3  setosa\n43          4.4         3.2          1.3         0.2  setosa\n44          5.0         3.5          1.6         0.6  setosa\n45          5.1         3.8          1.9         0.4  setosa\n46          4.8         3.0          1.4         0.3  setosa\n47          5.1         3.8          1.6         0.2  setosa\n48          4.6         3.2          1.4         0.2  setosa\n49          5.3         3.7          1.5         0.2  setosa\n50          5.0         3.3          1.4         0.2  setosa\n\n\n\n\n\nOf course there are times we want to check whether two conditions are both TRUE or at least one of the conditions is TRUE. The Logical Operators below help us with that:\n\n& ‘and’\n| ‘or’\n\n\n\n\nOperator\nA,B true\nA true, B false\nA,B false\n\n\n\n\n&\nA & B = TRUE\nA & B = FALSE\nA & B = FALSE\n\n\n|\nA | B = TRUE\nA | B = TRUE\nA | B = FALSE\n\n\n\nNote! && and || are alternatives that look at only first comparison when given a vector of comparisons. This is used a lot in writing functions but is generally not what you want to use.\nIn subsetting data, this let’s us do a lot more!\n\nOnly pull out large petal setosa flowers\n\n\n(iris$Petal.Length &gt; 1.5) & (iris$Species == \"setosa\")\n\n  [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n [25]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\niris[(iris$Petal.Length &gt; 1.5) & (iris$Species == \"setosa\"), ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n6           5.4         3.9          1.7         0.4  setosa\n12          4.8         3.4          1.6         0.2  setosa\n19          5.7         3.8          1.7         0.3  setosa\n21          5.4         3.4          1.7         0.2  setosa\n24          5.1         3.3          1.7         0.5  setosa\n25          4.8         3.4          1.9         0.2  setosa\n26          5.0         3.0          1.6         0.2  setosa\n27          5.0         3.4          1.6         0.4  setosa\n30          4.7         3.2          1.6         0.2  setosa\n31          4.8         3.1          1.6         0.2  setosa\n44          5.0         3.5          1.6         0.6  setosa\n45          5.1         3.8          1.9         0.4  setosa\n47          5.1         3.8          1.6         0.2  setosa\n\n\nThe parentheses are not required but are useful to keep things straight. For example, we might want only long petal or skinny petal, setosa flowers.\n\niris[((iris$Petal.Length &gt; 1.5) | (iris$Petal.Width &lt; 0.15)) & (iris$Species == \"setosa\"), ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n6           5.4         3.9          1.7         0.4  setosa\n10          4.9         3.1          1.5         0.1  setosa\n12          4.8         3.4          1.6         0.2  setosa\n13          4.8         3.0          1.4         0.1  setosa\n14          4.3         3.0          1.1         0.1  setosa\n19          5.7         3.8          1.7         0.3  setosa\n21          5.4         3.4          1.7         0.2  setosa\n24          5.1         3.3          1.7         0.5  setosa\n25          4.8         3.4          1.9         0.2  setosa\n26          5.0         3.0          1.6         0.2  setosa\n27          5.0         3.4          1.6         0.4  setosa\n30          4.7         3.2          1.6         0.2  setosa\n31          4.8         3.1          1.6         0.2  setosa\n33          5.2         4.1          1.5         0.1  setosa\n38          4.9         3.6          1.4         0.1  setosa\n44          5.0         3.5          1.6         0.6  setosa\n45          5.1         3.8          1.9         0.4  setosa\n47          5.1         3.8          1.6         0.2  setosa",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Control Flow: Logicals & if/then/else"
    ]
  },
  {
    "objectID": "Notes/11-Control_Flow_Logicals_if_then_else.html#logical-statements",
    "href": "Notes/11-Control_Flow_Logicals_if_then_else.html#logical-statements",
    "title": "Control Flow: Logicals & if/then/else",
    "section": "",
    "text": "A logical statement is a comparison of two quantities. It will resolve as TRUE or FALSE (note the all caps).\n\nTo compare to things in R, we can use standard operators\n\n== equality check (although this isn’t always the best choice!)\n!= not equal to\n&gt;=, &gt;, &lt;, &lt;= operators\n\n\n\n#Strings must be exactly the same to be equivalent\n\"hi\" == \"hi\"\n\n[1] TRUE\n\n\"hi\" == \" hi\"\n\n[1] FALSE\n\n\n\n4 &gt;= 1\n\n[1] TRUE\n\n4 != 1\n\n[1] TRUE\n\nsqrt(3)^2  == 3\n\n[1] FALSE\n\n\nThat last one should be true! The issue is the loss of precision with taking the square root of 3. Instead of using == we can use the near() function from the dplyr package (you may need to install this package, install.packages(\"dplyr\")). To call a function directly from a package we can use ::\n\ndplyr::near(sqrt(3)^2, 3)\n\n[1] TRUE\n\n\nThat’s more like it!\n\n\nIn addition to the standard operators, R has a family of is. (read as “is dot”) functions. These allow you to check a lot of things about an R object or value!\n\nis.numeric(\"Word\")\n\n[1] FALSE\n\nis.numeric(c(10, 12, 20))\n\n[1] TRUE\n\n\n\nis.character(c(10, 12, 20))\n\n[1] FALSE\n\nis.character(c(\"10\", \"12\"))\n\n[1] TRUE\n\nis.na(c(1:2, NA, 3))\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nThis last one is important!\n\nFirst, note that R applies the is.na() function element-wise to the vector. This is not common behavior.\nSecond, NA is the missing value indicator in R. When we start to read in data we need to check for missing values. More on that later.\nNA differs from NULL which is the undefined value in R\n\n\n\n\nCreating logical statements can be useful for subsetting data. We’ll see how to do this in a streamlined fashion later, but for now, let’s use Base R functionality to do some subsetting.\nRecall the iris data set. This has measurements on flowers.\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nAs R does comparisons element-wise, we can compare the Species column to a value. This returns a vector of TRUE and FALSE values (a logical vector).\n\niris$Species == 'setosa'\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nIf we index an object with a logical vector, it returns the values where a TRUE occurred and doesn’t return values where a FALSE occurred!\n\niris[iris$Species == \"setosa\", ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n11          5.4         3.7          1.5         0.2  setosa\n12          4.8         3.4          1.6         0.2  setosa\n13          4.8         3.0          1.4         0.1  setosa\n14          4.3         3.0          1.1         0.1  setosa\n15          5.8         4.0          1.2         0.2  setosa\n16          5.7         4.4          1.5         0.4  setosa\n17          5.4         3.9          1.3         0.4  setosa\n18          5.1         3.5          1.4         0.3  setosa\n19          5.7         3.8          1.7         0.3  setosa\n20          5.1         3.8          1.5         0.3  setosa\n21          5.4         3.4          1.7         0.2  setosa\n22          5.1         3.7          1.5         0.4  setosa\n23          4.6         3.6          1.0         0.2  setosa\n24          5.1         3.3          1.7         0.5  setosa\n25          4.8         3.4          1.9         0.2  setosa\n26          5.0         3.0          1.6         0.2  setosa\n27          5.0         3.4          1.6         0.4  setosa\n28          5.2         3.5          1.5         0.2  setosa\n29          5.2         3.4          1.4         0.2  setosa\n30          4.7         3.2          1.6         0.2  setosa\n31          4.8         3.1          1.6         0.2  setosa\n32          5.4         3.4          1.5         0.4  setosa\n33          5.2         4.1          1.5         0.1  setosa\n34          5.5         4.2          1.4         0.2  setosa\n35          4.9         3.1          1.5         0.2  setosa\n36          5.0         3.2          1.2         0.2  setosa\n37          5.5         3.5          1.3         0.2  setosa\n38          4.9         3.6          1.4         0.1  setosa\n39          4.4         3.0          1.3         0.2  setosa\n40          5.1         3.4          1.5         0.2  setosa\n41          5.0         3.5          1.3         0.3  setosa\n42          4.5         2.3          1.3         0.3  setosa\n43          4.4         3.2          1.3         0.2  setosa\n44          5.0         3.5          1.6         0.6  setosa\n45          5.1         3.8          1.9         0.4  setosa\n46          4.8         3.0          1.4         0.3  setosa\n47          5.1         3.8          1.6         0.2  setosa\n48          4.6         3.2          1.4         0.2  setosa\n49          5.3         3.7          1.5         0.2  setosa\n50          5.0         3.3          1.4         0.2  setosa\n\n\n\n\n\nOf course there are times we want to check whether two conditions are both TRUE or at least one of the conditions is TRUE. The Logical Operators below help us with that:\n\n& ‘and’\n| ‘or’\n\n\n\n\nOperator\nA,B true\nA true, B false\nA,B false\n\n\n\n\n&\nA & B = TRUE\nA & B = FALSE\nA & B = FALSE\n\n\n|\nA | B = TRUE\nA | B = TRUE\nA | B = FALSE\n\n\n\nNote! && and || are alternatives that look at only first comparison when given a vector of comparisons. This is used a lot in writing functions but is generally not what you want to use.\nIn subsetting data, this let’s us do a lot more!\n\nOnly pull out large petal setosa flowers\n\n\n(iris$Petal.Length &gt; 1.5) & (iris$Species == \"setosa\")\n\n  [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n [25]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\niris[(iris$Petal.Length &gt; 1.5) & (iris$Species == \"setosa\"), ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n6           5.4         3.9          1.7         0.4  setosa\n12          4.8         3.4          1.6         0.2  setosa\n19          5.7         3.8          1.7         0.3  setosa\n21          5.4         3.4          1.7         0.2  setosa\n24          5.1         3.3          1.7         0.5  setosa\n25          4.8         3.4          1.9         0.2  setosa\n26          5.0         3.0          1.6         0.2  setosa\n27          5.0         3.4          1.6         0.4  setosa\n30          4.7         3.2          1.6         0.2  setosa\n31          4.8         3.1          1.6         0.2  setosa\n44          5.0         3.5          1.6         0.6  setosa\n45          5.1         3.8          1.9         0.4  setosa\n47          5.1         3.8          1.6         0.2  setosa\n\n\nThe parentheses are not required but are useful to keep things straight. For example, we might want only long petal or skinny petal, setosa flowers.\n\niris[((iris$Petal.Length &gt; 1.5) | (iris$Petal.Width &lt; 0.15)) & (iris$Species == \"setosa\"), ]\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n6           5.4         3.9          1.7         0.4  setosa\n10          4.9         3.1          1.5         0.1  setosa\n12          4.8         3.4          1.6         0.2  setosa\n13          4.8         3.0          1.4         0.1  setosa\n14          4.3         3.0          1.1         0.1  setosa\n19          5.7         3.8          1.7         0.3  setosa\n21          5.4         3.4          1.7         0.2  setosa\n24          5.1         3.3          1.7         0.5  setosa\n25          4.8         3.4          1.9         0.2  setosa\n26          5.0         3.0          1.6         0.2  setosa\n27          5.0         3.4          1.6         0.4  setosa\n30          4.7         3.2          1.6         0.2  setosa\n31          4.8         3.1          1.6         0.2  setosa\n33          5.2         4.1          1.5         0.1  setosa\n38          4.9         3.6          1.4         0.1  setosa\n44          5.0         3.5          1.6         0.6  setosa\n45          5.1         3.8          1.9         0.4  setosa\n47          5.1         3.8          1.6         0.2  setosa",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Control Flow: Logicals & if/then/else"
    ]
  },
  {
    "objectID": "Notes/11-Control_Flow_Logicals_if_then_else.html#quick-aside-on-implicit-coercion",
    "href": "Notes/11-Control_Flow_Logicals_if_then_else.html#quick-aside-on-implicit-coercion",
    "title": "Control Flow: Logicals & if/then/else",
    "section": "Quick Aside on Implicit Coercion",
    "text": "Quick Aside on Implicit Coercion\nR attempts to coerce data into usable form when necessary. Unfortunately, it doesn’t always let us know it is doing so. This means we need to be careful and understand how R works.\nRecall the behavior of combining elements together into an atomic vector. R coerces to the more flexible data type.\n\n#coerce numeric to string\nc(\"hi\", 10)\n\n[1] \"hi\" \"10\"\n\n\nIn this way, R will treat TRUE as a 1 and FALSE as a 0 when math is done.\n\n#coerce TRUE/FALSE to numeric\nc(TRUE, FALSE) + 0\n\n[1] 1 0\n\nc(TRUE, FALSE) + 10\n\n[1] 11 10\n\nas.numeric(c(TRUE, FALSE, TRUE))\n\n[1] 1 0 1\n\nmean(c(TRUE, FALSE, TRUE))\n\n[1] 0.6666667\n\n\nThe order of coercion (from least flexible to most)\n\nlogical\ninteger\ndouble\ncharacter.",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Control Flow: Logicals & if/then/else"
    ]
  },
  {
    "objectID": "Notes/11-Control_Flow_Logicals_if_then_else.html#conditional-execution-via-ifthenelse",
    "href": "Notes/11-Control_Flow_Logicals_if_then_else.html#conditional-execution-via-ifthenelse",
    "title": "Control Flow: Logicals & if/then/else",
    "section": "Conditional Execution via if/then/else",
    "text": "Conditional Execution via if/then/else\nWe often want to execute statements conditionally. For instance, we might want to create a variable that takes on different values depending on whether or not some condition is met.\n\nif then else syntax\n\n\nif (condition) {\n  then execute code\n} \n\n#if then else\nif (condition) {\n  execute this code  \n} else {\n  execute this code\n}\n\n\n#Or more if statements\nif (condition) {\n  execute this code  \n} else if (condition2) {\n  execute this code\n} else if (condition3) {\n  execute this code\n} else {\n  #if no conditions met\n  execute this code\n}\n\nNote! You should keep the { on the lines as you see here. There are some occassions where something like this would work:\n\nif (condition) \n  {\n  execute this code  \n} else \n  {\n  execute this code\n}\n\nbut it generally won’t! So just mind the positioning.\nAs an example of using if/then/else consider the built-in data set airquality. This data has daily air quality measurements in New York from May (Day 1) to September (Day 153) in 1973.\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nWe may want to code a wind category variable as follows:\n\nhigh wind days (wind \\(\\geq\\) 15mph)\n\nwindy days (10mph \\(\\leq\\) wind &lt; 15mph)\n\nlightwind days (6mph \\(\\leq\\) wind &lt; 10mph)\n\ncalm days (wind \\(\\leq\\) 6mph)\n\nFor a given value we can do a check and assign a new value.\n\nairquality$Wind[1]\n\n[1] 7.4\n\nif(airquality$Wind[1] &gt;= 15) { \n  \"High Wind\"\n} else if (airquality$Wind[1] &gt;= 10){\n  \"Windy\"\n} else if (airquality$Wind[1] &gt;= 6) {\n  \"Light Wind\"\n} else if (airquality$Wind[1] &gt;= 0) {\n  \"Calm\"\n} else {\n  \"Error\"\n}\n\n[1] \"Light Wind\"\n\n\nUnfortunately, to apply this to each observation requires a loop or the use of a vectorized function. We’ll cover those shortly!\n\nQuick R video\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\nRecap\n\nLogical comparisons resolve as TRUE or FALSE\nCompound logical operators are & (and) and | (or)\nif/then/else logic to conditionally execute code\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Control Flow: Logicals & if/then/else"
    ]
  },
  {
    "objectID": "Notes/09-Base_R_Data_Structures_Data_Frames.html",
    "href": "Notes/09-Base_R_Data_Structures_Data_Frames.html",
    "title": "Base R Data Structures: Data Frames",
    "section": "",
    "text": "A data scientist needs to deal with data! We need to have a firm foundation in the ways that we can store our data in R. This section goes through the most commonly used ‘built-in’ R objects that we’ll use.\n\nThere are five major data structures used in R\n\nAtomic Vector (1d)\nMatrix (2d)\nArray (nd)\nData Frame (2d)\nList (1d)\n\n\n\n\n\n\n\n\n\n\nDimension\nHomogeneous (elements all the same)\nHeterogeneous (elements may differ)\n\n\n\n\n1d\nAtomic Vector\nList\n\n\n2d\nMatrix\nData Frame\n\n\nnd\nArray\n\n\n\n\n\n\nWe’ve now gone through the common data structures that are homogeneous. Next we’ll take on the data frame. These are 2D objects where the columns can be of different types!\nData Frames\n\nA collection (list) of vectors of the same length\nPerfect for most data sets!\n\n\n\n\n\n\n\n\n\n\n\n\nWe can create a data frame using the data.frame() function. From the help:\ndata.frame(..., row.names = NULL, check.rows = FALSE, check.names = TRUE, fix.empty.names = TRUE, stringsAsFactors = FALSE)\nThe help isn’t super clear on what the first argument ... should be. Essentially, this syntax allows us to pass vectors (each of the same length) as the columns of our data frame.\n\nx &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\ny &lt;- c(1, 3, 4, -1, 5, 6)\nz &lt;- 10:15\nmy_df &lt;- data.frame(x, y, z)\nmy_df\n\n  x  y  z\n1 a  1 10\n2 b  3 11\n3 c  4 12\n4 d -1 13\n5 e  5 14\n6 f  6 15\n\n\n\n\n\nData frames have one attribute of note: names.\n\nstr(my_df)\n\n'data.frame':   6 obs. of  3 variables:\n $ x: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ y: num  1 3 4 -1 5 6\n $ z: int  10 11 12 13 14 15\n\n\n\nattributes(my_df)\n\n$names\n[1] \"x\" \"y\" \"z\"\n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] 1 2 3 4 5 6\n\n\n\nOur data frame originally inherited the names from the data objects passed in.\nIf we didn’t pass in objects with names, we get some default nonsense.\n\n\ndata.frame(1:5, c(\"a\", \"b\", \"c\", \"d\", \"e\"))\n\n  X1.5 c..a....b....c....d....e..\n1    1                          a\n2    2                          b\n3    3                          c\n4    4                          d\n5    5                          e\n\n\n\nWe can set the names explicitly when we create the data frame.\n\n\nx &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\ny &lt;- c(1, 3, 4, -1, 5, 6)\nz &lt;- 10:15\nmy_df &lt;- data.frame(char = x, data1 = y, data2 = z)\nmy_df\n\n  char data1 data2\n1    a     1    10\n2    b     3    11\n3    c     4    12\n4    d    -1    13\n5    e     5    14\n6    f     6    15\n\n\n\ndata.frame(number = 1:5, letter = c(\"a\", \"b\", \"c\", \"d\", \"e\"))\n\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n\n\nNote: A syntactically valid name consists of letters, numbers and the dot or underline characters. It must start with a letter or a dot not followed by a number.\nYou shouldn’t use . with names generally. Periods are often used for methods and other things in programming.\nYou also should avoid reserved words: if else repeat while function for in next break TRUE FALSE NULL Inf NaN NA NA_integer_ NA_real_ NA_complex_ NA_character_ (type ?make.names into the console for more details)\n\n\n\n\nLet’s check out the ‘built-in’ iris data frame\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nThis is a 2D structure and we can access just like a matrix!\n\n\niris[1:4, 2:4] #returns a data frame\n\n  Sepal.Width Petal.Length Petal.Width\n1         3.5          1.4         0.2\n2         3.0          1.4         0.2\n3         3.2          1.3         0.2\n4         3.1          1.5         0.2\n\n\n\niris[1, ] #returns a data frame\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n\n\n\niris[1:10, 1] #returns a vector\n\n [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9\n\n\n\nNote again that R simplifies objects sometimes! The last return was simplified to a vector.\nThis behavior actually comes from using the [ function. Everything you do in R is a function call!\nWe can avoid the behavior by adding the drop = FALSE argument\n\n\niris[1:10, 1, drop = FALSE] #return a data frame\n\n   Sepal.Length\n1           5.1\n2           4.9\n3           4.7\n4           4.6\n5           5.0\n6           5.4\n7           4.6\n8           5.0\n9           4.4\n10          4.9\n\n\n\nOr we can call the [ function as a prefix function (like our usual function calls)\n\n\n`[`(iris, 1:10, 1, drop = FALSE)\n\n   Sepal.Length\n1           5.1\n2           4.9\n3           4.7\n4           4.6\n5           5.0\n6           5.4\n7           4.6\n8           5.0\n9           4.4\n10          4.9\n\n\n\nUsually data frames have meaningful column names. We can use these for subsetting\n\n\niris[1:5 , c(\"Sepal.Length\", \"Species\")]\n\n  Sepal.Length Species\n1          5.1  setosa\n2          4.9  setosa\n3          4.7  setosa\n4          4.6  setosa\n5          5.0  setosa\n\n\n\nThe most common way to access a single column is via the $ operator. This returns a vector.\n\n\niris$Sepal.Length\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1\n [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0\n [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5\n [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1\n [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5\n [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3\n[109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2\n[127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\n\n\nRStudio fills in options for us!\n\nType iris$\n\nIf no choices - hit tab\nHit tab again to choose\n\n\n\n\n\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\n\nData Frame (2D data structure)\n\nCollection (list) of vectors of the same length\nCreate with data.frame() function\nAccess with [ , ] or $\nPerfect for most data sets!\nMost functions that read 2D data store it as a data frame (or tibble - a special data frame covered later)\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Data Frames"
    ]
  },
  {
    "objectID": "Notes/09-Base_R_Data_Structures_Data_Frames.html#data-frame",
    "href": "Notes/09-Base_R_Data_Structures_Data_Frames.html#data-frame",
    "title": "Base R Data Structures: Data Frames",
    "section": "",
    "text": "We’ve now gone through the common data structures that are homogeneous. Next we’ll take on the data frame. These are 2D objects where the columns can be of different types!\nData Frames\n\nA collection (list) of vectors of the same length\nPerfect for most data sets!\n\n\n\n\n\n\n\n\n\n\n\n\nWe can create a data frame using the data.frame() function. From the help:\ndata.frame(..., row.names = NULL, check.rows = FALSE, check.names = TRUE, fix.empty.names = TRUE, stringsAsFactors = FALSE)\nThe help isn’t super clear on what the first argument ... should be. Essentially, this syntax allows us to pass vectors (each of the same length) as the columns of our data frame.\n\nx &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\ny &lt;- c(1, 3, 4, -1, 5, 6)\nz &lt;- 10:15\nmy_df &lt;- data.frame(x, y, z)\nmy_df\n\n  x  y  z\n1 a  1 10\n2 b  3 11\n3 c  4 12\n4 d -1 13\n5 e  5 14\n6 f  6 15\n\n\n\n\n\nData frames have one attribute of note: names.\n\nstr(my_df)\n\n'data.frame':   6 obs. of  3 variables:\n $ x: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ y: num  1 3 4 -1 5 6\n $ z: int  10 11 12 13 14 15\n\n\n\nattributes(my_df)\n\n$names\n[1] \"x\" \"y\" \"z\"\n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] 1 2 3 4 5 6\n\n\n\nOur data frame originally inherited the names from the data objects passed in.\nIf we didn’t pass in objects with names, we get some default nonsense.\n\n\ndata.frame(1:5, c(\"a\", \"b\", \"c\", \"d\", \"e\"))\n\n  X1.5 c..a....b....c....d....e..\n1    1                          a\n2    2                          b\n3    3                          c\n4    4                          d\n5    5                          e\n\n\n\nWe can set the names explicitly when we create the data frame.\n\n\nx &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\ny &lt;- c(1, 3, 4, -1, 5, 6)\nz &lt;- 10:15\nmy_df &lt;- data.frame(char = x, data1 = y, data2 = z)\nmy_df\n\n  char data1 data2\n1    a     1    10\n2    b     3    11\n3    c     4    12\n4    d    -1    13\n5    e     5    14\n6    f     6    15\n\n\n\ndata.frame(number = 1:5, letter = c(\"a\", \"b\", \"c\", \"d\", \"e\"))\n\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n\n\nNote: A syntactically valid name consists of letters, numbers and the dot or underline characters. It must start with a letter or a dot not followed by a number.\nYou shouldn’t use . with names generally. Periods are often used for methods and other things in programming.\nYou also should avoid reserved words: if else repeat while function for in next break TRUE FALSE NULL Inf NaN NA NA_integer_ NA_real_ NA_complex_ NA_character_ (type ?make.names into the console for more details)\n\n\n\n\nLet’s check out the ‘built-in’ iris data frame\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nThis is a 2D structure and we can access just like a matrix!\n\n\niris[1:4, 2:4] #returns a data frame\n\n  Sepal.Width Petal.Length Petal.Width\n1         3.5          1.4         0.2\n2         3.0          1.4         0.2\n3         3.2          1.3         0.2\n4         3.1          1.5         0.2\n\n\n\niris[1, ] #returns a data frame\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n\n\n\niris[1:10, 1] #returns a vector\n\n [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9\n\n\n\nNote again that R simplifies objects sometimes! The last return was simplified to a vector.\nThis behavior actually comes from using the [ function. Everything you do in R is a function call!\nWe can avoid the behavior by adding the drop = FALSE argument\n\n\niris[1:10, 1, drop = FALSE] #return a data frame\n\n   Sepal.Length\n1           5.1\n2           4.9\n3           4.7\n4           4.6\n5           5.0\n6           5.4\n7           4.6\n8           5.0\n9           4.4\n10          4.9\n\n\n\nOr we can call the [ function as a prefix function (like our usual function calls)\n\n\n`[`(iris, 1:10, 1, drop = FALSE)\n\n   Sepal.Length\n1           5.1\n2           4.9\n3           4.7\n4           4.6\n5           5.0\n6           5.4\n7           4.6\n8           5.0\n9           4.4\n10          4.9\n\n\n\nUsually data frames have meaningful column names. We can use these for subsetting\n\n\niris[1:5 , c(\"Sepal.Length\", \"Species\")]\n\n  Sepal.Length Species\n1          5.1  setosa\n2          4.9  setosa\n3          4.7  setosa\n4          4.6  setosa\n5          5.0  setosa\n\n\n\nThe most common way to access a single column is via the $ operator. This returns a vector.\n\n\niris$Sepal.Length\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1\n [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0\n [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5\n [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1\n [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5\n [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3\n[109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2\n[127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\n\n\nRStudio fills in options for us!\n\nType iris$\n\nIf no choices - hit tab\nHit tab again to choose\n\n\n\n\n\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\n\nData Frame (2D data structure)\n\nCollection (list) of vectors of the same length\nCreate with data.frame() function\nAccess with [ , ] or $\nPerfect for most data sets!\nMost functions that read 2D data store it as a data frame (or tibble - a special data frame covered later)\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Data Frames"
    ]
  },
  {
    "objectID": "Notes/07-Base_R_Data_Structures_Vectors.html",
    "href": "Notes/07-Base_R_Data_Structures_Vectors.html",
    "title": "Base R Data Structures: Vectors",
    "section": "",
    "text": "A data scientist needs to deal with data! We need to have a firm foundation in the ways that we can store our data in R. This section goes through the most commonly used ‘built-in’ R objects that we’ll use.\n\nThere are five major data structures used in R\n\nAtomic Vector (1d)\nMatrix (2d)\nArray (nd)\nData Frame (2d)\nList (1d)\n\n\n\n\n\n\n\n\n\n\nDimension\nHomogeneous (elements all the same)\nHeterogeneous (elements may differ)\n\n\n\n\n1d\nAtomic Vector\nList\n\n\n2d\nMatrix\nData Frame\n\n\nnd\nArray\n\n\n\n\n\n\n(Atomic) Vector (1D group of elements with an ordering that starts at 1)\n\n\n\n\n\n\n\n\n\n\nElements must be same the same ‘type’ (homogeneous). The most common types of data are:\n\nlogical, integer, double, and character\n\n\n\n\nMany functions output a vector but the most common way to create one yourself is to use the c() function.\n\nc() “combines” values together\n\nSimply separate the values with a comma\n\n\n\n#vectors (1 dimensional) objects\n#all elements of the same 'type'\nx &lt;- c(1, 3, 10, -20, sqrt(2))\nx\n\n[1]   1.000000   3.000000  10.000000 -20.000000   1.414214\n\n\n\nRecall the str() function to investigate the structure of the object\n\n\nstr(x)\n\n num [1:5] 1 3 10 -20 1.41\n\n\n\nThe typeof() function tells us which type of data is stored in the vector\n\n\ntypeof(x)\n\n[1] \"double\"\n\n\n\nLet’s create another vector y with strings stored in it\n\n\ny &lt;- c(\"cat\", \"dog\", \"bird\", \"floor\")\ny\n\n[1] \"cat\"   \"dog\"   \"bird\"  \"floor\"\n\n\n\nstr(y)\n\n chr [1:4] \"cat\" \"dog\" \"bird\" \"floor\"\n\n\n\ntypeof(y)\n\n[1] \"character\"\n\n\n\nWe can combine two vectors together using c() as well!\n\n\nz &lt;- c(x, y)\nz\n\n[1] \"1\"               \"3\"               \"10\"              \"-20\"            \n[5] \"1.4142135623731\" \"cat\"             \"dog\"             \"bird\"           \n[9] \"floor\"          \n\n\n\nstr(z)\n\n chr [1:9] \"1\" \"3\" \"10\" \"-20\" \"1.4142135623731\" \"cat\" \"dog\" \"bird\" \"floor\"\n\n\n\ntypeof(z)\n\n[1] \"character\"\n\n\nNotice that R coerces the elements to the data type of the more flexible elements (character - R knows how to convert a number to a character string but doesn’t know how to convert a character string to a number). No warning is produced or message!\nYou’ll need to get used to how R does these kinds of things implicitly. Check out how R does coercion on some other types of data.\n\nc(TRUE, \"hat\", 3)\n\n[1] \"TRUE\" \"hat\"  \"3\"   \n\n\n\nc(c(TRUE, 3), \"hat\")\n\n[1] \"1\"   \"3\"   \"hat\"\n\n\nNotice that the order of operations above is important to understand! In the first line, R coerces the three elements together (character as it is the most flexible). In the second line, R first coerces TRUE and 3 together. TRUE values are treated as 1 (FALSE values as 0). Then the values of 1 and 3 are coerced to character strings to create the final vector.\n\nOne really useful function that creates a vector is the seq() (or sequence) function\n\n\nseq(from = 1, to = 10, by = 2)\n\n[1] 1 3 5 7 9\n\n\n\nseq(1, 10, 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nseq(1, 10, length = 5)\n\n[1]  1.00  3.25  5.50  7.75 10.00\n\n\n\nA shorthand for the seq() function is to use a :\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nThis can easily be modified to get other sequences.\n\n\n20:30/2\n\n [1] 10.0 10.5 11.0 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0\n\n\n\n1:15*3\n\n [1]  3  6  9 12 15 18 21 24 27 30 33 36 39 42 45\n\n\nNote that R is doing element-wise math. This is the default behavior of doing math on our common data structures (not just vectors!)\n\nAnother function that creates a vector is the runif() (random uniform number generator) function.\n\n\nrunif(4, min = 0, max = 1)\n\n[1] 0.80788121 0.62661495 0.08349343 0.51929678\n\n\n\nrunif(10)\n\n [1] 0.01572544 0.98532162 0.53603854 0.83801644 0.45467089 0.36109061\n [7] 0.12155311 0.40764787 0.01460073 0.81960244\n\n\n\nrunif(5, 20, 30)\n\n[1] 23.92915 26.37658 28.85627 20.01053 28.83144\n\n\nWe’ll find this function really useful when we simulate different quantities!\nYou might find the different ways to call a function confusing right now! We’ll talk about how to use the help files to understand function calls shortly!\n\n\n\nR objects can have attributes associated with them. The main attribute that a vector might have associated with it are names for the elements.\n\nu &lt;- c(\"a\" = 1, \"b\" = 2, \"c\" = 3)\nu\n\na b c \n1 2 3 \n\n\n\nattributes(u)\n\n$names\n[1] \"a\" \"b\" \"c\"\n\n\nThere is a special function for getting at the names of an R object. It is the names() function (nice choice there).\n\nnames(u)\n\n[1] \"a\" \"b\" \"c\"\n\n\nNames can be useful when it comes to subsetting and matching observations.\n\n\n\nWhen thinking about accessing (or subsetting) a vector’s elements, remember that vectors are 1D. We can place the numbers corresponding to the positions of the elements we want inside of [] at the end of the vector to return them. Note: R starts its counting at 1, not 0 like many other languages.\n\nReturn vector elements using square brackets [] at the end of a vector.\n\n\nletters #built-in vector\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\nletters[1] #R starts counting at 1!\n\n[1] \"a\"\n\n\n\nletters[26]\n\n[1] \"z\"\n\n\n\nCan ‘feed’ in a vector of indices to []\n\n\nletters[1:4]\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n\n\nletters[c(5, 10, 15, 20, 25)]\n\n[1] \"e\" \"j\" \"o\" \"t\" \"y\"\n\n\n\nx &lt;- c(1, 2, 5)\nletters[x]\n\n[1] \"a\" \"b\" \"e\"\n\n\nWe’d call x above an indexing vector\n\nUse negative indices to return a vector without certain elements\n\n\nletters[-(1:4)]\n\n [1] \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\"\n[20] \"x\" \"y\" \"z\"\n\n\n\nx &lt;- c(1, 2, 5)\nletters[-x]\n\n [1] \"c\" \"d\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\"\n[20] \"w\" \"x\" \"y\" \"z\"\n\n\n\nIf we have a names attribute associated, we can use names to access elements.\n\n\nu\n\na b c \n1 2 3 \n\n\n\nu[\"a\"]\n\na \n1 \n\n\n\nt &lt;- c(\"a\", \"c\")\nu[t]\n\na c \n1 3 \n\n\n\n\n\nLet’s look at a quick example of creating and modifying R vectors.\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\n\n(Atomic) Vector (1D group of elements with an ordering)\n\nVectors useful to know about as they are the most basic data object we’ll use\nseq() and :\nSubset vectors using vec[index]\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Vectors"
    ]
  },
  {
    "objectID": "Notes/07-Base_R_Data_Structures_Vectors.html#atomic-vector",
    "href": "Notes/07-Base_R_Data_Structures_Vectors.html#atomic-vector",
    "title": "Base R Data Structures: Vectors",
    "section": "",
    "text": "(Atomic) Vector (1D group of elements with an ordering that starts at 1)\n\n\n\n\n\n\n\n\n\n\nElements must be same the same ‘type’ (homogeneous). The most common types of data are:\n\nlogical, integer, double, and character\n\n\n\n\nMany functions output a vector but the most common way to create one yourself is to use the c() function.\n\nc() “combines” values together\n\nSimply separate the values with a comma\n\n\n\n#vectors (1 dimensional) objects\n#all elements of the same 'type'\nx &lt;- c(1, 3, 10, -20, sqrt(2))\nx\n\n[1]   1.000000   3.000000  10.000000 -20.000000   1.414214\n\n\n\nRecall the str() function to investigate the structure of the object\n\n\nstr(x)\n\n num [1:5] 1 3 10 -20 1.41\n\n\n\nThe typeof() function tells us which type of data is stored in the vector\n\n\ntypeof(x)\n\n[1] \"double\"\n\n\n\nLet’s create another vector y with strings stored in it\n\n\ny &lt;- c(\"cat\", \"dog\", \"bird\", \"floor\")\ny\n\n[1] \"cat\"   \"dog\"   \"bird\"  \"floor\"\n\n\n\nstr(y)\n\n chr [1:4] \"cat\" \"dog\" \"bird\" \"floor\"\n\n\n\ntypeof(y)\n\n[1] \"character\"\n\n\n\nWe can combine two vectors together using c() as well!\n\n\nz &lt;- c(x, y)\nz\n\n[1] \"1\"               \"3\"               \"10\"              \"-20\"            \n[5] \"1.4142135623731\" \"cat\"             \"dog\"             \"bird\"           \n[9] \"floor\"          \n\n\n\nstr(z)\n\n chr [1:9] \"1\" \"3\" \"10\" \"-20\" \"1.4142135623731\" \"cat\" \"dog\" \"bird\" \"floor\"\n\n\n\ntypeof(z)\n\n[1] \"character\"\n\n\nNotice that R coerces the elements to the data type of the more flexible elements (character - R knows how to convert a number to a character string but doesn’t know how to convert a character string to a number). No warning is produced or message!\nYou’ll need to get used to how R does these kinds of things implicitly. Check out how R does coercion on some other types of data.\n\nc(TRUE, \"hat\", 3)\n\n[1] \"TRUE\" \"hat\"  \"3\"   \n\n\n\nc(c(TRUE, 3), \"hat\")\n\n[1] \"1\"   \"3\"   \"hat\"\n\n\nNotice that the order of operations above is important to understand! In the first line, R coerces the three elements together (character as it is the most flexible). In the second line, R first coerces TRUE and 3 together. TRUE values are treated as 1 (FALSE values as 0). Then the values of 1 and 3 are coerced to character strings to create the final vector.\n\nOne really useful function that creates a vector is the seq() (or sequence) function\n\n\nseq(from = 1, to = 10, by = 2)\n\n[1] 1 3 5 7 9\n\n\n\nseq(1, 10, 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nseq(1, 10, length = 5)\n\n[1]  1.00  3.25  5.50  7.75 10.00\n\n\n\nA shorthand for the seq() function is to use a :\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nThis can easily be modified to get other sequences.\n\n\n20:30/2\n\n [1] 10.0 10.5 11.0 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0\n\n\n\n1:15*3\n\n [1]  3  6  9 12 15 18 21 24 27 30 33 36 39 42 45\n\n\nNote that R is doing element-wise math. This is the default behavior of doing math on our common data structures (not just vectors!)\n\nAnother function that creates a vector is the runif() (random uniform number generator) function.\n\n\nrunif(4, min = 0, max = 1)\n\n[1] 0.80788121 0.62661495 0.08349343 0.51929678\n\n\n\nrunif(10)\n\n [1] 0.01572544 0.98532162 0.53603854 0.83801644 0.45467089 0.36109061\n [7] 0.12155311 0.40764787 0.01460073 0.81960244\n\n\n\nrunif(5, 20, 30)\n\n[1] 23.92915 26.37658 28.85627 20.01053 28.83144\n\n\nWe’ll find this function really useful when we simulate different quantities!\nYou might find the different ways to call a function confusing right now! We’ll talk about how to use the help files to understand function calls shortly!\n\n\n\nR objects can have attributes associated with them. The main attribute that a vector might have associated with it are names for the elements.\n\nu &lt;- c(\"a\" = 1, \"b\" = 2, \"c\" = 3)\nu\n\na b c \n1 2 3 \n\n\n\nattributes(u)\n\n$names\n[1] \"a\" \"b\" \"c\"\n\n\nThere is a special function for getting at the names of an R object. It is the names() function (nice choice there).\n\nnames(u)\n\n[1] \"a\" \"b\" \"c\"\n\n\nNames can be useful when it comes to subsetting and matching observations.\n\n\n\nWhen thinking about accessing (or subsetting) a vector’s elements, remember that vectors are 1D. We can place the numbers corresponding to the positions of the elements we want inside of [] at the end of the vector to return them. Note: R starts its counting at 1, not 0 like many other languages.\n\nReturn vector elements using square brackets [] at the end of a vector.\n\n\nletters #built-in vector\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\nletters[1] #R starts counting at 1!\n\n[1] \"a\"\n\n\n\nletters[26]\n\n[1] \"z\"\n\n\n\nCan ‘feed’ in a vector of indices to []\n\n\nletters[1:4]\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n\n\nletters[c(5, 10, 15, 20, 25)]\n\n[1] \"e\" \"j\" \"o\" \"t\" \"y\"\n\n\n\nx &lt;- c(1, 2, 5)\nletters[x]\n\n[1] \"a\" \"b\" \"e\"\n\n\nWe’d call x above an indexing vector\n\nUse negative indices to return a vector without certain elements\n\n\nletters[-(1:4)]\n\n [1] \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\"\n[20] \"x\" \"y\" \"z\"\n\n\n\nx &lt;- c(1, 2, 5)\nletters[-x]\n\n [1] \"c\" \"d\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\"\n[20] \"w\" \"x\" \"y\" \"z\"\n\n\n\nIf we have a names attribute associated, we can use names to access elements.\n\n\nu\n\na b c \n1 2 3 \n\n\n\nu[\"a\"]\n\na \n1 \n\n\n\nt &lt;- c(\"a\", \"c\")\nu[t]\n\na c \n1 3 \n\n\n\n\n\nLet’s look at a quick example of creating and modifying R vectors.\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\n\n(Atomic) Vector (1D group of elements with an ordering)\n\nVectors useful to know about as they are the most basic data object we’ll use\nseq() and :\nSubset vectors using vec[index]\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Vectors"
    ]
  },
  {
    "objectID": "Notes/06-Quarto.html",
    "href": "Notes/06-Quarto.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "Data Science!\n\nRead in raw data and manipulate it\nCombine data sources\nSummarize data to glean insights\nApply common analysis methods\nCommunicate Effectively\n\nThis entire process must be reproducible! Git and GitHub certainly help but we also need to make sure that we document our thoughts and process as we work through cleaning our data, summarizing it, and fitting our models.\nThere are two major tools to enhance how we program that work very well with RStudio:\n\nR Markdown:\n\nAllows for writing of thought processes, code, and interpretations\nEasy to create many types of final documents: HTML pages, PDFs, slideshows, and more!\nCreated via a .Rmd (R markdown file)\n\nQuarto: Next generation version of R Markdown!\n\nBuilt to use multiple programming languages (R, python, and Julia) easily\nWorks with Jupyter Notebook format as well\nCreated via a .qmd (Quarto markdown file)\nRenders most .Rmd files as is!\n\n\nWe’ll go through the basics to get you started. Much more is available on the Quarto docs page, the RStudio Quarto integration page, and in the R for Data Science book.",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "Notes/06-Quarto.html#what-do-we-want-to-be-able-to-do",
    "href": "Notes/06-Quarto.html#what-do-we-want-to-be-able-to-do",
    "title": "Quarto Basics",
    "section": "",
    "text": "Data Science!\n\nRead in raw data and manipulate it\nCombine data sources\nSummarize data to glean insights\nApply common analysis methods\nCommunicate Effectively\n\nThis entire process must be reproducible! Git and GitHub certainly help but we also need to make sure that we document our thoughts and process as we work through cleaning our data, summarizing it, and fitting our models.\nThere are two major tools to enhance how we program that work very well with RStudio:\n\nR Markdown:\n\nAllows for writing of thought processes, code, and interpretations\nEasy to create many types of final documents: HTML pages, PDFs, slideshows, and more!\nCreated via a .Rmd (R markdown file)\n\nQuarto: Next generation version of R Markdown!\n\nBuilt to use multiple programming languages (R, python, and Julia) easily\nWorks with Jupyter Notebook format as well\nCreated via a .qmd (Quarto markdown file)\nRenders most .Rmd files as is!\n\n\nWe’ll go through the basics to get you started. Much more is available on the Quarto docs page, the RStudio Quarto integration page, and in the R for Data Science book.",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "Notes/06-Quarto.html#markdown-idea",
    "href": "Notes/06-Quarto.html#markdown-idea",
    "title": "Quarto Basics",
    "section": "Markdown Idea",
    "text": "Markdown Idea\nMarkdown is a simpler version of a markup language. HTML is the most commonly known markup language (HTML = Hypertext markup language). With HTML you use tags to specify things that a web browser like chrome interprets. For instance,\n&lt;h1&gt;My first level header&lt;/h1&gt;\n&lt;a href = \"https://www.google.com\"&gt;Link to a search engine.&lt;/a&gt;\nis HTML interpreted by your browser to be a header and a link. Markdown is a simpler version of this. There are many markdown languages (including Quarto markdown, R markdown, and GitHub markdown) but most have the same base structure. An equivalent way to write the above using markdown would be\n# My first level header\n[Link to a search engine.](https://www.google.com)\nWhere R markdown and Quarto go beyond is in the ability to weave R code into the equation!\n\nYou can include code chunks in your .Rmd or .qmd file.\nYou then render the document through RStudio (or the command line).\nThe R code runs and output can be included in the final document!\nIt is very awesome.\n\nQuarto is designed to be used in three ways (R for Data Science)\n\nTo communicate to decision makers (focus on conclusions not code)\nTo collaborate with other data scientists (including future you!)\nAs environment to do data science (documents what you did and what you were thinking)",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "Notes/06-Quarto.html#documenting-with-markdown-via-quarto",
    "href": "Notes/06-Quarto.html#documenting-with-markdown-via-quarto",
    "title": "Quarto Basics",
    "section": "Documenting with Markdown via Quarto",
    "text": "Documenting with Markdown via Quarto\nIt is easy to create many types of documents in Quarto! Go to file –&gt; New File –&gt; Quarto Document\n\n\n\n\n\n\n\n\n\nCreate a new Quarto document by selecting HTML (as that is the easiest to render - aka build the output). Save the template file that is produced (this will be a .qmd file).\n.qmd files contain three important types of content:\n\n(An optional) YAML header surrounded by ---s\n\nDefines meta data about the document\n\nChunks of R code\n\nCode that may evaluate and produce output when knitting or rendering the document\n\nText mixed with simple text formatting instructions (Markdown syntax)\n\nThere is a visual editor in RStudio (similar to a word processing program) and a source editor. I’d recommend starting with the visual editor and trying to move to the source quickly. You’ll be way more efficient using the source editor!\n\nYAML Header\nYAML stands for “Yet Another Markup Language” or “YAML ain’t markup language”. This defines settings for the creation process (when you go to render the document).\nAs we used an HTML for our document, you should see something similar to this in the top part of your document:\n---\ntitle: \"Untitled\"\nformat: html\neditor: visual\n---\nIf you render the document (which I’ll likely call Knitting as that is what it was called with R Markdown), it obeys these instructions for what to create.\n\nTry to render the document and see if you can get the HTML output.\n\nDo this via the “Render” button or by using hot keys: CTRL/CMD + Shift + k\n\n\n\n\nMarkdown Syntax\nIn the template document created you’ll also see some text. Some of it is larger, some of it links, some of it plain text. If you click on the “Source” you’ll see the markdown syntax that you can use to spice up your outputted document:\n\n\n\n\n\n\n\n\n\nThe syntax is really easy to pick up. Below you’ll find some commonly used markdown syntax:\n\n#RMarkdown \\(\\rightarrow\\) First level header\n## Next \\(\\rightarrow\\) Second level header\n**Knit** or __Knit__ \\(\\rightarrow\\) Bold font (Knit)\n*italic* or _italic_ \\(\\rightarrow\\) Italic font (italic)\n*__both__* \\(\\rightarrow\\) Bold and italic (both)\n&lt;https://rstudio.github.io/cheatsheets/Quarto.pdf&gt; \\(\\rightarrow\\) A hyperlink: https://rstudio.github.io/cheatsheets/Quarto.pdf\n[Cheat Sheet link](https://rstudio.github.io/cheatsheets/Quarto.pdf) \\(\\rightarrow\\) Cheat Sheet link\n\nCheck this site for markdown basics.\nNote: By using headers you can easily create a table of contents - which is very useful for accessibility of documents.\n\n\nCode Chunks\nThe real power of Quarto and R markdown is the ability to run R code when rendering and having the output show in the final document. This saves so much time and makes updating reports/documents a breeze!\nA code chunk looks like the following:\n\n\n\n\n\n\n\n\n\n\nStart a code chunk by typing out the syntax or with the shortcut ‘CTRL/CMD + Alt/Option + I’\n\nWhen rendering:\n\nChunks run sequentially in the document\nChunks share objects. Essentially an R environment is created when rendering a document and all objects created in chunks are stored in it.\nYou can specify behavior of each code chunk (show R code or hide it, evaluate or don’t evaluate) and set global chunk behavior\n\nTo change the behavior of code chunks, we use chunk options:\n\nHide/show code with echo = FALSE/TRUE\n\nChoose if code is evaluated with eval = TRUE/FALSE\n\nHave code evaluate, not show, and show no output with include = TRUE/FALSE\nTurn on/off displaying of messages or warnings with message = TRUE/FALSE and warning = TRUE/FALSE\n\nSpecifying these options in the top part of the code chunk is the R Markdown way of doing things (which is still acceptable in a .qmd file):\n\n\n\n\n\n\n\n\n\nHowever, the new, better, way of doing it is via special comments (similar to building an R package or your own API).\n\nOne important difference between R Markdown documents and Quarto documents is that in Quarto chunk options are typically included in special comments at the top of code chunks rather than within the line that begins the chunk. For example:\n\n\n\n\n\n\n\n\n\n\nWith Quarto, if you want to specify global chunk options the best way to do so is in the YAML header. Be very careful about spacing in YAML headers! You don’t want extra spaces anywhere and you need to have the correct indenting!\nHere is an example that would make all code chunks be ‘collapsed’ by default.\n---\ntitle: \"My Document\"\nformat: html\nknitr:\n  opts_chunk: \n    collapse: true\n---\nYou can also (still) set code chunk options in a setup code chunk. This is just a code chunk you put at the beginning of the document that sets options for you. Something like the following in a code chunk:\n#| include: false\nknitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE)\nThe include: false tells knitr not to include the code or output of this chunk in the final document.\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Quarto Basics"
    ]
  },
  {
    "objectID": "Notes/04-R_Basics_Landing.html",
    "href": "Notes/04-R_Basics_Landing.html",
    "title": "R Basics",
    "section": "",
    "text": "The video below discusses the basic use of R through RStudio. We discuss how to create an R object, how to learn more about R objects, and the basics of object oriented programming in R.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "`R` Basics"
    ]
  },
  {
    "objectID": "Notes/04-R_Basics_Landing.html#notes",
    "href": "Notes/04-R_Basics_Landing.html#notes",
    "title": "R Basics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "`R` Basics"
    ]
  },
  {
    "objectID": "Notes/02-Workflows_Git_GitHub_Basics_Landing.html",
    "href": "Notes/02-Workflows_Git_GitHub_Basics_Landing.html",
    "title": "Workflows & Git/GitHub Basics",
    "section": "",
    "text": "The video below discusses data workflows and the basics of Git and GitHub.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Workflows & `Git`/`GitHub` Basics"
    ]
  },
  {
    "objectID": "Notes/02-Workflows_Git_GitHub_Basics_Landing.html#notes",
    "href": "Notes/02-Workflows_Git_GitHub_Basics_Landing.html#notes",
    "title": "Workflows & Git/GitHub Basics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Workflows & `Git`/`GitHub` Basics"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website houses the learning materials for ST 558 - Data Science for Statisticians at NC State."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Statisticians",
    "section": "",
    "text": "Welcome to ST 558 - Data Science for Statisticians!\nIn this course we’ll look at common tasks done by data scientists:\nWe’ll adopt the R programming language to do so and learn about using quarto, git, and github to ensure our data analysis workflow is reproducible, has version control, and can easily include collaborators.",
    "crumbs": [
      "Home",
      "Data Science for Statisticians"
    ]
  },
  {
    "objectID": "index.html#course-learning-outcomes",
    "href": "index.html#course-learning-outcomes",
    "title": "Data Science for Statisticians",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\nAt the end of this course students will be able to\n\nexplain the steps and purpose of programs\nefficiently read in, combine, and manipulate data\nutilize help and other resources to customize programs\nwrite programs using good programming practices\nexplore data and perform common analyses\ncreate reports, web pages, and dashboards to display and communicate results",
    "crumbs": [
      "Home",
      "Data Science for Statisticians"
    ]
  },
  {
    "objectID": "index.html#weekly-to-do-list",
    "href": "index.html#weekly-to-do-list",
    "title": "Data Science for Statisticians",
    "section": "Weekly To-do List",
    "text": "Weekly To-do List\nGenerally speaking, each week will have a few videos to watch and readings to do as well as corresponding homework assignments. We’ll have some projects and exams as well. Please see the syllabus on Moodle for homework policies, project information, and exam information.",
    "crumbs": [
      "Home",
      "Data Science for Statisticians"
    ]
  },
  {
    "objectID": "index.html#getting-help",
    "href": "index.html#getting-help",
    "title": "Data Science for Statisticians",
    "section": "Getting Help!",
    "text": "Getting Help!\nTo obtain course help there are a number of options:\n\nDiscussion Forum on Moodle - This should be used for any question you feel comfortable asking and having others view. The TA, other students, and I will answer questions on this board. This will be the fastest way to receive a response!\n\nE-mail - If there is a question that you don’t feel comfortable asking the whole class you can use e-mail. The TA and I will be checking daily (during the regular work week).\nZoom Office Hour Sessions - These sessions can be used to share screens and have multiple users. You can do text chat, voice, and video. They are great for a class like this!",
    "crumbs": [
      "Home",
      "Data Science for Statisticians"
    ]
  },
  {
    "objectID": "index.html#summer-2025-course-schedule",
    "href": "index.html#summer-2025-course-schedule",
    "title": "Data Science for Statisticians",
    "section": "Summer 2025 Course Schedule",
    "text": "Summer 2025 Course Schedule\n\n\n\n\n\n\n\n\n\n\nTopic/Week\nWeek 1\n5/14-5/16\nW-F\nLearning Materials\n01 - Read - What is Data Science?\n02 - Watch - Workflows & Git/GitHub Basics\n03 - Read - Git & GitHub Practice\n04 - Watch - R Basics\n05 - Read & Watch - R projects and Connecting with Github\n06 - Read & Watch - Quarto\nAssignments\nHW 1 due Tu 5/20\nCode-alongs\nCode-alongs (optional attendance) on Thursdays\n\n\n\n\n\n\n\n\n\n\nWeek 2\n5/19-5/23\nM-F\n07 - Base R Data Structures: Vectors\n08 - Base R Data Structures: Matrices\n09 - Base R Data Structures: Data Frames\n10 - Base R Data Structures: Lists\n11 - Control Flow: Logicals & if/then/else\n12 - Control Flow: Loops\n13 - Control Flow: Vectorized Functions\n14 - Writing Functions\nHW 2 due Tu, 5/27\n\n\n\n\nWeek 3\n5/27-5/30\nT-F\n15 - Packages\n16 - Tidyverse Essentials\n17 - Reading Delimited Data\n18 - Reading Excel Data\n19 - Manipulating Data with dplyr\n20 - Manipulating Data with tidyr\n21 - Connecting to Databases\n22 - SQL Style Joins\nHW 3 due Tu, 6/3\n\n\n\n\nWeek 4\n6/2-6/6\nM-F\n23 - EDA Concepts\n24 - Summarizing Categorical Variables\n25 - Barplots & ggplot2 Basics\n26 - Numerical Variable Summaries\n27 - Numerical Variable Graphs & More ggplot2\nProject 1 due Tu, 6/17\n\n\n\n\nWeek 5\n6/9-6/13\nM-F\nNo new material\nExam-1 (Wednesday or Thursday)\n\n\n\n\nWeek 6\nM-W, F\n6/16-6/18, 6/20\n28 - Big Recap\n29 - Apply Family of Functions\n30 - purrr & List Columns\n31 - Advanced Function Writing\n32 - Querying APIs & Dealing with JSON Data\nHW 4 due Tu, 6/24\n\n\n\n\nWeek 7\nM-F\n6/23-6/27\n33 - R Shiny Basics & UI\n34 -R Shiny Server\n35 - Dynamic UI\n36 - Deploying, Debugging, & Other Useful Stuff\n\nProject 2 due Tu, 7/8\n\n\n\n\nWeek 8\nM-Th\n6/30-7/3\n37 - Modeling Concepts\n38 - Prediction & Training/Test Sets\n39 - Cross Validation\n40 - Multiple Linear Regression\n41 - Modeling with tidymodels (caret package)\n42 - tidymodels Tutorial\n43 - LASSO Models\n44 - Models recap\n\n\n\n\n\nWeek 9\nM-F\n7/7-7/11\n45 - Logistic Regression\n46 - Regression and Classification Trees\n47 - Ensemble Trees (Random Forest; Bagged Trees)\n48 - Fitting Classification Trees\nHW 5 due Tu, 7/15\n\n\n\n\nWeek 10\nM-F\n7/14-7/18\n49 - Creating an API\n50 - Docker Basics\n51 - Building a Docker Image\n52 - Dockerizing Shiny Apps\nExam 2 (Wednesday or Thursday)\nProject Final due 7/29\n\n\n\n\nWeek 11\n7/21-7/25\nNo new material. Project work time!\n\n\n\n\n\nWeek 12\nM,Tu\n7/28-7/29\nFinal Examinations (Project due 7/29)",
    "crumbs": [
      "Home",
      "Data Science for Statisticians"
    ]
  },
  {
    "objectID": "Notes/01-What_is_Data_Science.html",
    "href": "Notes/01-What_is_Data_Science.html",
    "title": "What is Data Science?",
    "section": "",
    "text": "First, let’s get some perspectives on what data science is, how statistics and computer science relate, and important items to consider when trying to understand data. To do so, we’ll do some readings.\n\nIntroduction of R for Data Science\nChapters 1 and 2 of the Practitioner’s Guide to Data Science\nThe role of statistics in Data Science and Artificial Intelligence\nData Science wikipedia entry\n\nSome entries from some less reputable/reviewed places are below. Please read these as well but put less weight on the things mentioned in these articles - I’m just assigning these readings so you can see the different perspectives people may have.\n\nhttps://www.indeed.com/career-advice/finding-a-job/data-scientist-vs-statistician\nData Science vs. Data Analytics vs. Machine Learning - Expert Talk\nhttps://scientistcafe.com/ids/comparison-between-statistician-and-data-scientist.html\n\nYou’ll write about these ideas as part of your first homework assignment! For now, onto understanding data workflows and Git and GitHub basics!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "What is Data Science?"
    ]
  },
  {
    "objectID": "Notes/03-Git_GitHub_Practice.html",
    "href": "Notes/03-Git_GitHub_Practice.html",
    "title": "Git & GitHub Practice",
    "section": "",
    "text": "Hopefully you now have some idea about the purpose of Git/GitHub. GitHub is an online hosting service for Git repositories. If you’ve never used Git and GitHub before it is really quite intimidating. We’ll start small and build as we go through the semester! Now let’s do some practice.\n\nFirst, go to github.com and signup for a GitHub account (if you haven’t already).\nOnce you have an account, read this introduction to GitHub (we’ll look at using Git and the command line shortly - for now we’ll stick to the web editor).\n\nFollow the steps in the article to create your first repository and do some Git actions!\n\n\nOnce you’ve completed the above part. You should download Git to your computer.\n\nSee the installing Git page or the corresponding chapter from happy git with R (this one includes some troubleshooting as well).\nFor Windows, there aren’t many things to select as you install but go ahead and include the bash terminal if that’s an option.\n\nNow read through the introduce yourself to Git chapter of happygitwithr.\n\nFor those that want to use the shell/terminal: Mac has a native shell/terminal/command line interpreter (usually called terminal in your launchpad area).  For Windows folks, after you’ve installed Git you’ll have access to the Git Bash terminal.  Access this by clicking on the start menu and typing in bash. You can also use the command prompt in Windows (cmd in the start menu). I’ll use a bit of both the bash terminal and the Windows command prompt. Commands differ but the functionality is very similar.\n\nGreat, now try to work through chapters 9, 11, and 12 of happygitwithr.\n\nIf you get stuck here, that’s ok. Just move on! This part isn’t really required right now but should be figured out soon so that you can start working seamlessly!\nIf stuck, there is a bit more material about Git/GitHub/RStudio this week. Check that out and then maybe come back here.\nSet up a meeting with me if you can’t figure this part out soon!\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "`Git` & `GitHub` Practice"
    ]
  },
  {
    "objectID": "Notes/03-Git_GitHub_Practice.html#getting-comfortable-with-github",
    "href": "Notes/03-Git_GitHub_Practice.html#getting-comfortable-with-github",
    "title": "Git & GitHub Practice",
    "section": "",
    "text": "Hopefully you now have some idea about the purpose of Git/GitHub. GitHub is an online hosting service for Git repositories. If you’ve never used Git and GitHub before it is really quite intimidating. We’ll start small and build as we go through the semester! Now let’s do some practice.\n\nFirst, go to github.com and signup for a GitHub account (if you haven’t already).\nOnce you have an account, read this introduction to GitHub (we’ll look at using Git and the command line shortly - for now we’ll stick to the web editor).\n\nFollow the steps in the article to create your first repository and do some Git actions!\n\n\nOnce you’ve completed the above part. You should download Git to your computer.\n\nSee the installing Git page or the corresponding chapter from happy git with R (this one includes some troubleshooting as well).\nFor Windows, there aren’t many things to select as you install but go ahead and include the bash terminal if that’s an option.\n\nNow read through the introduce yourself to Git chapter of happygitwithr.\n\nFor those that want to use the shell/terminal: Mac has a native shell/terminal/command line interpreter (usually called terminal in your launchpad area).  For Windows folks, after you’ve installed Git you’ll have access to the Git Bash terminal.  Access this by clicking on the start menu and typing in bash. You can also use the command prompt in Windows (cmd in the start menu). I’ll use a bit of both the bash terminal and the Windows command prompt. Commands differ but the functionality is very similar.\n\nGreat, now try to work through chapters 9, 11, and 12 of happygitwithr.\n\nIf you get stuck here, that’s ok. Just move on! This part isn’t really required right now but should be figured out soon so that you can start working seamlessly!\nIf stuck, there is a bit more material about Git/GitHub/RStudio this week. Check that out and then maybe come back here.\nSet up a meeting with me if you can’t figure this part out soon!\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "`Git` & `GitHub` Practice"
    ]
  },
  {
    "objectID": "Notes/05-R_Projects_Connecting_with_GitHub.html",
    "href": "Notes/05-R_Projects_Connecting_with_GitHub.html",
    "title": "R projects and Connecting with Github",
    "section": "",
    "text": "When considering our data science workflow, we want to have a seamless way to\nAll of this can be accomplished using R projects with git and github!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "R projects and Connecting with Github"
    ]
  },
  {
    "objectID": "Notes/05-R_Projects_Connecting_with_GitHub.html#r-projects",
    "href": "Notes/05-R_Projects_Connecting_with_GitHub.html#r-projects",
    "title": "R projects and Connecting with Github",
    "section": "R Projects",
    "text": "R Projects\nFirst, it is important to know that we are going to be reading in some files locally. That is, from a folder somewhere on your computer (as opposed to reading a file in from the web). That means we need to be able to tell R where a file exists on our computer.\n\nLocating Files\nHow does R locate the file?\n\nWe can give a full path name to the file\n\nex: C:/Users/jbpost2/Documents/Repos/ST-558/datasets/\nex: C:\\\\\\\\Users\\\\\\\\jbpost2\\\\\\\\Documents\\\\\\\\Repos\\\\\\\\ST-558\\\\\\\\datasets\n\n\nNote: The \\ character is called an escape character. This allows us to use different symbols and things when we are working within a string of text. For instance, \\n is a line break. The \\ tells R to interpret the next character(s) in a different way than usual.\nFor example (cat is kind of like a different version of a print function):\n\ncat(\"Hi my name is Justin. I work at NC State.\")\n\nHi my name is Justin. I work at NC State.\n\n\nwith the \\n in there we get a line break:\n\ncat(\"Hi my name is Justin.\\nI work at NC State.\")\n\nHi my name is Justin.\nI work at NC State.\n\n\nTherefore, when we specify a path to a file as a string, if we try to use a \\ we actually need two \\\\ so that R knows we actually want a \\! Confusing I know. But, we can just replace \\ with / in paths to files to avoid that.\n\nFull path names are not good to use generally!\n\nIf you share your code with someone else, they don’t have the same folder structure, username, etc.\nInstead, use a relative path. That is, a path from R’s current working directory (the place it looks by default)\n\nDetermine R’s working directory via getwd() (get working directory)\n\n\ngetwd()\n\n[1] \"C:/Users/esmeyer2/Documents/repos/ST-558-Data-Science-for-Statisticians/Notes\"\n\n\nNow if I had my file in the same folder as my working directory, I don’t need to use a full path as R is looking in that folder by default. If I had the file chickens.csv in my working directory, I could tell R where it is via something like:\n\nread.csv(\"chickens.csv\")\n\n\nGreat! How do we set that working directory? Via setwd() (set working directory)\n\n\nsetwd(\"C:/Users/jbpost2/Documents/Repos/ST-558/datasets/\")\n\n\nWe can also set it via menus:\n\n\n\n\n\n\n\n\n\n\n\nOk, but our goal is to share our code with others so they can run it. We could say to our collaborators, “just to update that one line of code to change your working directory and make sure to have all the files in the same directory that we use for this analysis.” Clearly, that leaves something to be desired….\nInstead we can use R projects!\n\n\n\nUsing an R Project\n\nAs we often have many files associated with each analysis, it can make keeping analyses separate difficult. The project feature in RStudio is made to alleviate this!\n\nR projects provide a straightforward way to divide your work into multiple contexts. Each with their own:\n\nWorking directory\nWorkspace (environment with stored data/objects)\nHistory\nFolder structure & source documents\n\nThey are very easy to create!\n\n\n\n\n\n\n\n\n\n\nWhen you create an R project, you might note that it gets associated with a directory (or folder or repo). That folder is what the project uses as the working directory.\n\nThis is important! This means if we can share an entire folder (with subfolders and everything else in the same relative place), another user can pick up an R project and the paths to things should work! (Assuming we’ve used relative paths for everything.)\nThat’s exactly how we’ll use R projects with github! Github is a remote folder (that can have subfolders and what-not). If we associate an R project with that folder and upload that, another user can download it and open that R project, allowing them to work seamlessly!\nCaveat - all R packages and versions of R packages are not necessarily going to be the same on someone else’s computer. There are ways to require certain versions (see renv files) but we’ll leave that for another time.\n\nYou might create a new project for materials related to this course or for each homework assignment etc. It is up to you how much clarity you want on a specific folder/project you are working on.\nNote you can quickly switch between projects in the upper right hand of RStudio or via the File menu.",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "R projects and Connecting with Github"
    ]
  },
  {
    "objectID": "Notes/05-R_Projects_Connecting_with_GitHub.html#connecting-gitgithub-with-rstudio",
    "href": "Notes/05-R_Projects_Connecting_with_GitHub.html#connecting-gitgithub-with-rstudio",
    "title": "R projects and Connecting with Github",
    "section": "Connecting Git/GitHub with RStudio",
    "text": "Connecting Git/GitHub with RStudio\nIdeally we want to document our process, easily collaborate, and widely share our work\n\nFor reproducibility, ideally we would save different versions of our analysis, write-up, etc. along the way\nRemember that Git is a version control software to help:\n\nTrack the changes we commit to the files\nAllow multiple users to work on the same project\n\nGitHub is a hosting service that allows us to do Git-based projects on the internet and share them widely!\n\nRecall our basic workflow. First we create a repo on GitHub (remotely). We then associate a folder on our local computer with that repo using Git. Then we:\n\nPull down most recent files (git pull) or do initial download (git clone)\nAdd files you want to keep changes to (git add)\nCommit to the changes (git commit)\nPush the changes to the remote repo (git push)\n\nLet’s go through some explicit steps to do this! Make sure you’ve downloaded Git to your machine already (see earlier material on Git/GitHub)!\n\nCloning a Repo & Working in RStudio\nWe don’t want to use the github.com web interface as that is kind of inefficient.\n\nOne option is to clone the repo (i.e. download the entire repo locally).\n\nRepo main page has a green button. Click on that.\nCan download a zip and unzip it to an appropriate folder\n\n\n\n\n\n\n\n\n\n\n\n\nA better option is to clone the repo via the URL and use RStudio! Open RStudio,\n\ngo to File –&gt; new project\nselect from version control\nchoose Git\npaste in the repo link\nselect a directory to save this repo in\nhit create project!\n\nNow you have the files locally and this associates an R project with a Git repo!\n\nTry this out on your own! Create your own repo on GitHub. Then try to download it as an R project.\n\n\nCommunication Between GitHub and RStudio\nWe need to make sure RStudio and github can communicate. This can sometimes be tough to get working! Do the following:\n\nModify a file in the github repo you just created and downloaded.\nGo to the Git tab in your Environment area\nYou should see any files where you’ve made changes\nYou can add files that you’d like to commit up to the remote repo\nClick on all of the boxes (equivalent to git add -A when using the command line) and click the Commit button\nA window pops up with a comparison of files. When satisfied, write a commit message in the box and click the commit button (equivalent to git commit -m \"message\" when using the command line)\nHit close on that window\nClick the push button in the top right (equivalent to git push when using the command line)\nYou may be prompted to log-in in some way. If successful, the repo on gitub.com should show the changes!\n\n\n\nUsing the Command Line Interface (CLI)\nWhen working by myself on a repo, I’m not worried about merge conflicts with other people’s changes. As such, my workflow is as follows:\n\nOpen the appropriate project in RStudio\nGo to the Terminal and type git pull\nWork… at a good spot for saving, back to the terminal\n\nType git add -A to add all files that have been modified\nType git commit -m \"Message\" to stage a commit\nType git push to push the local changes to the remote repo\n\n\n\n\nCreating a Repo From an Existing R Project\nSometimes you’ll have an R project that already exists but you don’t have a corresponding repo on GitHub. The easiest way to get that project into a repo is to do the following:\n\nCreate a new repository on GitHub\nClone it to your computer via RStudio new project (in a different folder than your current project)\nMove all the files from the R Project you already have into the new project folder\nadd, commit, and push up the files\n\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "R projects and Connecting with Github"
    ]
  },
  {
    "objectID": "Notes/06_5-Week1_2_em.html",
    "href": "Notes/06_5-Week1_2_em.html",
    "title": "Week 2 Overview",
    "section": "",
    "text": "This wraps up the content for week 1. Now we require some practice! You should head back to our Moodle site to check out your homework assignment for this week.\nThis second week we’ll go through a systematic view of the most common data objects you’ll see in R. We’ll see how to access these objects using Base R commands.\nWe learned, and will continue investigating an R object via\nWe see that R commonly uses the attributes of the object to determine how to apply (or dispatch) a function. We will look at common R objects and how to access their elements:\nand continue to practice documenting our code by using a notebook environment such as Quarto (along with git/github for version control and sharing/collaborating).\nNow we want to look at how to control the execution of our code. The three main things we’ll look at here are\nThen we’ll see how to write our own functions! We are already learning a lot of necessary material to be a data scientist. Let’s go!!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Week 2 Overview"
    ]
  },
  {
    "objectID": "Notes/06_5-Week1_2_em.html#week-2-additional-readingslearning-materials",
    "href": "Notes/06_5-Week1_2_em.html#week-2-additional-readingslearning-materials",
    "title": "Week 2 Overview",
    "section": "Week 2 Additional Readings/Learning Materials",
    "text": "Week 2 Additional Readings/Learning Materials\n\nCommon Data Objects in R\n\nChapter 12 of R 4 Data Science\nChapter 27 of R 4 Data Science (27.1-27.3)\n\n\n\nWriting Functions\n\nChapter 25 of R 4 Data Science\nChapter 5 of Advanced R\nConsistent naming conventions in R&gt; and R’s style guide\nChapter 7 of Modern R with the tidyverse\n\n\n\nLoops and Vectorized Functions\n\nChapter 27 of R 4 Data Science (27.4-27.5)",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Week 2 Overview"
    ]
  },
  {
    "objectID": "Notes/06_5-Week1_2_em.html#week-2-learning-objectives",
    "href": "Notes/06_5-Week1_2_em.html#week-2-learning-objectives",
    "title": "Week 2 Overview",
    "section": "Week 2 Learning Objectives",
    "text": "Week 2 Learning Objectives\nUpon completion of this week, students will be able to: (CO is the corresponding course learning objective this helps build toward)\n\nR Basics\n\n  create common data objects such as data frames, matrices, vectors, and lists and subset each of them (CO 1, 2, 3)\n\nrecognize the most appropriate method for accessing an R object b. determine the appropriate type of object in which to save differing types of data c. contrast matrices and data frames d. determine the type of R object you are working with e. compare and contrast the attributes of the common data types\n\n  utilize R help and other resources to appropriately use and/or modify an R function or object (CO 3)\n\nUpon completion of this week, students will be able to: (CO is the corresponding course learning objective this helps build toward)\n\n\nLogicals & if/then/else\n\ndescribe the idea of coercion and compare and contrast the use of implicit and explicit coercion (CO 1, 4)\na.    outline the rules for implicit coercion b.    use common explicit coercion functions c.    hypothesize the type of coercion implicitly done by a section of R code\n\n\n\nFor Loops and Vectorized Functions\n\ndescribe the process of a for loop, while loop, and repeat statement in R (CO 1, 3, 4)\na.    program R to break from a loop when conditions are met b.    program R to skip an iteration of a loop c.    utilize different “counters” or “iterators” in an R loop d.    explain the use and process of nested for loops e.    program a nested R loop in R f.    explain why loops are not always ideal in R\n   program using the “apply family” of functions in R (CO 1, 3, 4)\na.    differentiate between the apply family of functions in R including, but not limited to, mapply, sapply, lapply, apply, and replicate b.    manipulate R code that uses loops to instead use the proper apply function c.    utilize anonymous functions in calls to the apply family of functions in R\n   recall common “vectorized” functions in R such as colMeans, rowMeans, ifelse, etc. (CO 1, 4)\n   write their own vectorized version of a function in R (CO 1, 4)\n   describe what an anonymous function is and how it differs from other functions (CO 1, 4)\n\n\n\nWriting Functions\nNote: Not all of these are covered in this week’s function writing videos - we’ll revisit the topic later!\n\ndisplay code from previously written functions in R (CO 1, 3)\ndescribe the parts of an R function (CO 1, 4)\ndescribe the difference between a prefix and an infix function (CO 1, 4)\n\nunderstand the order of evaluation for user created and built-in infix functions\n\nwrite an R function that has default arguments, allows for additional arguments, and returns a well-structured named object (CO 1, 3, 4)\n\ndetermine appropriate uses for the stop function\ndevelop a pipeable function that acts on a data frame and one that produces side effects\ncreate a custom infix function\n\ndescribe the way that inputs can be specified to a function (CO 1, 3)\n\ndescribe positional matching and predict what input values will be assigned to which function values\nexplain lazy evaluation and give examples of where it may be important\n\nuse if/then/else logic to conditionally execute code\nutilize the switch function and compare and contrast its use against if/then/else logic (CO 1, 3, 4)\ndefine environments including temporary environments (CO 1)\n\nsummarize the order in which R attempts to find a requested object\nsketch out the R environment framework and explain the idea of lexical scoping\ndiscuss the concept of local variables and local environments as compared to global variables and environments\nstate the naming conventions for R objects\n\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Week 2 Overview"
    ]
  },
  {
    "objectID": "Notes/08-Base_R_Data_Structures_Matrices.html",
    "href": "Notes/08-Base_R_Data_Structures_Matrices.html",
    "title": "Base R Data Structures: Matrices",
    "section": "",
    "text": "A data scientist needs to deal with data! We need to have a firm foundation in the ways that we can store our data in R. This section goes through the most commonly used ‘built-in’ R objects that we’ll use.\n\nThere are five major data structures used in R\n\nAtomic Vector (1d)\nMatrix (2d)\nArray (nd)\nData Frame (2d)\nList (1d)\n\n\n\n\n\n\n\n\n\n\nDimension\nHomogeneous (elements all the same)\nHeterogeneous (elements may differ)\n\n\n\n\n1d\nAtomic Vector\nList\n\n\n2d\nMatrix\nData Frame\n\n\nnd\nArray\n\n\n\n\n\n\nVectors are useful to know about but generally not great for dealing with a dataset. When we think of a dataset, we often think about a spreadsheet having rows corresponding to observations and columns corresponding to variables.\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\nHere the observations correspond to measurements on flowers. The variables that were measured on each flower are the columns.\nWe can see that a vector is not really useful for handling this kind of ‘standard’ data since it is 1D. However, we can think of vectors as the building blocks for more complicated types of data.\nAs matrices are homogeneous (all elements must be the same type), we can think of a matrix as a collection of vectors of the same type and length (although this is not formally how it works in R). Think of each column of the matrix as a vector.\n\n\n\nWe can create a matrix using the matrix() function. Looking at the help for matrix we see the function definition as:\n\n\nmatrix(data = NA, \n       nrow = 1, \n       ncol = 1, \n       byrow = FALSE,\n       dimnames = NULL)\n\nwhere data is\n\nan optional data vector (including a list or expression vector). Non-atomic classed R objects are coerced by as.vector and all attributes discarded.\n\nOk, so really we need to supply a vector of data. Then the nrow and ncol arguments define how many rows and columns. The byrow argument is a Boolean telling R whether or not to fill in the matrix by row or by column (the default). dimnames is an optional attribute. We’ll look at that shortly.\nA basic call to matrix() might look like this:\n\nmy_mat &lt;- matrix(c(1, 3, 4, -1, 5, 6), \n                 nrow = 3, \n                 ncol = 2)\nmy_mat\n\n     [,1] [,2]\n[1,]    1   -1\n[2,]    3    5\n[3,]    4    6\n\n\nNotice how the values fill in the first column then the second column. This is the default behavior but can be changed via the byrow argument.\n\nmy_mat &lt;- matrix(c(1, 3, 4, -1, 5, 6), \n                 nrow = 3, \n                 ncol = 2,\n                 byrow = TRUE)\nmy_mat\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    4   -1\n[3,]    5    6\n\n\nLet’s think of a matrix as a collection of vectors of the same type and length (how you might think of a dataset). We could construct the matrix by giving appropriate vectors of the same type and length.\n\n#populate vectors\nx &lt;- rep(0.2, times = 6)\ny &lt;- c(1, 3, 4, -1, 5, 6)\n\n\nCheck they are the same type of element. In this case, as long as they are both numeric, we can put them together and not lose any precision (integers are coerced to doubles if needed).\n\n\nstr(x)\n\n num [1:6] 0.2 0.2 0.2 0.2 0.2 0.2\n\nis.numeric(x)\n\n[1] TRUE\n\n\n\nstr(y)\n\n num [1:6] 1 3 4 -1 5 6\n\nis.numeric(y)\n\n[1] TRUE\n\n\n\nA length() function exists. We can use that to see the vectors have the same number of elements.\n\n\n#check 'length'\nlength(x)\n\n[1] 6\n\nlength(y)\n\n[1] 6\n\n\n\nNow construct the matrix by combining the vectors together with c().\n\n\nmy_mat2 &lt;- matrix(c(x, y), ncol = 2)\nmy_mat2\n\n     [,1] [,2]\n[1,]  0.2    1\n[2,]  0.2    3\n[3,]  0.2    4\n[4,]  0.2   -1\n[5,]  0.2    5\n[6,]  0.2    6\n\n\nAs byrow = FALSE is the default, the vectors become the columns!\nThat being said, the way to really think about the matrix is as a long vector we are giving dimensions to. In fact, if we coerce our matrix to a vector explicitly (via the as.vector() function), we see the original data vector we passed in.\n\nas.vector(my_mat2)\n\n [1]  0.2  0.2  0.2  0.2  0.2  0.2  1.0  3.0  4.0 -1.0  5.0  6.0\n\n\n\nMatrices don’t have to be made up of numeric data. As long as the elements are all the same type, we are good to go!\n\n\nx &lt;- c(\"Hi\", \"There\", \"!\")\ny &lt;- c(\"a\", \"b\", \"c\")\nz &lt;- c(\"One\", \"Two\", \"Three\")\nmatrix(c(x, y, z), nrow = 3)\n\n     [,1]    [,2] [,3]   \n[1,] \"Hi\"    \"a\"  \"One\"  \n[2,] \"There\" \"b\"  \"Two\"  \n[3,] \"!\"     \"c\"  \"Three\"\n\n\n\nNotice that we don’t need to specify the number of columns. Generally, we need to only specify the nrow or ncol argument and R figures the rest out!\nWe do need to be careful about how R recycles things:\n\n\nmatrix(c(x, y, z), ncol = 2)\n\nWarning in matrix(c(x, y, z), ncol = 2): data length [9] is not a sub-multiple\nor multiple of the number of rows [5]\n\n\n     [,1]    [,2]   \n[1,] \"Hi\"    \"c\"    \n[2,] \"There\" \"One\"  \n[3,] \"!\"     \"Two\"  \n[4,] \"a\"     \"Three\"\n[5,] \"b\"     \"Hi\"   \n\n\n\nWe were 1 element short of being able to fill a matrix with 2 columns and 5 rows (5 chosen to make sure all the data was included). R recycles the first element we passed to fill in the last value. This is a common thing that R does. It can be useful for shorthanding things but otherwise is just something we need to be aware of as a common error to be fixed!\nLet’s shorthand create a matrix of all 0’s.\n\n\nmatrix(0, nrow = 2, ncol = 2)\n\n     [,1] [,2]\n[1,]    0    0\n[2,]    0    0\n\n\n\n\n\nSimilar to a vector, a matrix can have attributes (this is true of any R object!). The common attributes associated with a matrix are the dim or dimensions and the dimnames or dimension names.\n\nmy_iris &lt;- as.matrix(iris[, 1:4])\nhead(my_iris)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]          5.1         3.5          1.4         0.2\n[2,]          4.9         3.0          1.4         0.2\n[3,]          4.7         3.2          1.3         0.2\n[4,]          4.6         3.1          1.5         0.2\n[5,]          5.0         3.6          1.4         0.2\n[6,]          5.4         3.9          1.7         0.4\n\n\n\nstr(my_iris)\n\n num [1:150, 1:4] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n\n\n\nattributes(my_iris)\n\n$dim\n[1] 150   4\n\n$dimnames\n$dimnames[[1]]\nNULL\n\n$dimnames[[2]]\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n\n\nWe can access these attributes through functions designed for them:\n\ndim(my_iris)\n\n[1] 150   4\n\n\n\ndimnames(my_iris)\n\n[[1]]\nNULL\n\n[[2]]\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n\n\nHere we can name the rows and the columns of a matrix by assigning a list() to the dimnames attribute. (Lists are covered shortly.)\n\ndimnames(my_iris) &lt;- list(\n  1:150, #first list element is a vector for the row names\n  c(\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\") #second element is a vector for column names\n)\nhead(my_iris)\n\n  Sepal Length Sepal Width Petal Length Petal Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\n\nWe can assign these when we create a matrix!\n\nmy_mat3 &lt;- matrix(c(runif(10), \n                    rnorm(10),\n                    rgamma(10, shape = 1, scale = 1)),\n                  ncol = 3,\n                  dimnames = list(1:10, c(\"Uniform\", \"Normal\", \"Gamma\")))\nmy_mat3\n\n      Uniform      Normal     Gamma\n1  0.17917969  1.20420196 1.8343261\n2  0.90436881 -0.76908382 0.3196138\n3  0.28268897 -0.77996655 0.8879761\n4  0.85723002  1.89686632 1.5989007\n5  0.92261973 -0.07752369 0.7245798\n6  0.61851780  0.18301513 1.9846702\n7  0.07360534 -1.10904253 0.9916780\n8  0.54449506 -0.67551690 0.5018890\n9  0.91895819 -0.27763180 0.4849135\n10 0.39869803  1.68097820 0.4110543\n\n\n\n\n\nWe saw that [] allowed us to access the elements of a vector. We’ll use the same syntax to get at elements of a matrix. However, we now have two dimensions! That means we need to specify which row elements and which column elements. This is done by using square brackets with a comma in between our indices.\n\nNotice the default row names and column names! This gives us hints on this syntax!\n\n\nmat &lt;- matrix(c(1:4, 20:17), ncol = 2)\nmat\n\n     [,1] [,2]\n[1,]    1   20\n[2,]    2   19\n[3,]    3   18\n[4,]    4   17\n\n\n\nmat[2, 2]\n\n[1] 19\n\n\n\nIf we leave an index blank, we get the entirety of that index back.\n\n\nmat[ , 1]\n\n[1] 1 2 3 4\n\n\n\nmat[2, ]\n\n[1]  2 19\n\n\n\nmat[2:4, 1]\n\n[1] 2 3 4\n\n\n\nmat[c(2, 4), ]\n\n     [,1] [,2]\n[1,]    2   19\n[2,]    4   17\n\n\nNotice that R simplifies the result where possible. That is, returns an atomic vector if you have only 1 dimension and a matrix if two.\nAlso, if you only give a single value in the [] then R uses the count of the value in the matrix (essentially treating the matrix elements as a long vector). These counts go down columns first.\n\nIf you do have dimnames associated, then you can access elements with those.\n\n\nmat &lt;- matrix(c(1:4, 20:17), ncol = 2,\n          dimnames = list(NULL,\n                c(\"First\", \"Second\"))\n        )\nmat\n\n     First Second\n[1,]     1     20\n[2,]     2     19\n[3,]     3     18\n[4,]     4     17\n\n\n\nmat[, \"First\"]\n\n[1] 1 2 3 4\n\n\n\n\n\n\nArrays are the n-dimensional extension of matrices. Like matrices, they must have all elements of the same type.\n\nmy_array &lt;- array(1:24, dim = c(4, 2, 3))\nmy_array\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n, , 2\n\n     [,1] [,2]\n[1,]    9   13\n[2,]   10   14\n[3,]   11   15\n[4,]   12   16\n\n, , 3\n\n     [,1] [,2]\n[1,]   17   21\n[2,]   18   22\n[3,]   19   23\n[4,]   20   24\n\n\nAccessing elements is similar to vectors and matrices!\n\nmy_array[1, 1, 1]\n\n[1] 1\n\n\n\nmy_array[4, 2, 1]\n\n[1] 8\n\n\nArrays are often used with images in deep learning.\n\n\n\nA 2D object where all elements are of the same type\nAccess elements via [ , ]\nNot ideal for data since all elements must be the same type (see data frames!)\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Matrices"
    ]
  },
  {
    "objectID": "Notes/08-Base_R_Data_Structures_Matrices.html#matrix",
    "href": "Notes/08-Base_R_Data_Structures_Matrices.html#matrix",
    "title": "Base R Data Structures: Matrices",
    "section": "",
    "text": "Vectors are useful to know about but generally not great for dealing with a dataset. When we think of a dataset, we often think about a spreadsheet having rows corresponding to observations and columns corresponding to variables.\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\nHere the observations correspond to measurements on flowers. The variables that were measured on each flower are the columns.\nWe can see that a vector is not really useful for handling this kind of ‘standard’ data since it is 1D. However, we can think of vectors as the building blocks for more complicated types of data.\nAs matrices are homogeneous (all elements must be the same type), we can think of a matrix as a collection of vectors of the same type and length (although this is not formally how it works in R). Think of each column of the matrix as a vector.\n\n\n\nWe can create a matrix using the matrix() function. Looking at the help for matrix we see the function definition as:\n\n\nmatrix(data = NA, \n       nrow = 1, \n       ncol = 1, \n       byrow = FALSE,\n       dimnames = NULL)\n\nwhere data is\n\nan optional data vector (including a list or expression vector). Non-atomic classed R objects are coerced by as.vector and all attributes discarded.\n\nOk, so really we need to supply a vector of data. Then the nrow and ncol arguments define how many rows and columns. The byrow argument is a Boolean telling R whether or not to fill in the matrix by row or by column (the default). dimnames is an optional attribute. We’ll look at that shortly.\nA basic call to matrix() might look like this:\n\nmy_mat &lt;- matrix(c(1, 3, 4, -1, 5, 6), \n                 nrow = 3, \n                 ncol = 2)\nmy_mat\n\n     [,1] [,2]\n[1,]    1   -1\n[2,]    3    5\n[3,]    4    6\n\n\nNotice how the values fill in the first column then the second column. This is the default behavior but can be changed via the byrow argument.\n\nmy_mat &lt;- matrix(c(1, 3, 4, -1, 5, 6), \n                 nrow = 3, \n                 ncol = 2,\n                 byrow = TRUE)\nmy_mat\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    4   -1\n[3,]    5    6\n\n\nLet’s think of a matrix as a collection of vectors of the same type and length (how you might think of a dataset). We could construct the matrix by giving appropriate vectors of the same type and length.\n\n#populate vectors\nx &lt;- rep(0.2, times = 6)\ny &lt;- c(1, 3, 4, -1, 5, 6)\n\n\nCheck they are the same type of element. In this case, as long as they are both numeric, we can put them together and not lose any precision (integers are coerced to doubles if needed).\n\n\nstr(x)\n\n num [1:6] 0.2 0.2 0.2 0.2 0.2 0.2\n\nis.numeric(x)\n\n[1] TRUE\n\n\n\nstr(y)\n\n num [1:6] 1 3 4 -1 5 6\n\nis.numeric(y)\n\n[1] TRUE\n\n\n\nA length() function exists. We can use that to see the vectors have the same number of elements.\n\n\n#check 'length'\nlength(x)\n\n[1] 6\n\nlength(y)\n\n[1] 6\n\n\n\nNow construct the matrix by combining the vectors together with c().\n\n\nmy_mat2 &lt;- matrix(c(x, y), ncol = 2)\nmy_mat2\n\n     [,1] [,2]\n[1,]  0.2    1\n[2,]  0.2    3\n[3,]  0.2    4\n[4,]  0.2   -1\n[5,]  0.2    5\n[6,]  0.2    6\n\n\nAs byrow = FALSE is the default, the vectors become the columns!\nThat being said, the way to really think about the matrix is as a long vector we are giving dimensions to. In fact, if we coerce our matrix to a vector explicitly (via the as.vector() function), we see the original data vector we passed in.\n\nas.vector(my_mat2)\n\n [1]  0.2  0.2  0.2  0.2  0.2  0.2  1.0  3.0  4.0 -1.0  5.0  6.0\n\n\n\nMatrices don’t have to be made up of numeric data. As long as the elements are all the same type, we are good to go!\n\n\nx &lt;- c(\"Hi\", \"There\", \"!\")\ny &lt;- c(\"a\", \"b\", \"c\")\nz &lt;- c(\"One\", \"Two\", \"Three\")\nmatrix(c(x, y, z), nrow = 3)\n\n     [,1]    [,2] [,3]   \n[1,] \"Hi\"    \"a\"  \"One\"  \n[2,] \"There\" \"b\"  \"Two\"  \n[3,] \"!\"     \"c\"  \"Three\"\n\n\n\nNotice that we don’t need to specify the number of columns. Generally, we need to only specify the nrow or ncol argument and R figures the rest out!\nWe do need to be careful about how R recycles things:\n\n\nmatrix(c(x, y, z), ncol = 2)\n\nWarning in matrix(c(x, y, z), ncol = 2): data length [9] is not a sub-multiple\nor multiple of the number of rows [5]\n\n\n     [,1]    [,2]   \n[1,] \"Hi\"    \"c\"    \n[2,] \"There\" \"One\"  \n[3,] \"!\"     \"Two\"  \n[4,] \"a\"     \"Three\"\n[5,] \"b\"     \"Hi\"   \n\n\n\nWe were 1 element short of being able to fill a matrix with 2 columns and 5 rows (5 chosen to make sure all the data was included). R recycles the first element we passed to fill in the last value. This is a common thing that R does. It can be useful for shorthanding things but otherwise is just something we need to be aware of as a common error to be fixed!\nLet’s shorthand create a matrix of all 0’s.\n\n\nmatrix(0, nrow = 2, ncol = 2)\n\n     [,1] [,2]\n[1,]    0    0\n[2,]    0    0\n\n\n\n\n\nSimilar to a vector, a matrix can have attributes (this is true of any R object!). The common attributes associated with a matrix are the dim or dimensions and the dimnames or dimension names.\n\nmy_iris &lt;- as.matrix(iris[, 1:4])\nhead(my_iris)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]          5.1         3.5          1.4         0.2\n[2,]          4.9         3.0          1.4         0.2\n[3,]          4.7         3.2          1.3         0.2\n[4,]          4.6         3.1          1.5         0.2\n[5,]          5.0         3.6          1.4         0.2\n[6,]          5.4         3.9          1.7         0.4\n\n\n\nstr(my_iris)\n\n num [1:150, 1:4] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n\n\n\nattributes(my_iris)\n\n$dim\n[1] 150   4\n\n$dimnames\n$dimnames[[1]]\nNULL\n\n$dimnames[[2]]\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n\n\nWe can access these attributes through functions designed for them:\n\ndim(my_iris)\n\n[1] 150   4\n\n\n\ndimnames(my_iris)\n\n[[1]]\nNULL\n\n[[2]]\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n\n\nHere we can name the rows and the columns of a matrix by assigning a list() to the dimnames attribute. (Lists are covered shortly.)\n\ndimnames(my_iris) &lt;- list(\n  1:150, #first list element is a vector for the row names\n  c(\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\") #second element is a vector for column names\n)\nhead(my_iris)\n\n  Sepal Length Sepal Width Petal Length Petal Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\n\nWe can assign these when we create a matrix!\n\nmy_mat3 &lt;- matrix(c(runif(10), \n                    rnorm(10),\n                    rgamma(10, shape = 1, scale = 1)),\n                  ncol = 3,\n                  dimnames = list(1:10, c(\"Uniform\", \"Normal\", \"Gamma\")))\nmy_mat3\n\n      Uniform      Normal     Gamma\n1  0.17917969  1.20420196 1.8343261\n2  0.90436881 -0.76908382 0.3196138\n3  0.28268897 -0.77996655 0.8879761\n4  0.85723002  1.89686632 1.5989007\n5  0.92261973 -0.07752369 0.7245798\n6  0.61851780  0.18301513 1.9846702\n7  0.07360534 -1.10904253 0.9916780\n8  0.54449506 -0.67551690 0.5018890\n9  0.91895819 -0.27763180 0.4849135\n10 0.39869803  1.68097820 0.4110543\n\n\n\n\n\nWe saw that [] allowed us to access the elements of a vector. We’ll use the same syntax to get at elements of a matrix. However, we now have two dimensions! That means we need to specify which row elements and which column elements. This is done by using square brackets with a comma in between our indices.\n\nNotice the default row names and column names! This gives us hints on this syntax!\n\n\nmat &lt;- matrix(c(1:4, 20:17), ncol = 2)\nmat\n\n     [,1] [,2]\n[1,]    1   20\n[2,]    2   19\n[3,]    3   18\n[4,]    4   17\n\n\n\nmat[2, 2]\n\n[1] 19\n\n\n\nIf we leave an index blank, we get the entirety of that index back.\n\n\nmat[ , 1]\n\n[1] 1 2 3 4\n\n\n\nmat[2, ]\n\n[1]  2 19\n\n\n\nmat[2:4, 1]\n\n[1] 2 3 4\n\n\n\nmat[c(2, 4), ]\n\n     [,1] [,2]\n[1,]    2   19\n[2,]    4   17\n\n\nNotice that R simplifies the result where possible. That is, returns an atomic vector if you have only 1 dimension and a matrix if two.\nAlso, if you only give a single value in the [] then R uses the count of the value in the matrix (essentially treating the matrix elements as a long vector). These counts go down columns first.\n\nIf you do have dimnames associated, then you can access elements with those.\n\n\nmat &lt;- matrix(c(1:4, 20:17), ncol = 2,\n          dimnames = list(NULL,\n                c(\"First\", \"Second\"))\n        )\nmat\n\n     First Second\n[1,]     1     20\n[2,]     2     19\n[3,]     3     18\n[4,]     4     17\n\n\n\nmat[, \"First\"]\n\n[1] 1 2 3 4",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Matrices"
    ]
  },
  {
    "objectID": "Notes/08-Base_R_Data_Structures_Matrices.html#arrays",
    "href": "Notes/08-Base_R_Data_Structures_Matrices.html#arrays",
    "title": "Base R Data Structures: Matrices",
    "section": "",
    "text": "Arrays are the n-dimensional extension of matrices. Like matrices, they must have all elements of the same type.\n\nmy_array &lt;- array(1:24, dim = c(4, 2, 3))\nmy_array\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n, , 2\n\n     [,1] [,2]\n[1,]    9   13\n[2,]   10   14\n[3,]   11   15\n[4,]   12   16\n\n, , 3\n\n     [,1] [,2]\n[1,]   17   21\n[2,]   18   22\n[3,]   19   23\n[4,]   20   24\n\n\nAccessing elements is similar to vectors and matrices!\n\nmy_array[1, 1, 1]\n\n[1] 1\n\n\n\nmy_array[4, 2, 1]\n\n[1] 8\n\n\nArrays are often used with images in deep learning.\n\n\n\nA 2D object where all elements are of the same type\nAccess elements via [ , ]\nNot ideal for data since all elements must be the same type (see data frames!)\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Matrices"
    ]
  },
  {
    "objectID": "Notes/10-Base_R_Data_Structures_Lists.html",
    "href": "Notes/10-Base_R_Data_Structures_Lists.html",
    "title": "Base R Data Structures: Lists",
    "section": "",
    "text": "A data scientist needs to deal with data! We need to have a firm foundation in the ways that we can store our data in R. This section goes through the most commonly used ‘built-in’ R objects that we’ll use.\n\nThere are five major data structures used in R\n\nAtomic Vector (1d)\nMatrix (2d)\nArray (nd)\nData Frame (2d)\nList (1d)\n\n\n\n\n\n\n\n\n\n\nDimension\nHomogeneous (elements all the same)\nHeterogeneous (elements may differ)\n\n\n\n\n1d\nAtomic Vector\nList\n\n\n2d\nMatrix\nData Frame\n\n\nnd\nArray\n\n\n\n\n\n\n\nA vector that can have differing elements! (still 1D)\n\n\n\n\n\n\n\n\n\n\n\nAn ordered set of objects (ordering starts at 1)\nUseful for more complex types of data\n\n\n\n\nCreate with list()\nThe help gives:\n\nlist(...)\nwhere ... is\n\nobjects, possibly named.\n\n\nWe can essentially take any objects and store them as elements of our list!\n\n\nmy_df &lt;- data.frame(number = 1:5, letter = c(\"a\", \"b\", \"c\", \"d\", \"e\"))\nmy_list &lt;- list(my_df, rnorm(4), c(\"!\", \"?\"))\nmy_list\n\n[[1]]\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n[[2]]\n[1]  0.9883239 -1.4783524  0.7888078 -1.7032734\n\n[[3]]\n[1] \"!\" \"?\"\n\n\n\nSimilar to creating a data frame, we can add names to the list elements upon creation\n\n\nmy_list &lt;- list(my_data_frame = my_df, normVals = rnorm(4), punctuation = c(\"!\", \"?\"))\nmy_list\n\n$my_data_frame\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n$normVals\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n$punctuation\n[1] \"!\" \"?\"\n\n\n\n\n\nThe most common attribute for a list is similar to a data frame, the names.\n\nstr(my_list)\n\nList of 3\n $ my_data_frame:'data.frame':  5 obs. of  2 variables:\n  ..$ number: int [1:5] 1 2 3 4 5\n  ..$ letter: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n $ normVals     : num [1:4] -1.156 0.96 -0.979 1.206\n $ punctuation  : chr [1:2] \"!\" \"?\"\n\n\n\nattributes(my_list)\n\n$names\n[1] \"my_data_frame\" \"normVals\"      \"punctuation\"  \n\n\n\nThe names function gives us quick access to the names.\n\n\nnames(my_list)\n\n[1] \"my_data_frame\" \"normVals\"      \"punctuation\"  \n\n\n\n\n\nThere are many ways to access list elements!\n\nUse single square brackets [ ] for multiple list elements to be returned\n\n\nmy_list\n\n$my_data_frame\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n$normVals\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n$punctuation\n[1] \"!\" \"?\"\n\n\n\nmy_list[2:3]\n\n$normVals\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n$punctuation\n[1] \"!\" \"?\"\n\n\n\nUse double square brackets [[ ]] (or [ ]) for a single list element\n\n\nmy_list[1]\n\n$my_data_frame\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n\n\nmy_list[[1]]\n\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n\n\nNotice the difference in how these are returned!\n\n[] returns a list with a named element (my_data_frame)\n[[]] returns just the element itself (the data frame)\n\n\n\nstr(my_list[1])\n\nList of 1\n $ my_data_frame:'data.frame':  5 obs. of  2 variables:\n  ..$ number: int [1:5] 1 2 3 4 5\n  ..$ letter: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n\n\n\nstr(my_list[[1]])\n\n'data.frame':   5 obs. of  2 variables:\n $ number: int  1 2 3 4 5\n $ letter: chr  \"a\" \"b\" \"c\" \"d\" ...\n\n\n\nWe can do multiple subsets on a single line!\n\n\nmy_list[[2]]\n\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n\n\nmy_list[[2]][3:4]\n\n[1] -0.9794277  1.2063547\n\n\n\nIf we have named list elements, we can use $ just like with data frames!\n\n\nstr(my_list)\n\nList of 3\n $ my_data_frame:'data.frame':  5 obs. of  2 variables:\n  ..$ number: int [1:5] 1 2 3 4 5\n  ..$ letter: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n $ normVals     : num [1:4] -1.156 0.96 -0.979 1.206\n $ punctuation  : chr [1:2] \"!\" \"?\"\n\n\n\nmy_list$normVals\n\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n\n\nNote that the attributes() function actually returns a list!\n\n\nattributes(my_list)\n\n$names\n[1] \"my_data_frame\" \"normVals\"      \"punctuation\"  \n\n\n\nstr(attributes(my_list))\n\nList of 1\n $ names: chr [1:3] \"my_data_frame\" \"normVals\" \"punctuation\"\n\n\n\nThat means we can access the named list element names via the $ operator.\n\n\nattributes(my_list)$names\n\n[1] \"my_data_frame\" \"normVals\"      \"punctuation\"  \n\n\n\n\n\n\n\nBig Connection: A Data Frame is a list of equal length vectors!\nThis can be seen in the similar nature of the structure of these two objects.\n\n\nstr(my_list)\n\nList of 3\n $ my_data_frame:'data.frame':  5 obs. of  2 variables:\n  ..$ number: int [1:5] 1 2 3 4 5\n  ..$ letter: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n $ normVals     : num [1:4] -1.156 0.96 -0.979 1.206\n $ punctuation  : chr [1:2] \"!\" \"?\"\n\nis.list(my_list)\n\n[1] TRUE\n\n\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nis.list(iris)\n\n[1] TRUE\n\n\n\nThat means we can access parts of a data frame in the same way we did with a list. To get the 2nd column (list element) of iris we can do:\n\n\nhead(iris[2])\n\n  Sepal.Width\n1         3.5\n2         3.0\n3         3.2\n4         3.1\n5         3.6\n6         3.9\n\n\n\nhead(iris[[2]])\n\n[1] 3.5 3.0 3.2 3.1 3.6 3.9\n\n\n\nNotice again the change in simplification between the two methods for accessing list elements. Think of [] as preserving and [[]] as simplifying!\nWe can also look at the typeof() each of these objects\n\n\ntypeof(my_list)\n\n[1] \"list\"\n\n\n\ntypeof(iris)\n\n[1] \"list\"\n\n\n\n\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\n\nList (1D group of objects with ordering)\n\nA vector that can have differing elements\nCreate with list()\nMore flexible than a Data Frame!\nUseful for more complex types of data\nAccess with [ ], [[ ]], or $\n\n\n\n\n\nWe now know how we’ll handle data using R. We will end up using vectors, lists, and data frames a lot (although we’ll use a special form of a data frame called a tibble).\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1d\nAtomic Vector\nList\n\n\n2d\nMatrix\nData Frame\n\n\nnd\nArray\n\n\n\n\nCommon Attributes exist\n\ndimnames for matrices\nnames for vectors, data frames, and lists\nNote: colnames() is a function that generically tries to get at the names, whether you have a matrix or data frame (rownames() exists as well!)\n\nBasic access via\n\nAtomic vectors - x[ ]\nMatrices - x[ , ]\nData Frames - x[ , ] or x$name\nLists - x[ ], x[[ ]], or x$name\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Lists"
    ]
  },
  {
    "objectID": "Notes/10-Base_R_Data_Structures_Lists.html#list",
    "href": "Notes/10-Base_R_Data_Structures_Lists.html#list",
    "title": "Base R Data Structures: Lists",
    "section": "",
    "text": "A vector that can have differing elements! (still 1D)\n\n\n\n\n\n\n\n\n\n\n\nAn ordered set of objects (ordering starts at 1)\nUseful for more complex types of data\n\n\n\n\nCreate with list()\nThe help gives:\n\nlist(...)\nwhere ... is\n\nobjects, possibly named.\n\n\nWe can essentially take any objects and store them as elements of our list!\n\n\nmy_df &lt;- data.frame(number = 1:5, letter = c(\"a\", \"b\", \"c\", \"d\", \"e\"))\nmy_list &lt;- list(my_df, rnorm(4), c(\"!\", \"?\"))\nmy_list\n\n[[1]]\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n[[2]]\n[1]  0.9883239 -1.4783524  0.7888078 -1.7032734\n\n[[3]]\n[1] \"!\" \"?\"\n\n\n\nSimilar to creating a data frame, we can add names to the list elements upon creation\n\n\nmy_list &lt;- list(my_data_frame = my_df, normVals = rnorm(4), punctuation = c(\"!\", \"?\"))\nmy_list\n\n$my_data_frame\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n$normVals\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n$punctuation\n[1] \"!\" \"?\"\n\n\n\n\n\nThe most common attribute for a list is similar to a data frame, the names.\n\nstr(my_list)\n\nList of 3\n $ my_data_frame:'data.frame':  5 obs. of  2 variables:\n  ..$ number: int [1:5] 1 2 3 4 5\n  ..$ letter: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n $ normVals     : num [1:4] -1.156 0.96 -0.979 1.206\n $ punctuation  : chr [1:2] \"!\" \"?\"\n\n\n\nattributes(my_list)\n\n$names\n[1] \"my_data_frame\" \"normVals\"      \"punctuation\"  \n\n\n\nThe names function gives us quick access to the names.\n\n\nnames(my_list)\n\n[1] \"my_data_frame\" \"normVals\"      \"punctuation\"  \n\n\n\n\n\nThere are many ways to access list elements!\n\nUse single square brackets [ ] for multiple list elements to be returned\n\n\nmy_list\n\n$my_data_frame\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n$normVals\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n$punctuation\n[1] \"!\" \"?\"\n\n\n\nmy_list[2:3]\n\n$normVals\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n$punctuation\n[1] \"!\" \"?\"\n\n\n\nUse double square brackets [[ ]] (or [ ]) for a single list element\n\n\nmy_list[1]\n\n$my_data_frame\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n\n\nmy_list[[1]]\n\n  number letter\n1      1      a\n2      2      b\n3      3      c\n4      4      d\n5      5      e\n\n\n\nNotice the difference in how these are returned!\n\n[] returns a list with a named element (my_data_frame)\n[[]] returns just the element itself (the data frame)\n\n\n\nstr(my_list[1])\n\nList of 1\n $ my_data_frame:'data.frame':  5 obs. of  2 variables:\n  ..$ number: int [1:5] 1 2 3 4 5\n  ..$ letter: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n\n\n\nstr(my_list[[1]])\n\n'data.frame':   5 obs. of  2 variables:\n $ number: int  1 2 3 4 5\n $ letter: chr  \"a\" \"b\" \"c\" \"d\" ...\n\n\n\nWe can do multiple subsets on a single line!\n\n\nmy_list[[2]]\n\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n\n\nmy_list[[2]][3:4]\n\n[1] -0.9794277  1.2063547\n\n\n\nIf we have named list elements, we can use $ just like with data frames!\n\n\nstr(my_list)\n\nList of 3\n $ my_data_frame:'data.frame':  5 obs. of  2 variables:\n  ..$ number: int [1:5] 1 2 3 4 5\n  ..$ letter: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n $ normVals     : num [1:4] -1.156 0.96 -0.979 1.206\n $ punctuation  : chr [1:2] \"!\" \"?\"\n\n\n\nmy_list$normVals\n\n[1] -1.1563882  0.9597658 -0.9794277  1.2063547\n\n\n\nNote that the attributes() function actually returns a list!\n\n\nattributes(my_list)\n\n$names\n[1] \"my_data_frame\" \"normVals\"      \"punctuation\"  \n\n\n\nstr(attributes(my_list))\n\nList of 1\n $ names: chr [1:3] \"my_data_frame\" \"normVals\" \"punctuation\"\n\n\n\nThat means we can access the named list element names via the $ operator.\n\n\nattributes(my_list)$names\n\n[1] \"my_data_frame\" \"normVals\"      \"punctuation\"",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Lists"
    ]
  },
  {
    "objectID": "Notes/10-Base_R_Data_Structures_Lists.html#lists-data-frames",
    "href": "Notes/10-Base_R_Data_Structures_Lists.html#lists-data-frames",
    "title": "Base R Data Structures: Lists",
    "section": "",
    "text": "Big Connection: A Data Frame is a list of equal length vectors!\nThis can be seen in the similar nature of the structure of these two objects.\n\n\nstr(my_list)\n\nList of 3\n $ my_data_frame:'data.frame':  5 obs. of  2 variables:\n  ..$ number: int [1:5] 1 2 3 4 5\n  ..$ letter: chr [1:5] \"a\" \"b\" \"c\" \"d\" ...\n $ normVals     : num [1:4] -1.156 0.96 -0.979 1.206\n $ punctuation  : chr [1:2] \"!\" \"?\"\n\nis.list(my_list)\n\n[1] TRUE\n\n\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nis.list(iris)\n\n[1] TRUE\n\n\n\nThat means we can access parts of a data frame in the same way we did with a list. To get the 2nd column (list element) of iris we can do:\n\n\nhead(iris[2])\n\n  Sepal.Width\n1         3.5\n2         3.0\n3         3.2\n4         3.1\n5         3.6\n6         3.9\n\n\n\nhead(iris[[2]])\n\n[1] 3.5 3.0 3.2 3.1 3.6 3.9\n\n\n\nNotice again the change in simplification between the two methods for accessing list elements. Think of [] as preserving and [[]] as simplifying!\nWe can also look at the typeof() each of these objects\n\n\ntypeof(my_list)\n\n[1] \"list\"\n\n\n\ntypeof(iris)\n\n[1] \"list\"\n\n\n\n\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\n\nList (1D group of objects with ordering)\n\nA vector that can have differing elements\nCreate with list()\nMore flexible than a Data Frame!\nUseful for more complex types of data\nAccess with [ ], [[ ]], or $",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Lists"
    ]
  },
  {
    "objectID": "Notes/10-Base_R_Data_Structures_Lists.html#big-recap",
    "href": "Notes/10-Base_R_Data_Structures_Lists.html#big-recap",
    "title": "Base R Data Structures: Lists",
    "section": "",
    "text": "We now know how we’ll handle data using R. We will end up using vectors, lists, and data frames a lot (although we’ll use a special form of a data frame called a tibble).\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1d\nAtomic Vector\nList\n\n\n2d\nMatrix\nData Frame\n\n\nnd\nArray\n\n\n\n\nCommon Attributes exist\n\ndimnames for matrices\nnames for vectors, data frames, and lists\nNote: colnames() is a function that generically tries to get at the names, whether you have a matrix or data frame (rownames() exists as well!)\n\nBasic access via\n\nAtomic vectors - x[ ]\nMatrices - x[ , ]\nData Frames - x[ , ] or x$name\nLists - x[ ], x[[ ]], or x$name\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Base R Data Structures: Lists"
    ]
  },
  {
    "objectID": "Notes/12-Control_Flow_Loops.html",
    "href": "Notes/12-Control_Flow_Loops.html",
    "title": "Control Flow: Loops",
    "section": "",
    "text": "We want to look at how to control the execution of our code. The three main things we are looking at here are\nThis section looks at how to do loops (repeated execution of code) in R.",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Control Flow: Loops"
    ]
  },
  {
    "objectID": "Notes/12-Control_Flow_Loops.html#looping-in-r",
    "href": "Notes/12-Control_Flow_Loops.html#looping-in-r",
    "title": "Control Flow: Loops",
    "section": "Looping in R",
    "text": "Looping in R\nThere are a number of ways to do looping in R\n\nfor()\nwhile()\nrepeat()\n\nThe idea of each is to run some code repeatedly; often changing something with each execution of the code.\n\nFor Loops\nThe syntax for a for loop (most commonly used loop in R) is\n\nfor(index in values){\n  code to be run\n}\n\nwhere\n\nindex defines a ‘counter’ or variable that varies\n‘values’ define which values the index takes on\n\nFor example, our index below is i and the values it can take on are the integers from 1 to 10 (1:10)\n\nfor (i in 1:10){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nThe values don’t need to take on numbers and the object you use for the index can be changed:\n\nfor (index in c(\"cat\",\"hat\",\"worm\")){\n  print(index)\n}\n\n[1] \"cat\"\n[1] \"hat\"\n[1] \"worm\"\n\n\nOf course, the idea is to use the changing values in some meaningful way. Here is a quick example of printing out a particular string based on inputs.\nCreate two vectors of length 5.\n\nwords&lt;-c(\"first\", \"second\", \"third\", \"fourth\", \"fifth\")\ndata &lt;- runif(5)\n\n\nLoop through the elements of these and print out the phrase\n\n“The (#ed) data point is (# from data vector).”\n\nTo put character strings together with other R objects (which will be coerced to strings) we can use the paste() function. Checking the help we see:\n\npaste (..., sep = \" \", collapse = NULL, recycle0 = FALSE)\nwhere ... ‘is one or more R objects, to be converted to character vectors.’ and the sep = argument determines the value by which to separate these objects.\n\npaste(\"The \", words[2], \" data point is \", data[2], \".\", sep = \"&\")\n\n[1] \"The &second& data point is &0.99061964568682&.\"\n\n\n\npaste(\"The \", words[1], \" data point is \", data[1], \".\", sep = \"\")\n\n[1] \"The first data point is 0.956676640780643.\"\n\n\nNote: sep = \"\" is equivalent to using the paste0() function.\nOk, let’s put this into a loop!\n\nfor (i in 1:5){\n  print(paste0(\"The \", words[i], \" data point is \", data[i], \".\"))\n}\n\n[1] \"The first data point is 0.956676640780643.\"\n[1] \"The second data point is 0.99061964568682.\"\n[1] \"The third data point is 0.413699142867699.\"\n[1] \"The fourth data point is 0.571669399505481.\"\n[1] \"The fifth data point is 0.662238640943542.\"\n\n\n\nAs i iterates from 1 to 5, we pull out the corresponding elements of words and data to make our sentence!\n\nA more useful example would be finding summary statistics about different numeric columns of a data frame (recall this is a 2D structure we often use to store datasets).\n\nConsider a dataset on batting of Major League Baseball (MLB) players.\n\nYou may need to run install.packages(\"Lahman\") once on your machine before you can run this code\n\n\n\nlibrary(Lahman)\nmy_batting &lt;- Batting[, c(\"playerID\", \"teamID\", \"G\", \"AB\", \"R\", \"H\", \"X2B\", \"X3B\", \"HR\")]\nhead(my_batting)\n\n   playerID teamID  G AB R H X2B X3B HR\n1 aardsda01    SFN 11  0 0 0   0   0  0\n2 aardsda01    CHN 45  2 0 0   0   0  0\n3 aardsda01    CHA 25  0 0 0   0   0  0\n4 aardsda01    BOS 47  1 0 0   0   0  0\n5 aardsda01    SEA 73  0 0 0   0   0  0\n6 aardsda01    SEA 53  0 0 0   0   0  0\n\n\n\nLet’s say we want to find the summary() for each numeric column of this data set.\n\n\nsummary(my_batting[ , \"G\"])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   12.00   34.00   50.38   78.00  165.00 \n\n\n\nsummary(my_batting[ , \"AB\"])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0     3.0    44.0   137.4   220.0   716.0 \n\n\nThat’s fine but we want to do it for all the numeric columns. Let’s use a for loop!\n\ndim(my_batting)\n\n[1] 113799      9\n\n\nWe could do a loop that takes on values of 3:9 (or programmatically 3:dim(my_batting)[2]).\n\nfor (i in 3:dim(my_batting)[2]){\n  print(summary(my_batting[ , i]))\n}\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   12.00   34.00   50.38   78.00  165.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0     3.0    44.0   137.4   220.0   716.0 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00   18.24   26.00  198.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    8.00   35.84   54.00  262.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    1.00    6.14    9.00   67.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.00    1.21    1.00   36.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   2.874   2.000  73.000 \n\n\nAlternatively, the seq_along() function can be useful. This looks at the length of the object and creates a sequence from 1 to that length. Remember that a data frame is truly a list of equal length vectors (usually). The length of a list is number of elements. Here that is the number of columns!\n\nlength(my_batting)\n\n[1] 9\n\nseq_along(my_batting)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nNow we can just remove the 1st and 2nd entries of that vector (as they are not numeric columns) and use that as our values to iterate across.\n\nfor (i in seq_along(my_batting)[-1:-2]){\n  print(summary(my_batting[ , i]))\n}\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   12.00   34.00   50.38   78.00  165.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0     3.0    44.0   137.4   220.0   716.0 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00   18.24   26.00  198.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    8.00   35.84   54.00  262.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    1.00    6.14    9.00   67.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.00    1.21    1.00   36.00 \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   2.874   2.000  73.000 \n\n\nWe likely don’t enjoy this format. Although we’ll see much easier ways to deal with this, let’s initialize a data frame to store our results in. We can initialize the type of data to store in a particular column using character(), numeric(), logical(), etc.\n\nsummary_df &lt;- data.frame(stat = character(), \n                         min = numeric(),\n                         Q1 = numeric(),\n                         Median = numeric(),\n                         Mean = numeric(),\n                         Q3 = numeric(),\n                         Max  = numeric())\nsummary_df\n\n[1] stat   min    Q1     Median Mean   Q3     Max   \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nOk, now let’s fill this in as we loop (note we use i-2 to start filling in at row 1 and we grab the statistic we are summarizing from the colnames of the my_batting data frame).\n\nfor (i in seq_along(my_batting)[-1:-2]){\n  summary_df[i-2, ] &lt;- c(colnames(my_batting[i]),\n                         summary(my_batting[ , i]))\n}\nsummary_df\n\n  stat min Q1 Median             Mean  Q3 Max\n1    G   1 12     34 50.3842125150485  78 165\n2   AB   0  3     44  137.41551331734 220 716\n3    R   0  0      4 18.2432183059605  26 198\n4    H   0  0      8 35.8410706596719  54 262\n5  X2B   0  0      1 6.14015061643775   9  67\n6  X3B   0  0      0  1.2099754830886   1  36\n7   HR   0  0      0  2.8742959076969   2  73\n\n\n\n\nWhile Loops\n\nThese provide an alternative way to loop when we don’t necessarily know how many iterations to do before we start.\n\n\nwhile(cond) {\n    expr\n}\n\n\nIf cond is FALSE then the loop never executes.\nWe won’t use these much.\n\n\n\nOther Loop Things\n\nSometimes we need to jump out of a loop. break kicks you out of the loop.\n\n\nfor (i in 1:5){\n  if (i == 3) break #can put code to execute on the same line\n  print(paste0(\"The \", words[i], \" data point is \", data[i], \".\"))\n}\n\n[1] \"The first data point is 0.956676640780643.\"\n[1] \"The second data point is 0.99061964568682.\"\n\n\n\nSometimes we need to skip an iteration. next jumps to the next iteration of the loop.\n\n\nfor (i in 1:5){\n    if (i == 3) next\n  print(paste0(\"The \", words[i], \" data point is \", data[i], \".\"))\n}\n\n[1] \"The first data point is 0.956676640780643.\"\n[1] \"The second data point is 0.99061964568682.\"\n[1] \"The fourth data point is 0.571669399505481.\"\n[1] \"The fifth data point is 0.662238640943542.\"\n\n\n\n\nQuick R Video\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\nRecap!\n\nLoops provide a mechanism to run the same code repeatedly\n\n\nfor(index in values){\n  #code to evaluate\n}\n\n\nindex is the variable that changes during each iteration\nvalues are the values the index takes on\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Control Flow: Loops"
    ]
  },
  {
    "objectID": "Notes/14-Writing_Functions.html",
    "href": "Notes/14-Writing_Functions.html",
    "title": "Writing Functions",
    "section": "",
    "text": "Next up we take on writing our own functions (we’ll revisit this later on to go deeper). Knowing how to write functions vital to custom analyses!\n\nFunction writing syntax\n\n\nnameOfFunction &lt;- function(input1, input2, ...) {\n  #code\n  #return something with return()\n  #or the function returns last thing done\n}\n\nOne nice thing is that you can generally look at the code for the functions you use by typing the function without () into the console.\n\nNotice the arguments in the function definition\nNotice that each of these functions return whatever is the last code run (no use of return())\n\n\nvar\n\nfunction (x, y = NULL, na.rm = FALSE, use) \n{\n    if (missing(use)) \n        use &lt;- if (na.rm) \n            \"na.or.complete\"\n        else \"everything\"\n    na.method &lt;- pmatch(use, c(\"all.obs\", \"complete.obs\", \"pairwise.complete.obs\", \n        \"everything\", \"na.or.complete\"))\n    if (is.na(na.method)) \n        stop(\"invalid 'use' argument\")\n    if (is.data.frame(x)) \n        x &lt;- as.matrix(x)\n    else if (!is.null(x)) \n        stopifnot(is.atomic(x))\n    if (is.data.frame(y)) \n        y &lt;- as.matrix(y)\n    else if (!is.null(y)) \n        stopifnot(is.atomic(y))\n    .Call(C_cov, x, y, na.method, FALSE)\n}\n&lt;bytecode: 0x000001ff7161f1e8&gt;\n&lt;environment: namespace:stats&gt;\n\n\n\nUnless the if statements cause the function to stop, the result of .Call(C_cov, x, y, na.method, FALSE) is returned.\n\n\ncolMeans\n\nfunction (x, na.rm = FALSE, dims = 1L) \n{\n    if (is.data.frame(x)) \n        x &lt;- as.matrix(x)\n    if (!is.array(x) || length(dn &lt;- dim(x)) &lt; 2L) \n        stop(\"'x' must be an array of at least two dimensions\")\n    if (dims &lt; 1L || dims &gt; length(dn) - 1L) \n        stop(\"invalid 'dims'\")\n    n &lt;- prod(dn[id &lt;- seq_len(dims)])\n    dn &lt;- dn[-id]\n    z &lt;- if (is.complex(x)) \n        .Internal(colMeans(Re(x), n, prod(dn), na.rm)) + (0+1i) * \n            .Internal(colMeans(Im(x), n, prod(dn), na.rm))\n    else .Internal(colMeans(x, n, prod(dn), na.rm))\n    if (length(dn) &gt; 1L) {\n        dim(z) &lt;- dn\n        dimnames(z) &lt;- dimnames(x)[-id]\n    }\n    else names(z) &lt;- dimnames(x)[[dims + 1L]]\n    z\n}\n&lt;bytecode: 0x000001ff799a7b88&gt;\n&lt;environment: namespace:base&gt;\n\n\n\nUnless the if statements cause the function to stop, z is the last code run and is what gets returned.\n\nFor some functions, they are generic and they won’t show anything useful.\n\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x000001ff75345810&gt;\n&lt;environment: namespace:base&gt;\n\n\nFor those, you can pick a particular version of the function:\n\nmean.default\n\nfunction (x, trim = 0, na.rm = FALSE, ...) \n{\n    if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {\n        warning(\"argument is not numeric or logical: returning NA\")\n        return(NA_real_)\n    }\n    if (isTRUE(na.rm)) \n        x &lt;- x[!is.na(x)]\n    if (!is.numeric(trim) || length(trim) != 1L) \n        stop(\"'trim' must be numeric of length one\")\n    n &lt;- length(x)\n    if (trim &gt; 0 && n) {\n        if (is.complex(x)) \n            stop(\"trimmed means are not defined for complex data\")\n        if (anyNA(x)) \n            return(NA_real_)\n        if (trim &gt;= 0.5) \n            return(stats::median(x, na.rm = FALSE))\n        lo &lt;- floor(n * trim) + 1\n        hi &lt;- n + 1 - lo\n        x &lt;- sort.int(x, partial = unique(c(lo, hi)))[lo:hi]\n    }\n    .Internal(mean(x))\n}\n&lt;bytecode: 0x000001ff79e7e9c8&gt;\n&lt;environment: namespace:base&gt;\n\n\nOk, now you’ve seen some functions. Let’s write our own!\nGoal: Create a standardize() function (creating z-scores for a vector essentially)\n\nTake vector of values\n\nsubtract mean\n\ndivide by standard deviation\n\nFormula: For value i,\n\\[\\frac{(value[i]-mean(value))}{sd(value)}\\]\n\nLet’s take our generic syntax and apply it here.\n\nnameOfFunction &lt;- function(input1, input2, ...) {\n  #code\n  #return something with return()\n  #or returns last value\n}\n\n\nstandardize &lt;- function(vector) {\n  return((vector - mean(vector)) / sd(vector))\n}\n\n\nNote that vector is just the name of the argument! The user could pass something that isn’t a vector and we may be in trouble.\nNow let’s use it! First, create some data:\n\n\nset.seed(10)\ndata &lt;- runif(15)\ndata\n\n [1] 0.50747820 0.30676851 0.42690767 0.69310208 0.08513597 0.22543662\n [7] 0.27453052 0.27230507 0.61582931 0.42967153 0.65165567 0.56773775\n[13] 0.11350898 0.59592531 0.35804998\n\n\n\nApply the function:\n\n\nresult &lt;- standardize(data)\nresult\n\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n\n\nCheck that the result has mean 0 and sd 1\n\n\nmean(result)\n\n[1] 2.312784e-17\n\nsd(result)\n\n[1] 1\n\n\nGoal: Add more inputs\n\nMake centering optional\n\nMake scaling optional\n\n\nstandardize &lt;- function(vector, center, scale) {\n  if (center) {\n    vector &lt;- vector - mean(vector)\n    }\n  if (scale) {\n    vector &lt;- vector / sd(vector)\n    }\n  return(vector)\n  }\n\nHere we’ve added arguments that should implicitly be TRUE or FALSE values (it would be better to give a default value so people using the function would know what is expected).\n\nresult &lt;- standardize(data, center = TRUE, scale = TRUE)\nresult\n\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n\n\nresult &lt;- standardize(data, center = FALSE, scale = TRUE)\nresult\n\n [1] 2.6115093 1.5786467 2.1968891 3.5667395 0.4381141 1.1601086 1.4127484\n [8] 1.4012961 3.1690897 2.2111121 3.3534540 2.9216081 0.5841231 3.0666627\n[15] 1.8425438\n\n\n\nGive center and scale default arguments\n\n\nstandardize &lt;- function(vector, center = TRUE, scale = TRUE) {\n  if (center) {\n    vector &lt;- vector - mean(vector)\n    }\n  if (scale) {\n    vector &lt;- vector / sd(vector)\n    }\n  return(vector)\n  }\n\n\nApply it! The defaults will be used and aren’t necessary if you don’t want to change things.\n\n\nstandardize(data, center = TRUE, scale = TRUE)\n\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\nstandardize(data)\n\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n\nGoal: Also return\n\nmean() of original data\n\nsd() of original data\n\nReturn more than 1 object by returning a list (so we return one object, but a very flexible object that easily contains other objects!)\n\nstandardize &lt;- function(vector, center = TRUE, scale = TRUE) {\n  mean &lt;- mean(vector) #save these so we can return them\n  stdev &lt;- sd(vector)\n  if (center) {\n    vector &lt;- vector - mean\n    }\n  if (scale) {\n    vector &lt;- vector / stdev\n    }\n  return(list(vector, mean, stdev))\n  }\n\n\nApply it!\n\n\nresult &lt;- standardize(data)\nresult  \n\n[[1]]\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n[[2]]\n[1] 0.4082695\n\n[[3]]\n[1] 0.1943237\n\nresult[[2]]\n\n[1] 0.4082695\n\n\n\nWe can fancy up what we return by giving names to the list elements!\n\n\nstandardize &lt;- function(vector, center = TRUE, scale = TRUE) {\n  mean &lt;- mean(vector)\n  stdev &lt;- sd(vector)\n  if (center) {\n    vector &lt;- vector - mean\n    }\n  if (scale) {\n    vector &lt;- vector / stdev\n    }\n  return(list(result = vector, mean = mean, sd = stdev))\n  }\n\n\nApply it!\n\n\nresult &lt;- standardize(data, center = TRUE, scale = TRUE)\nresult  \n\n$result\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n$mean\n[1] 0.4082695\n\n$sd\n[1] 0.1943237\n\nresult$sd\n\n[1] 0.1943237\n\n\n\n\nOften you want to check on inputs to make sure they are of the right form (that’s good practice if you are going to share your code). You can use if() or switch() to do this check.\nHere we’ll write a function to create a summary (mean, median, or trimmed mean).\n\nFirst we check the input to make sure it is a numeric vector.\nThen we use stop() to jump out if that condition isn’t met.\nIf the condition is met, we use switch() an alternative to if/then/else to pick which function to apply.\n\n\nsummarizer &lt;- function(vec, type, trim = 0.05) {\n  if(!is.vector(vec) | !is.numeric(vec)){\n    stop(\"Not a vector or not numeric my friend.\")\n  }\n  switch(type, \n         mean = mean(vec),\n         median = median(vec),\n         trimmed = mean(vec, trim),\n         stop(\"Mistake!\")\n         )\n}\nsummarizer(letters, \"mean\")\n\nError in summarizer(letters, \"mean\"): Not a vector or not numeric my friend.\n\nsummarizer(c(1,1,1,6,10), \"mean\")\n\n[1] 3.8\n\nsummarizer(c(1,1,1,6,10), \"trimmed\", 0.2)\n\n[1] 2.666667\n\nsummarizer(c(1,1,1,6,10), \"means\")\n\nError in summarizer(c(1, 1, 1, 6, 10), \"means\"): Mistake!\n\n\n\n\n\nThat’s the basics of function writing. Let’s talk about a framework to make coherent code. Use of consistent naming schemes is important!\nGenerally, when naming objects they must:\n\nstart with a letter\n\nonly have letters, numbers, _, and .\n\nWhen we write functions and create objects we should try to follow this advice:\n\nFunctions named using verbs\n\nstandardize() or find_mean() or renderDataTable()\n\nData objects named using nouns\n\nmy_df or weather_df\n\n\nNaming things is actually really tough… You should try to follow a common naming scheme:\n\nsnake_case_used\n\ncamelCaseUsed\n\nUpperCamelCase\n\nuse.of.periods (not recommended)\n\nYou’ll also need to name inputs to your functions. Try to stick to these when possible:\n\nx, y, z: vectors\n\nw: a vector of weights\n\ndf: a data frame\ni, j: numeric indices (typically rows and columns)\nn: length, or number of rows\np: number of columns\n\nOtherwise, consider matching names of arguments in existing R functions. For example, use na.rm to determine if missing values should be removed.\nThere are some readings on this available in a previous lesson.\n\n\n\nYou might wonder why sometimes we name our arguments when we call our functions and sometimes we don’t. Generally, we don’t name the first 2-3 arguments but name ones after that. However, that is just convention. In R, you can use positional matching for everything or name each input, or combine the two ideas!\nLet’s look at some examples. Consider the inputs of the cor() function\n\nfunction (x, y = NULL, use = \"everything\", method = c(\"pearson\", \n    \"kendall\", \"spearman\")) \n\n\nApply it to iris data using positional matching (first argument to x second to y):\n\n\ncor(iris$Sepal.Length, iris$Sepal.Width)\n\n[1] -0.1175698\n\n\n\nR will use positional matching for all inputs not explicitly named. Here it applies iris$Sepal.Width to the first input of the function that wasn’t specified, here y.\n\n\ncor(x = iris$Sepal.Length, method = \"spearman\", iris$Sepal.Width)\n\n[1] -0.1667777\n\n\n\nR will also do partial matching but you should avoid this generally.\n\n\ncor(iris$Sepal.Length, iris$Sepal.Width, met = \"spearman\")\n\n\n\n\nLastly, let’s take up the idea of an infix function. An infix function is a function that goes between arguments (as opposed to prefix that goes prior to the arguments - which is what we usually do).\n\nmean(3:5) #prefix\n\n[1] 4\n\n3 + 5 #+ is infix\n\n[1] 8\n\n`+`(3, 5) #used as a prefix function\n\n[1] 8\n\n\nCommon built-in infix functions include:\n\n:: (look directly in a package for a function)\n$ (grab a column)\n^\n*\n/\n+\n-\n&gt;\n&gt;=\n&lt;\n&lt;=\n==\n!=\n& (and)\n| (or)\n&lt;- (storage arrow)\n|&gt; (pipe!)\n\nOthers infix operators use %symbol% syntax:\n\n%*% (matrix multiplication)\n%in% (check if LHS value(s) is(are) in RHS value(s)\n\nWe can call infix functions like prefix functions if we need to using the backtick symbol ` (top left of the keyboard usually)\n\ncars &lt;- as.matrix(cars)\nt(cars) %*% cars\n\n      speed   dist\nspeed 13228  38482\ndist  38482 124903\n\n\n\n`%*%`(t(cars), cars)\n\n      speed   dist\nspeed 13228  38482\ndist  38482 124903\n\n\nYou can also write your own infix function!\n\n`%+%` &lt;- function(a, b) paste0(a, b)\n\"new\" %+% \" string\"\n\n[1] \"new string\"\n\n\nR actually allows you to overwrite + and other operators: just don’t do that… that wouldn’t be good (unless you really want to mess with someone)\nWith infix functions we can use precedence rules to save typing:\n\nx &lt;- y &lt;- 2\n`&lt;-`(x, `&lt;-`(y, 2)) #interpretation of above code!\n\nx &lt;- y = 2# error! &lt;- has higher precedence\n`=`(`&lt;-`(x, y), 2) #interpretation of above code!\n\nx = y &lt;- 2 # this will work!\n`=`(x, `&lt;-`(y, 2)) #interpretation of above code!\n\nThis is one of the major differences between = and &lt;- usage. You can’t do\n\nx = y = 2\n\nbut can do it with the storage arrow.\nThere is a weird difference between how infix functions are evaluated. For user defined infix functions, they evaluate left to right. For built-in ones, they evaluate right to left!\n\nUser defined example:\n\n\n`%-%` &lt;- function(a, b) {\n    paste0(\"(\", a, \" %-% \", b, \")\")\n}\n\"a\" %-% \"b\" %-% \"c\" #user defined infix are evaluated left to right!\n\n[1] \"((a %-% b) %-% c)\"\n\n`%-%`(`%-%`(\"a\", \"b\"), \"c\")  #interpretation of above code!\n\n[1] \"((a %-% b) %-% c)\"\n\n\n\nBuilt-in example:\n\n\nx &lt;- y &lt;- 2\n`&lt;-`(x, `&lt;-`(y, 2)) #interpretation of above code!\n\n\n\n\nThis one deserves its own section! The pipe operator (%&gt;%) was made popular by the tidyverse and the magrittr package. You would need to read in dplyr (part of the tidyverse) or magrittr to have access to the pipe.\nDue to the popularity, R created a Base R pipe (|&gt;). The idea of the pipe is to make code more readable! Essentially, you can read code left to right when using a pipe instead of inside out.\nConsider the code below:\n\nlibrary(dplyr)\narrange(select(filter(as_tibble(Lahman::Batting), teamID == \"PIT\"), playerID, G, X2B), desc(X2B))\n\n# A tibble: 5,110 × 3\n   playerID      G   X2B\n   &lt;chr&gt;     &lt;int&gt; &lt;int&gt;\n 1 wanerpa01   154    62\n 2 sanchfr01   157    53\n 3 wanerpa01   148    53\n 4 wanerpa01   152    50\n 5 comorad01   152    47\n 6 mclouna01   152    46\n 7 parkeda01   158    45\n 8 vanslan01   154    45\n 9 wagneho01   135    45\n10 bayja01     162    44\n# ℹ 5,100 more rows\n\n\n\nForget what the functions do for a minute. To parse this we need to start on the inside.\n\nThe first function is as_tibble(Lahman::Batting)\nThe result of that is then the first argument to filter()\nThe result of this is then the first argument to select()\nThe result of that is then the first argument to arrange()\n\nYikes. Piping makes things way easier to read!\n\n\nLahman::Batting |&gt; #read the pipe as \"then\"\n  as_tibble() |&gt;\n  filter(teamID == \"PIT\") |&gt;\n  select(playerID, G, X2B) |&gt; \n  arrange(desc(X2B)) \n\n# A tibble: 5,110 × 3\n   playerID      G   X2B\n   &lt;chr&gt;     &lt;int&gt; &lt;int&gt;\n 1 wanerpa01   154    62\n 2 sanchfr01   157    53\n 3 wanerpa01   148    53\n 4 wanerpa01   152    50\n 5 comorad01   152    47\n 6 mclouna01   152    46\n 7 parkeda01   158    45\n 8 vanslan01   154    45\n 9 wagneho01   135    45\n10 bayja01     162    44\n# ℹ 5,100 more rows\n\n\n\nThis is easy to parse!\n\nFirst take the Batting dataset and turn it into a tibble (special data frame)\nThen filter it\nThen select from that\nThen arrange that\n\n\nGenerically, |&gt; does the following\n\nx |&gt; f(y) turns into f(x,y)\n\nx |&gt; f(y) |&gt; g(z) turns into g(f(x, y), z)\n\nWe’ll be using this a lot from here on out!\n\n\n\nPlease pop this video out and watch it in the full panopto player!\n\n\n\n\nFunctions allow you to customize your code\n\nCan specify default values and return multiple objects using a named list\nMuch more to know!\n\nUnnamed arguments\nInput matching, environments, and lazy evaluation\nWriting pipeable functions & side-effect functions\nInfix functions\nHelper functions and function writing strategy\n\nNaming conventions and input matching\nstop() and switch()\ninfix functions\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Writing Functions"
    ]
  },
  {
    "objectID": "Notes/14-Writing_Functions.html#writing-functions",
    "href": "Notes/14-Writing_Functions.html#writing-functions",
    "title": "Writing Functions",
    "section": "",
    "text": "Next up we take on writing our own functions (we’ll revisit this later on to go deeper). Knowing how to write functions vital to custom analyses!\n\nFunction writing syntax\n\n\nnameOfFunction &lt;- function(input1, input2, ...) {\n  #code\n  #return something with return()\n  #or the function returns last thing done\n}\n\nOne nice thing is that you can generally look at the code for the functions you use by typing the function without () into the console.\n\nNotice the arguments in the function definition\nNotice that each of these functions return whatever is the last code run (no use of return())\n\n\nvar\n\nfunction (x, y = NULL, na.rm = FALSE, use) \n{\n    if (missing(use)) \n        use &lt;- if (na.rm) \n            \"na.or.complete\"\n        else \"everything\"\n    na.method &lt;- pmatch(use, c(\"all.obs\", \"complete.obs\", \"pairwise.complete.obs\", \n        \"everything\", \"na.or.complete\"))\n    if (is.na(na.method)) \n        stop(\"invalid 'use' argument\")\n    if (is.data.frame(x)) \n        x &lt;- as.matrix(x)\n    else if (!is.null(x)) \n        stopifnot(is.atomic(x))\n    if (is.data.frame(y)) \n        y &lt;- as.matrix(y)\n    else if (!is.null(y)) \n        stopifnot(is.atomic(y))\n    .Call(C_cov, x, y, na.method, FALSE)\n}\n&lt;bytecode: 0x000001ff7161f1e8&gt;\n&lt;environment: namespace:stats&gt;\n\n\n\nUnless the if statements cause the function to stop, the result of .Call(C_cov, x, y, na.method, FALSE) is returned.\n\n\ncolMeans\n\nfunction (x, na.rm = FALSE, dims = 1L) \n{\n    if (is.data.frame(x)) \n        x &lt;- as.matrix(x)\n    if (!is.array(x) || length(dn &lt;- dim(x)) &lt; 2L) \n        stop(\"'x' must be an array of at least two dimensions\")\n    if (dims &lt; 1L || dims &gt; length(dn) - 1L) \n        stop(\"invalid 'dims'\")\n    n &lt;- prod(dn[id &lt;- seq_len(dims)])\n    dn &lt;- dn[-id]\n    z &lt;- if (is.complex(x)) \n        .Internal(colMeans(Re(x), n, prod(dn), na.rm)) + (0+1i) * \n            .Internal(colMeans(Im(x), n, prod(dn), na.rm))\n    else .Internal(colMeans(x, n, prod(dn), na.rm))\n    if (length(dn) &gt; 1L) {\n        dim(z) &lt;- dn\n        dimnames(z) &lt;- dimnames(x)[-id]\n    }\n    else names(z) &lt;- dimnames(x)[[dims + 1L]]\n    z\n}\n&lt;bytecode: 0x000001ff799a7b88&gt;\n&lt;environment: namespace:base&gt;\n\n\n\nUnless the if statements cause the function to stop, z is the last code run and is what gets returned.\n\nFor some functions, they are generic and they won’t show anything useful.\n\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x000001ff75345810&gt;\n&lt;environment: namespace:base&gt;\n\n\nFor those, you can pick a particular version of the function:\n\nmean.default\n\nfunction (x, trim = 0, na.rm = FALSE, ...) \n{\n    if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {\n        warning(\"argument is not numeric or logical: returning NA\")\n        return(NA_real_)\n    }\n    if (isTRUE(na.rm)) \n        x &lt;- x[!is.na(x)]\n    if (!is.numeric(trim) || length(trim) != 1L) \n        stop(\"'trim' must be numeric of length one\")\n    n &lt;- length(x)\n    if (trim &gt; 0 && n) {\n        if (is.complex(x)) \n            stop(\"trimmed means are not defined for complex data\")\n        if (anyNA(x)) \n            return(NA_real_)\n        if (trim &gt;= 0.5) \n            return(stats::median(x, na.rm = FALSE))\n        lo &lt;- floor(n * trim) + 1\n        hi &lt;- n + 1 - lo\n        x &lt;- sort.int(x, partial = unique(c(lo, hi)))[lo:hi]\n    }\n    .Internal(mean(x))\n}\n&lt;bytecode: 0x000001ff79e7e9c8&gt;\n&lt;environment: namespace:base&gt;\n\n\nOk, now you’ve seen some functions. Let’s write our own!\nGoal: Create a standardize() function (creating z-scores for a vector essentially)\n\nTake vector of values\n\nsubtract mean\n\ndivide by standard deviation\n\nFormula: For value i,\n\\[\\frac{(value[i]-mean(value))}{sd(value)}\\]\n\nLet’s take our generic syntax and apply it here.\n\nnameOfFunction &lt;- function(input1, input2, ...) {\n  #code\n  #return something with return()\n  #or returns last value\n}\n\n\nstandardize &lt;- function(vector) {\n  return((vector - mean(vector)) / sd(vector))\n}\n\n\nNote that vector is just the name of the argument! The user could pass something that isn’t a vector and we may be in trouble.\nNow let’s use it! First, create some data:\n\n\nset.seed(10)\ndata &lt;- runif(15)\ndata\n\n [1] 0.50747820 0.30676851 0.42690767 0.69310208 0.08513597 0.22543662\n [7] 0.27453052 0.27230507 0.61582931 0.42967153 0.65165567 0.56773775\n[13] 0.11350898 0.59592531 0.35804998\n\n\n\nApply the function:\n\n\nresult &lt;- standardize(data)\nresult\n\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n\n\nCheck that the result has mean 0 and sd 1\n\n\nmean(result)\n\n[1] 2.312784e-17\n\nsd(result)\n\n[1] 1\n\n\nGoal: Add more inputs\n\nMake centering optional\n\nMake scaling optional\n\n\nstandardize &lt;- function(vector, center, scale) {\n  if (center) {\n    vector &lt;- vector - mean(vector)\n    }\n  if (scale) {\n    vector &lt;- vector / sd(vector)\n    }\n  return(vector)\n  }\n\nHere we’ve added arguments that should implicitly be TRUE or FALSE values (it would be better to give a default value so people using the function would know what is expected).\n\nresult &lt;- standardize(data, center = TRUE, scale = TRUE)\nresult\n\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n\n\nresult &lt;- standardize(data, center = FALSE, scale = TRUE)\nresult\n\n [1] 2.6115093 1.5786467 2.1968891 3.5667395 0.4381141 1.1601086 1.4127484\n [8] 1.4012961 3.1690897 2.2111121 3.3534540 2.9216081 0.5841231 3.0666627\n[15] 1.8425438\n\n\n\nGive center and scale default arguments\n\n\nstandardize &lt;- function(vector, center = TRUE, scale = TRUE) {\n  if (center) {\n    vector &lt;- vector - mean(vector)\n    }\n  if (scale) {\n    vector &lt;- vector / sd(vector)\n    }\n  return(vector)\n  }\n\n\nApply it! The defaults will be used and aren’t necessary if you don’t want to change things.\n\n\nstandardize(data, center = TRUE, scale = TRUE)\n\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\nstandardize(data)\n\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n\nGoal: Also return\n\nmean() of original data\n\nsd() of original data\n\nReturn more than 1 object by returning a list (so we return one object, but a very flexible object that easily contains other objects!)\n\nstandardize &lt;- function(vector, center = TRUE, scale = TRUE) {\n  mean &lt;- mean(vector) #save these so we can return them\n  stdev &lt;- sd(vector)\n  if (center) {\n    vector &lt;- vector - mean\n    }\n  if (scale) {\n    vector &lt;- vector / stdev\n    }\n  return(list(vector, mean, stdev))\n  }\n\n\nApply it!\n\n\nresult &lt;- standardize(data)\nresult  \n\n[[1]]\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n[[2]]\n[1] 0.4082695\n\n[[3]]\n[1] 0.1943237\n\nresult[[2]]\n\n[1] 0.4082695\n\n\n\nWe can fancy up what we return by giving names to the list elements!\n\n\nstandardize &lt;- function(vector, center = TRUE, scale = TRUE) {\n  mean &lt;- mean(vector)\n  stdev &lt;- sd(vector)\n  if (center) {\n    vector &lt;- vector - mean\n    }\n  if (scale) {\n    vector &lt;- vector / stdev\n    }\n  return(list(result = vector, mean = mean, sd = stdev))\n  }\n\n\nApply it!\n\n\nresult &lt;- standardize(data, center = TRUE, scale = TRUE)\nresult  \n\n$result\n [1]  0.51053294 -0.52232963  0.09591275  1.46576309 -1.66286222 -0.94086777\n [7] -0.68822797 -0.69968029  1.06811337  0.11013572  1.25247769  0.82063172\n[13] -1.51685322  0.96568634 -0.25843252\n\n$mean\n[1] 0.4082695\n\n$sd\n[1] 0.1943237\n\nresult$sd\n\n[1] 0.1943237\n\n\n\n\nOften you want to check on inputs to make sure they are of the right form (that’s good practice if you are going to share your code). You can use if() or switch() to do this check.\nHere we’ll write a function to create a summary (mean, median, or trimmed mean).\n\nFirst we check the input to make sure it is a numeric vector.\nThen we use stop() to jump out if that condition isn’t met.\nIf the condition is met, we use switch() an alternative to if/then/else to pick which function to apply.\n\n\nsummarizer &lt;- function(vec, type, trim = 0.05) {\n  if(!is.vector(vec) | !is.numeric(vec)){\n    stop(\"Not a vector or not numeric my friend.\")\n  }\n  switch(type, \n         mean = mean(vec),\n         median = median(vec),\n         trimmed = mean(vec, trim),\n         stop(\"Mistake!\")\n         )\n}\nsummarizer(letters, \"mean\")\n\nError in summarizer(letters, \"mean\"): Not a vector or not numeric my friend.\n\nsummarizer(c(1,1,1,6,10), \"mean\")\n\n[1] 3.8\n\nsummarizer(c(1,1,1,6,10), \"trimmed\", 0.2)\n\n[1] 2.666667\n\nsummarizer(c(1,1,1,6,10), \"means\")\n\nError in summarizer(c(1, 1, 1, 6, 10), \"means\"): Mistake!\n\n\n\n\n\nThat’s the basics of function writing. Let’s talk about a framework to make coherent code. Use of consistent naming schemes is important!\nGenerally, when naming objects they must:\n\nstart with a letter\n\nonly have letters, numbers, _, and .\n\nWhen we write functions and create objects we should try to follow this advice:\n\nFunctions named using verbs\n\nstandardize() or find_mean() or renderDataTable()\n\nData objects named using nouns\n\nmy_df or weather_df\n\n\nNaming things is actually really tough… You should try to follow a common naming scheme:\n\nsnake_case_used\n\ncamelCaseUsed\n\nUpperCamelCase\n\nuse.of.periods (not recommended)\n\nYou’ll also need to name inputs to your functions. Try to stick to these when possible:\n\nx, y, z: vectors\n\nw: a vector of weights\n\ndf: a data frame\ni, j: numeric indices (typically rows and columns)\nn: length, or number of rows\np: number of columns\n\nOtherwise, consider matching names of arguments in existing R functions. For example, use na.rm to determine if missing values should be removed.\nThere are some readings on this available in a previous lesson.\n\n\n\nYou might wonder why sometimes we name our arguments when we call our functions and sometimes we don’t. Generally, we don’t name the first 2-3 arguments but name ones after that. However, that is just convention. In R, you can use positional matching for everything or name each input, or combine the two ideas!\nLet’s look at some examples. Consider the inputs of the cor() function\n\nfunction (x, y = NULL, use = \"everything\", method = c(\"pearson\", \n    \"kendall\", \"spearman\")) \n\n\nApply it to iris data using positional matching (first argument to x second to y):\n\n\ncor(iris$Sepal.Length, iris$Sepal.Width)\n\n[1] -0.1175698\n\n\n\nR will use positional matching for all inputs not explicitly named. Here it applies iris$Sepal.Width to the first input of the function that wasn’t specified, here y.\n\n\ncor(x = iris$Sepal.Length, method = \"spearman\", iris$Sepal.Width)\n\n[1] -0.1667777\n\n\n\nR will also do partial matching but you should avoid this generally.\n\n\ncor(iris$Sepal.Length, iris$Sepal.Width, met = \"spearman\")\n\n\n\n\nLastly, let’s take up the idea of an infix function. An infix function is a function that goes between arguments (as opposed to prefix that goes prior to the arguments - which is what we usually do).\n\nmean(3:5) #prefix\n\n[1] 4\n\n3 + 5 #+ is infix\n\n[1] 8\n\n`+`(3, 5) #used as a prefix function\n\n[1] 8\n\n\nCommon built-in infix functions include:\n\n:: (look directly in a package for a function)\n$ (grab a column)\n^\n*\n/\n+\n-\n&gt;\n&gt;=\n&lt;\n&lt;=\n==\n!=\n& (and)\n| (or)\n&lt;- (storage arrow)\n|&gt; (pipe!)\n\nOthers infix operators use %symbol% syntax:\n\n%*% (matrix multiplication)\n%in% (check if LHS value(s) is(are) in RHS value(s)\n\nWe can call infix functions like prefix functions if we need to using the backtick symbol ` (top left of the keyboard usually)\n\ncars &lt;- as.matrix(cars)\nt(cars) %*% cars\n\n      speed   dist\nspeed 13228  38482\ndist  38482 124903\n\n\n\n`%*%`(t(cars), cars)\n\n      speed   dist\nspeed 13228  38482\ndist  38482 124903\n\n\nYou can also write your own infix function!\n\n`%+%` &lt;- function(a, b) paste0(a, b)\n\"new\" %+% \" string\"\n\n[1] \"new string\"\n\n\nR actually allows you to overwrite + and other operators: just don’t do that… that wouldn’t be good (unless you really want to mess with someone)\nWith infix functions we can use precedence rules to save typing:\n\nx &lt;- y &lt;- 2\n`&lt;-`(x, `&lt;-`(y, 2)) #interpretation of above code!\n\nx &lt;- y = 2# error! &lt;- has higher precedence\n`=`(`&lt;-`(x, y), 2) #interpretation of above code!\n\nx = y &lt;- 2 # this will work!\n`=`(x, `&lt;-`(y, 2)) #interpretation of above code!\n\nThis is one of the major differences between = and &lt;- usage. You can’t do\n\nx = y = 2\n\nbut can do it with the storage arrow.\nThere is a weird difference between how infix functions are evaluated. For user defined infix functions, they evaluate left to right. For built-in ones, they evaluate right to left!\n\nUser defined example:\n\n\n`%-%` &lt;- function(a, b) {\n    paste0(\"(\", a, \" %-% \", b, \")\")\n}\n\"a\" %-% \"b\" %-% \"c\" #user defined infix are evaluated left to right!\n\n[1] \"((a %-% b) %-% c)\"\n\n`%-%`(`%-%`(\"a\", \"b\"), \"c\")  #interpretation of above code!\n\n[1] \"((a %-% b) %-% c)\"\n\n\n\nBuilt-in example:\n\n\nx &lt;- y &lt;- 2\n`&lt;-`(x, `&lt;-`(y, 2)) #interpretation of above code!\n\n\n\n\nThis one deserves its own section! The pipe operator (%&gt;%) was made popular by the tidyverse and the magrittr package. You would need to read in dplyr (part of the tidyverse) or magrittr to have access to the pipe.\nDue to the popularity, R created a Base R pipe (|&gt;). The idea of the pipe is to make code more readable! Essentially, you can read code left to right when using a pipe instead of inside out.\nConsider the code below:\n\nlibrary(dplyr)\narrange(select(filter(as_tibble(Lahman::Batting), teamID == \"PIT\"), playerID, G, X2B), desc(X2B))\n\n# A tibble: 5,110 × 3\n   playerID      G   X2B\n   &lt;chr&gt;     &lt;int&gt; &lt;int&gt;\n 1 wanerpa01   154    62\n 2 sanchfr01   157    53\n 3 wanerpa01   148    53\n 4 wanerpa01   152    50\n 5 comorad01   152    47\n 6 mclouna01   152    46\n 7 parkeda01   158    45\n 8 vanslan01   154    45\n 9 wagneho01   135    45\n10 bayja01     162    44\n# ℹ 5,100 more rows\n\n\n\nForget what the functions do for a minute. To parse this we need to start on the inside.\n\nThe first function is as_tibble(Lahman::Batting)\nThe result of that is then the first argument to filter()\nThe result of this is then the first argument to select()\nThe result of that is then the first argument to arrange()\n\nYikes. Piping makes things way easier to read!\n\n\nLahman::Batting |&gt; #read the pipe as \"then\"\n  as_tibble() |&gt;\n  filter(teamID == \"PIT\") |&gt;\n  select(playerID, G, X2B) |&gt; \n  arrange(desc(X2B)) \n\n# A tibble: 5,110 × 3\n   playerID      G   X2B\n   &lt;chr&gt;     &lt;int&gt; &lt;int&gt;\n 1 wanerpa01   154    62\n 2 sanchfr01   157    53\n 3 wanerpa01   148    53\n 4 wanerpa01   152    50\n 5 comorad01   152    47\n 6 mclouna01   152    46\n 7 parkeda01   158    45\n 8 vanslan01   154    45\n 9 wagneho01   135    45\n10 bayja01     162    44\n# ℹ 5,100 more rows\n\n\n\nThis is easy to parse!\n\nFirst take the Batting dataset and turn it into a tibble (special data frame)\nThen filter it\nThen select from that\nThen arrange that\n\n\nGenerically, |&gt; does the following\n\nx |&gt; f(y) turns into f(x,y)\n\nx |&gt; f(y) |&gt; g(z) turns into g(f(x, y), z)\n\nWe’ll be using this a lot from here on out!\n\n\n\nPlease pop this video out and watch it in the full panopto player!\n\n\n\n\nFunctions allow you to customize your code\n\nCan specify default values and return multiple objects using a named list\nMuch more to know!\n\nUnnamed arguments\nInput matching, environments, and lazy evaluation\nWriting pipeable functions & side-effect functions\nInfix functions\nHelper functions and function writing strategy\n\nNaming conventions and input matching\nstop() and switch()\ninfix functions\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Data Science & R Basics",
      "Writing Functions"
    ]
  },
  {
    "objectID": "Notes/15-Packages_Landing.html",
    "href": "Notes/15-Packages_Landing.html",
    "title": "Packages",
    "section": "",
    "text": "The video below formally discusses R packages. Packages are collections of code (or data, etc.) around some topic. Packages (also called modules in other languages) extend what you can do with R.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Packages"
    ]
  },
  {
    "objectID": "Notes/15-Packages_Landing.html#notes",
    "href": "Notes/15-Packages_Landing.html#notes",
    "title": "Packages",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThis wraps up the content for week 3. Now we require some practice! You should head back to our Moodle site to check out your homework assignment for this week.\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Packages"
    ]
  },
  {
    "objectID": "Notes/17-Reading_Delimited_Data.html",
    "href": "Notes/17-Reading_Delimited_Data.html",
    "title": "Reading Delimited Data",
    "section": "",
    "text": "As one of our goals is to read in and wrangle data, we need to learn how to effectively take raw data (data not in R) and bring it into R. For most of our data sources, we’ll store the data as a data frame (usually a tibble). For some types of data we’ll need to read it in as a character string or as a list and then parse it with R.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Reading Delimited Data"
    ]
  },
  {
    "objectID": "Notes/17-Reading_Delimited_Data.html#data-formats",
    "href": "Notes/17-Reading_Delimited_Data.html#data-formats",
    "title": "Reading Delimited Data",
    "section": "Data Formats",
    "text": "Data Formats\nData comes in many formats such as\n\n‘Delimited’ data: Character (such as ‘,’ , ‘&gt;’, or [’ ’]) separated data\nFixed field data\nExcel data\nFrom other statistical software, Ex: SPSS formatted data or SAS data sets\nFrom a database\nFrom an Application Programming Interface (API)\n\nAs with many tasks in R, there are many ways to read in data from these sources.\n\nWe could stick with Base R (use functions like read.csv())\nUse functions from a particular ecosystem (tidyverse or data.table)\n\nWe’ll use the tidyverse due to its popularity and ease of functionality.\n\nMake sure tidyverse package is installed (this can take a while)\n\n\n#does not evaluate here\ninstall.packages(\"tidyverse\") #only run this once on your computer!\n\n\nLoad the library into your current session\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nYou can see this loads in the eight core packages mentioned previously. The warnings can easily be ignored but we should take care with the conflicts. We’ve overwritten some functions from Base R. Recall, we can call those functions explicitly if we’d like (stats::filter()).",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Reading Delimited Data"
    ]
  },
  {
    "objectID": "Notes/17-Reading_Delimited_Data.html#locating-files",
    "href": "Notes/17-Reading_Delimited_Data.html#locating-files",
    "title": "Reading Delimited Data",
    "section": "Locating Files",
    "text": "Locating Files\n\nOnce our library is loaded, check help(read_csv) in your console. This brings up help for a suite of functions useful for reading in delimited data.\nFocus on file argument as everything else has defaults. Notice a path to a file, a connection, or literal data must be given.\n\nBefore we start reading in data, let’s recap how R finds files.\n\nWe can give a full path name to the file\n\nex: C:/Users/jbpost2/Documents/Repos/ST-558/datasets/\nex: C:\\\\\\\\Users\\\\\\\\jbpost2\\\\\\\\Documents\\\\\\\\Repos\\\\\\\\ST-558\\\\\\\\datasets\n\nFull path names are not good to use generally!\n\nIf you share your code with someone else, they don’t have the same folder structure, username, etc.\nInstead, use a relative path. That is, a path from R’s current working directory (the place it looks by default)\n\nIt is recommend to do everything in an R project!\n\nWhen you create an R project, you might note that it gets associated with a directory (or folder or repo). That folder is what the project uses as the working directory.\nYou should try to always use relative paths from your project’s working directory.\nNote: When you render a .qmd (or .Rmd) file, the working directory for that rendering is the folder in which the .qmd (or .Rmd) file lives in.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Reading Delimited Data"
    ]
  },
  {
    "objectID": "Notes/17-Reading_Delimited_Data.html#plan",
    "href": "Notes/17-Reading_Delimited_Data.html#plan",
    "title": "Reading Delimited Data",
    "section": "Plan",
    "text": "Plan\n\nGo though examples of reading different types of data raw data\n\n\n\n\n\n\n\n\n\nType of file\nPackage\nFunction\n\n\n\n\nDelimited\nreadr\nread_csv(), read_tsv(),read_table(), read_delim(...,delim = ,...)\n\n\nExcel (.xls,.xlsx)\nreadxl\nread_excel()\n\n\nSPSS (.sav)\nhaven\nread_spss()\n\n\nSAS (.sas7bdat)\nhaven\nread_sas()\n\n\n\n\nReading CSV Files\nLet’s start by considering a comma separated value (or CSV) file. This is a common basic format for raw data in which the delimiter is a comma (,).\nSuppose we want to read in the file called bikeDetails.csv available at: https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv\n\nWe can download the file and store it locally, reading it in from there\nOr, for this type of file, we can also read it directly from the web!\n\nWe’ll use the read_csv() function from the readr package. The inputs are:\nread_csv(\n  file,\n  col_names = TRUE,\n  col_types = NULL,\n  col_select = NULL,\n  id = NULL,\n  locale = default_locale(),\n  na = c(\"\", \"NA\"),\n  quoted_na = TRUE,\n  quote = \"\\\"\",\n  comment = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  name_repair = \"unique\",\n  num_threads = readr_threads(),\n  progress = show_progress(),\n  show_col_types = should_show_types(),\n  skip_empty_rows = TRUE,\n  lazy = should_read_lazy()\n)\nWe really only need to specify the file argument but we see there are a few others that might be useful. We’ll cover some important arguments shortly. Let’s start with a basic call and see how it works:\n\nbike_details &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv\")\n\nRows: 1061 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, seller_type, owner\ndbl (4): selling_price, year, km_driven, ex_showroom_price\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike_details\n\n# A tibble: 1,061 × 7\n   name        selling_price  year seller_type owner km_driven ex_showroom_price\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n 1 Royal Enfi…        175000  2019 Individual  1st …       350                NA\n 2 Honda Dio           45000  2017 Individual  1st …      5650                NA\n 3 Royal Enfi…        150000  2018 Individual  1st …     12000            148114\n 4 Yamaha Faz…         65000  2015 Individual  1st …     23000             89643\n 5 Yamaha SZ …         20000  2011 Individual  2nd …     21000                NA\n 6 Honda CB T…         18000  2010 Individual  1st …     60000             53857\n 7 Honda CB H…         78500  2018 Individual  1st …     17000             87719\n 8 Royal Enfi…        180000  2008 Individual  2nd …     39000                NA\n 9 Hero Honda…         30000  2010 Individual  1st …     32000                NA\n10 Bajaj Disc…         50000  2016 Individual  1st …     42000             60122\n# ℹ 1,051 more rows\n\n\nNotice the fancy printing! As we read the data in with a function from the tidyverse, we have our data in the form of a tibble (special data frame).\nAside from the special printing, tibbles have one other important difference from data frames: they do not coerce down to a vector when you subset to only one column using [\n\nbike_details[1:10,1]\n\n# A tibble: 10 × 1\n   name                                \n   &lt;chr&gt;                               \n 1 Royal Enfield Classic 350           \n 2 Honda Dio                           \n 3 Royal Enfield Classic Gunmetal Grey \n 4 Yamaha Fazer FI V 2.0 [2016-2018]   \n 5 Yamaha SZ [2013-2014]               \n 6 Honda CB Twister                    \n 7 Honda CB Hornet 160R                \n 8 Royal Enfield Bullet 350 [2007-2011]\n 9 Hero Honda CBZ extreme              \n10 Bajaj Discover 125                  \n\n\n\nas.data.frame(bike_details)[1:10 ,1]\n\n [1] \"Royal Enfield Classic 350\"           \n [2] \"Honda Dio\"                           \n [3] \"Royal Enfield Classic Gunmetal Grey\" \n [4] \"Yamaha Fazer FI V 2.0 [2016-2018]\"   \n [5] \"Yamaha SZ [2013-2014]\"               \n [6] \"Honda CB Twister\"                    \n [7] \"Honda CB Hornet 160R\"                \n [8] \"Royal Enfield Bullet 350 [2007-2011]\"\n [9] \"Hero Honda CBZ extreme\"              \n[10] \"Bajaj Discover 125\"                  \n\n\nIf we use our usual $ operator we do coerce to a vector though\n\nbike_details$name[1:10]\n\n [1] \"Royal Enfield Classic 350\"           \n [2] \"Honda Dio\"                           \n [3] \"Royal Enfield Classic Gunmetal Grey\" \n [4] \"Yamaha Fazer FI V 2.0 [2016-2018]\"   \n [5] \"Yamaha SZ [2013-2014]\"               \n [6] \"Honda CB Twister\"                    \n [7] \"Honda CB Hornet 160R\"                \n [8] \"Royal Enfield Bullet 350 [2007-2011]\"\n [9] \"Hero Honda CBZ extreme\"              \n[10] \"Bajaj Discover 125\"                  \n\n\nThe function commonly used from the tidyverse to grab a single column and return it as a vector is the pull() function from dplyr.\n\nbike_details[1:10, ] |&gt;\n  pull(name)\n\n [1] \"Royal Enfield Classic 350\"           \n [2] \"Honda Dio\"                           \n [3] \"Royal Enfield Classic Gunmetal Grey\" \n [4] \"Yamaha Fazer FI V 2.0 [2016-2018]\"   \n [5] \"Yamaha SZ [2013-2014]\"               \n [6] \"Honda CB Twister\"                    \n [7] \"Honda CB Hornet 160R\"                \n [8] \"Royal Enfield Bullet 350 [2007-2011]\"\n [9] \"Hero Honda CBZ extreme\"              \n[10] \"Bajaj Discover 125\"                  \n\n\nOk, back to the main task - reading the data in. We see in the fancy printing that R has each column stored in a particular format. How did R determine the column types?\nFrom the help under col_types we see the following:\n\nOne of NULL, a cols() specification, or a string. See vignette(“readr”) for more details.\nIf NULL, all column types will be inferred from guess_max rows of the input, interspersed throughout the file. This is convenient (and fast), but not robust. If the guessed types are wrong, you’ll need to increase guess_max or supply the correct types yourself.\nColumn specifications created by list() or cols() must contain one column specification for each column. If you only want to read a subset of the columns, use cols_only().\nAlternatively, you can use a compact string representation where each character represents one column:\nc = character\ni = integer\nn = number\nd = double\nl = logical\nf = factor\nD = date\nT = date time\nt = time\n? = guess\n_ or - = skip\nBy default, reading a file without a column specification will print a message showing what readr guessed they were. To remove this message, set show_col_types = FALSE or set ’options(readr.show_col_types = FALSE).\n\nAhh, so the guess_max argument tells our function to scan the first x number of rows and try to determine the column type. Note it says you may need to increase that argument to make sure data can be read in.\n\nChecking column type is a basic data validation step!\nYou should check that each column was read in the way you would expect. If not, you may need to clean the data and convert the column to the appropriate data type.\n\n\n\nReading in Any Delimited File\n\nFunctions from readr and their purpose\n\n\n\n\n\n\n\n\nDelimiter\nFunction\n\n\n\n\ncomma ‘,’\nread_csv()\n\n\ntab\nread_tsv()\n\n\nspace ’ ’\nread_table()\n\n\nsemi-colon ‘;’\nread_csv2() (This uses ; instead of commas, which is common in many countries)\n\n\nother\nread_delim(…,delim = ,…)\n\n\n\nConsider the umps.txt file available at: https://www4.stat.ncsu.edu/~online/datasets/umps2012.txt\n\nDownload the file or open it in your browser.\nNote that the delimiter is a &gt; sign!\nNote that there are no column names provided:\n\nYear Month Day Home Away HPUmpire are the appropriate column names\n\n\nWe can use read_delim() to read in a generic delimited raw data file! Let’s check the help:\nread_delim(\n  file,\n  delim = NULL,\n  quote = \"\\\"\",\n  escape_backslash = FALSE,\n  escape_double = TRUE,\n  col_names = TRUE,\n  col_types = NULL,\n  col_select = NULL,\n  id = NULL,\n  locale = default_locale(),\n  na = c(\"\", \"NA\"),\n  quoted_na = TRUE,\n  comment = \"\",\n  trim_ws = FALSE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  name_repair = \"unique\",\n  num_threads = readr_threads(),\n  progress = show_progress(),\n  show_col_types = should_show_types(),\n  skip_empty_rows = TRUE,\n  lazy = should_read_lazy()\n)\nWe see two arguments we need to worry about right off:\n\nfile (path to file)\ndelim the delimiter used in the raw data file\n\nSingle character used to separate fields within a record.\nWe want to specify a character string with the delimiter for this.\n\n\nAs we don’t have column names we should also consider the col_names argument. This is set to TRUE by default. The help says:\n\nEither TRUE, FALSE or a character vector of column names.\nIf TRUE, the first row of the input will be used as the column names, and will not be included in the data frame. If FALSE, column names will be generated automatically: X1, X2, X3 etc.\nIf col_names is a character vector, the values will be used as the names of the columns, and the first row of the input will be read into the first row of the output data frame.\nMissing (NA) column names will generate a warning, and be filled in with dummy names …1, …2 etc. Duplicate column names will generate a warning and be made unique, see name_repair to control how this is done.\n\n\nThis means we want to set the value to FALSE or supply a character vector with the corresponding names!\n\n\nump_data &lt;- read_delim(\"https://www4.stat.ncsu.edu/~online/datasets/umps2012.txt\", \n                       delim = \"&gt;\",\n                       col_names = c(\"Year\", \"Month\", \"Day\", \"Home\", \"Away\", \"HPUmpire\")\n)\n\nRows: 2359 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"&gt;\"\nchr (3): Home, Away, HPUmpire\ndbl (3): Year, Month, Day\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nump_data\n\n# A tibble: 2,359 × 6\n    Year Month   Day Home  Away  HPUmpire        \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           \n 1  2012     4    12 MIN   LAA   D.J. Reyburn    \n 2  2012     4    12 SD    ARI   Marty Foster    \n 3  2012     4    12 WSH   CIN   Mike Everitt    \n 4  2012     4    12 PHI   MIA   Jeff Nelson     \n 5  2012     4    12 CHC   MIL   Fieldin Culbreth\n 6  2012     4    12 LAD   PIT   Wally Bell      \n 7  2012     4    12 TEX   SEA   Doug Eddings    \n 8  2012     4    12 COL   SF    Ron Kulpa       \n 9  2012     4    12 DET   TB    Mark Carlson    \n10  2012     4    13 NYY   LAA   Mike DiMuro     \n# ℹ 2,349 more rows\n\n\n\nQuick Aside: Date data\nWe see that the first three columns represent a Year, Month, and Day. These are currently stored as dbl (numeric data). Obviously, that’s not great. We can’t easily subtract two dates to get the difference in time or anything like that.\nInsert the lubridate package. This is the tidyverse package for dealing with dates!\n\ninstall.packages(\"lubridate\") #only do this once!\n\n\nlibrary(lubridate) #do this each session\n\nIf we look at help(lubridate) you can see under the section for parsing dates:\n\nLubridate’s parsing functions read strings into R as POSIXct date-time objects. Users should choose the function whose name models the order in which the year (‘y’), month (‘m’) and day (‘d’) elements appear the string to be parsed: dmy(), myd(), ymd(), ydm(), dym(), mdy(), ymd_hms()). A very flexible and user friendly parser is provided by parse_date_time().\n\nOk, so we want to use ymd() or a variant and pass it a character string of the date to parse! No problem, we know how to do that :)\nUnder the help for the ymd() function, examples are given at the bottom of how to use the function. One example is\n\nx &lt;- c(\"09-01-01\", \"09-01-02\", \"09-01-03\")\nymd(x)\n\n[1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\"\n\n\nLet’s write a quick loop to loop through our observations, create this type of character string, and output a date variable!\nWe’ll see a better way to do this once we get into dplyr but for now, let’s initialize column to store date in and give it a date value.\n\nump_data$date &lt;- ymd(\"2012-01-01\")\nump_data\n\n# A tibble: 2,359 × 7\n    Year Month   Day Home  Away  HPUmpire         date      \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;date&gt;    \n 1  2012     4    12 MIN   LAA   D.J. Reyburn     2012-01-01\n 2  2012     4    12 SD    ARI   Marty Foster     2012-01-01\n 3  2012     4    12 WSH   CIN   Mike Everitt     2012-01-01\n 4  2012     4    12 PHI   MIA   Jeff Nelson      2012-01-01\n 5  2012     4    12 CHC   MIL   Fieldin Culbreth 2012-01-01\n 6  2012     4    12 LAD   PIT   Wally Bell       2012-01-01\n 7  2012     4    12 TEX   SEA   Doug Eddings     2012-01-01\n 8  2012     4    12 COL   SF    Ron Kulpa        2012-01-01\n 9  2012     4    12 DET   TB    Mark Carlson     2012-01-01\n10  2012     4    13 NYY   LAA   Mike DiMuro      2012-01-01\n# ℹ 2,349 more rows\n\n\nNow we’ll loop through, paste together the three columns, and parse the date (storing it appropriately).\n\nfor (i in 1:nrow(ump_data)){\n  ump_data$date[i] &lt;- ymd(paste(ump_data$Year[i],\n                                ump_data$Month[i],\n                                ump_data$Day[i], \n                                sep = \"-\"))\n}\nump_data\n\n# A tibble: 2,359 × 7\n    Year Month   Day Home  Away  HPUmpire         date      \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;date&gt;    \n 1  2012     4    12 MIN   LAA   D.J. Reyburn     2012-04-12\n 2  2012     4    12 SD    ARI   Marty Foster     2012-04-12\n 3  2012     4    12 WSH   CIN   Mike Everitt     2012-04-12\n 4  2012     4    12 PHI   MIA   Jeff Nelson      2012-04-12\n 5  2012     4    12 CHC   MIL   Fieldin Culbreth 2012-04-12\n 6  2012     4    12 LAD   PIT   Wally Bell       2012-04-12\n 7  2012     4    12 TEX   SEA   Doug Eddings     2012-04-12\n 8  2012     4    12 COL   SF    Ron Kulpa        2012-04-12\n 9  2012     4    12 DET   TB    Mark Carlson     2012-04-12\n10  2012     4    13 NYY   LAA   Mike DiMuro      2012-04-13\n# ℹ 2,349 more rows\n\n\nGreat! Now we can subtract dates and do other useful things with date data. We’ll cover this kind of code shortly but we might want to know the days between being home plate umpire:\n\nump_data |&gt;\n  filter(HPUmpire == \"Marty Foster\") |&gt;\n  mutate(days_off = date - lag(date))\n\n# A tibble: 34 × 8\n    Year Month   Day Home  Away  HPUmpire     date       days_off\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;        &lt;date&gt;     &lt;drtn&gt;  \n 1  2012     4    12 SD    ARI   Marty Foster 2012-04-12 NA days \n 2  2012     4    16 SF    PHI   Marty Foster 2012-04-16  4 days \n 3  2012     4    21 KC    TOR   Marty Foster 2012-04-21  5 days \n 4  2012     4    25 TB    LAA   Marty Foster 2012-04-25  4 days \n 5  2012     4    29 BAL   OAK   Marty Foster 2012-04-29  4 days \n 6  2012     5     4 CHC   LAD   Marty Foster 2012-05-04  5 days \n 7  2012     5     8 HOU   MIA   Marty Foster 2012-05-08  4 days \n 8  2012     5    13 CIN   WSH   Marty Foster 2012-05-13  5 days \n 9  2012     5    17 DET   MIN   Marty Foster 2012-05-17  4 days \n10  2012     5    21 MIL   SF    Marty Foster 2012-05-21  4 days \n# ℹ 24 more rows\n\n\nThis is easily done as we can take a date and subtract another date (via lag(date), which grabs the date from the previous row).\n\n\n\nReading in Tricky Raw Data Files\nSometimes our raw data will be in a .txt type file but not in a super nice format. In that case, we have a few functions that can help us out:\n\nread_file()\n\nreads an entire file into a single string\n\nread_lines()\n\nreads a file into a character vector with one element per line\n\n\nOnce the data is read into an R object, we can then usually parse it with regular expressions. Hopefully, that’s not something you need to do very often!\n\n\nQuick R Video\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\nRecap!\nThe tidyverse has a package called readr that has many functions for reading in delimited data (raw data separated by a character string)\n\n\n\n\n\n\n\nDelimiter\nFunction\n\n\n\n\ncomma ‘,’\nread_csv()\n\n\ntab\nread_tsv()\n\n\nspace ’ ’\nread_table()\n\n\nsemi-colon ‘;’\nread_csv2() (This uses ; instead of commas, which is common in many countries)\n\n\nother\nread_delim(…,delim = ,…)\n\n\n\nlubridate is another package in the tidyverse that is really useful for dealing with date-type data!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Reading Delimited Data"
    ]
  },
  {
    "objectID": "Notes/19-Manipulating_Data_with_dplyr.html",
    "href": "Notes/19-Manipulating_Data_with_dplyr.html",
    "title": "Manipulating Data with dplyr",
    "section": "",
    "text": "Thinking about our big goals (doing data science):\nWe now have a good idea about reading in certain types of data. Let’s take a bit of time going through common data manipulation tasks before returning to reading data in.\nThe two major tasks we’ll consider are\nWe can use Base R for this (via [ or the subset() function). However, the tidyverse has a more coherent set of functions to allow us to do all of our tasks without having to spend as much time learning syntax. So let’s go that route!\nRecall the basic fundamentals about the tidyverse:\nmy_data |&gt;\n  select(var1:var10, var20) |&gt;\n  filter(var1 == \"first\") |&gt;\n  mutate(new_var = lag(var20)) |&gt;\n  ...",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Manipulating Data with `dplyr`"
    ]
  },
  {
    "objectID": "Notes/19-Manipulating_Data_with_dplyr.html#coercing-a-data-frame-to-a-tibble",
    "href": "Notes/19-Manipulating_Data_with_dplyr.html#coercing-a-data-frame-to-a-tibble",
    "title": "Manipulating Data with dplyr",
    "section": "Coercing a Data Frame to a Tibble",
    "text": "Coercing a Data Frame to a Tibble\nIf you happen to have a data frame that isn’t already a tibble, we can easily coerce it using the as_tibble() function.\nConsider a data set on major league baseball players (batting statistics):\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(Lahman)\n\nWarning: package 'Lahman' was built under R version 4.4.2\n\nbatting_tbl &lt;- as_tibble(Batting)\nbatting_tbl\n\n# A tibble: 113,799 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n 2 aardsda01   2006     1 CHN    NL       45     2     0     0     0     0     0\n 3 aardsda01   2007     1 CHA    AL       25     0     0     0     0     0     0\n 4 aardsda01   2008     1 BOS    AL       47     1     0     0     0     0     0\n 5 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n 6 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n 7 aardsda01   2012     1 NYA    AL        1     0     0     0     0     0     0\n 8 aardsda01   2013     1 NYN    NL       43     0     0     0     0     0     0\n 9 aardsda01   2015     1 ATL    NL       33     1     0     0     0     0     0\n10 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n# ℹ 113,789 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\nNice, now we can work on this tibble!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Manipulating Data with `dplyr`"
    ]
  },
  {
    "objectID": "Notes/19-Manipulating_Data_with_dplyr.html#row-manipulations-with-dplyr",
    "href": "Notes/19-Manipulating_Data_with_dplyr.html#row-manipulations-with-dplyr",
    "title": "Manipulating Data with dplyr",
    "section": "Row Manipulations with dplyr",
    "text": "Row Manipulations with dplyr\nA common task is to only grab certain types of observations (filter rows)\n\n\n\n\n\n\n\n\n\nor rearrange the order of the observations (rows). The two functions from dplyr that help us here are\n\nfilter() - subset rows\narrange() - reorder rows\n\n\nfilter()\nfilter() generally takes a tibble as its first argument and then a logical vector as the next (of the same length as the number of rows):\n\nReturns observations where the number of games played is greater than 50 (the G column):\n\n\nfilter(batting_tbl, G &gt; 50)\n\n# A tibble: 41,727 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n 2 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n 3 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n 4 aaronha01   1955     1 ML1    NL      153   602   105   189    37     9    27\n 5 aaronha01   1956     1 ML1    NL      153   609   106   200    34    14    26\n 6 aaronha01   1957     1 ML1    NL      151   615   118   198    27     6    44\n 7 aaronha01   1958     1 ML1    NL      153   601   109   196    34     4    30\n 8 aaronha01   1959     1 ML1    NL      154   629   116   223    46     7    39\n 9 aaronha01   1960     1 ML1    NL      153   590   102   172    20    11    40\n10 aaronha01   1961     1 ML1    NL      155   603   115   197    39    10    34\n# ℹ 41,717 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\nNotice the number of observations is far less now!\n\nOf course, we’ll do many operations so let’s use chaining even in this simple case:\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 50)\n\n# A tibble: 41,727 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n 2 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n 3 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n 4 aaronha01   1955     1 ML1    NL      153   602   105   189    37     9    27\n 5 aaronha01   1956     1 ML1    NL      153   609   106   200    34    14    26\n 6 aaronha01   1957     1 ML1    NL      151   615   118   198    27     6    44\n 7 aaronha01   1958     1 ML1    NL      153   601   109   196    34     4    30\n 8 aaronha01   1959     1 ML1    NL      154   629   116   223    46     7    39\n 9 aaronha01   1960     1 ML1    NL      153   590   102   172    20    11    40\n10 aaronha01   1961     1 ML1    NL      155   603   115   197    39    10    34\n# ℹ 41,717 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nIf we want to return observations where than one condition is TRUE we can either pass additional arguments or use the compound logical operator & we discussed earlier.\n\nCondition on those that played more than 50 games and played in 2018\n\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 50 & yearID == 2018)\n\n# A tibble: 518 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 abreujo02   2018     1 CHA    AL      128   499    68   132    36     1    22\n 2 acunaro01   2018     1 ATL    NL      111   433    78   127    26     4    26\n 3 adamewi01   2018     1 TBA    AL       85   288    43    80     7     0    10\n 4 adamsma01   2018     1 WAS    NL       94   249    37    64     9     0    18\n 5 adducji02   2018     1 DET    AL       59   176    19    47     8     2     3\n 6 adriaeh01   2018     1 MIN    AL      114   335    42    84    23     1     6\n 7 aguilje01   2018     1 MIL    NL      149   492    80   135    25     0    35\n 8 ahmedni01   2018     1 ARI    NL      153   516    61   121    33     5    16\n 9 albieoz01   2018     1 ATL    NL      158   639   105   167    40     5    24\n10 alexasc01   2018     1 LAN    NL       73     5     0     0     0     0     0\n# ℹ 508 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n#equivalently\nbatting_tbl |&gt;\n  filter(G &gt; 50, yearID == 2018)\n\n# A tibble: 518 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 abreujo02   2018     1 CHA    AL      128   499    68   132    36     1    22\n 2 acunaro01   2018     1 ATL    NL      111   433    78   127    26     4    26\n 3 adamewi01   2018     1 TBA    AL       85   288    43    80     7     0    10\n 4 adamsma01   2018     1 WAS    NL       94   249    37    64     9     0    18\n 5 adducji02   2018     1 DET    AL       59   176    19    47     8     2     3\n 6 adriaeh01   2018     1 MIN    AL      114   335    42    84    23     1     6\n 7 aguilje01   2018     1 MIL    NL      149   492    80   135    25     0    35\n 8 ahmedni01   2018     1 ARI    NL      153   516    61   121    33     5    16\n 9 albieoz01   2018     1 ATL    NL      158   639   105   167    40     5    24\n10 alexasc01   2018     1 LAN    NL       73     5     0     0     0     0     0\n# ℹ 508 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nOften we want to check if a variable is in a specific group of values. We might think this is the way:\n\n\n####wrong!!!!! Common mistake\nbatting_tbl |&gt;\n  filter(G &gt; 50, yearID == c(2018, 2019, 2020))\n\n# A tibble: 387 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 adamsma01   2018     1 WAS    NL       94   249    37    64     9     0    18\n 2 alberma01   2019     1 MIL    NL       67     0     0     0     0     0     0\n 3 albieoz01   2018     1 ATL    NL      158   639   105   167    40     5    24\n 4 albieoz01   2019     1 ATL    NL      160   640   102   189    43     8    24\n 5 alfarjo01   2018     1 PHI    NL      108   344    35    90    16     2    10\n 6 alfarjo01   2019     1 MIA    NL      130   431    44   113    14     1    18\n 7 allengr01   2018     1 CLE    AL       91   265    36    68    11     3     2\n 8 allengr01   2019     1 CLE    AL       89   231    30    53     9     3     4\n 9 almoral01   2018     1 CHN    NL      152   444    62   127    24     1     5\n10 almoral01   2019     1 CHN    NL      130   339    41    80    11     1    12\n# ℹ 377 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nHere, R uses recycling on the vector and actually compares the 1st element to 2018, the 2nd to 2019, the third to 2020, the fourth to 2018, the fifth to 2019, …\nUse %in% to choose any observations matching an element of a vector\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 50, yearID %in% c(2018, 2019, 2020))\n\n# A tibble: 1,172 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 abreujo02   2018     1 CHA    AL      128   499    68   132    36     1    22\n 2 abreujo02   2019     1 CHA    AL      159   634    85   180    38     1    33\n 3 abreujo02   2020     1 CHA    AL       60   240    43    76    15     0    19\n 4 acunaro01   2018     1 ATL    NL      111   433    78   127    26     4    26\n 5 acunaro01   2019     1 ATL    NL      156   626   127   175    22     2    41\n 6 adamewi01   2018     1 TBA    AL       85   288    43    80     7     0    10\n 7 adamewi01   2019     1 TBA    AL      152   531    69   135    25     1    20\n 8 adamewi01   2020     1 TBA    AL       54   185    29    48    15     1     8\n 9 adamsma01   2018     1 WAS    NL       94   249    37    64     9     0    18\n10 adamsma01   2019     1 WAS    NL      111   310    42    70    14     0    20\n# ℹ 1,162 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nIf we want an or condition, we use the compound logical operator for that\n\nHere, grab those with either games greater than 100 or those that played in 2018, 2019, or 2020 (or both)\n\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100 | yearID %in% c(2018, 2019, 2020))\n\n# A tibble: 25,417 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n 2 aaronha01   1955     1 ML1    NL      153   602   105   189    37     9    27\n 3 aaronha01   1956     1 ML1    NL      153   609   106   200    34    14    26\n 4 aaronha01   1957     1 ML1    NL      151   615   118   198    27     6    44\n 5 aaronha01   1958     1 ML1    NL      153   601   109   196    34     4    30\n 6 aaronha01   1959     1 ML1    NL      154   629   116   223    46     7    39\n 7 aaronha01   1960     1 ML1    NL      153   590   102   172    20    11    40\n 8 aaronha01   1961     1 ML1    NL      155   603   115   197    39    10    34\n 9 aaronha01   1962     1 ML1    NL      156   592   127   191    28     6    45\n10 aaronha01   1963     1 ML1    NL      161   631   121   201    29     4    44\n# ℹ 25,407 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nAgain, note the change in the observation count!\n\n\n\narrange()\nThe other major observation (row) manipulation is to reorder the observations (rows). This is done through arrange() from dplyr (or sort() in Base R)\n\nLet’s take our result from above and reorder by teamID\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(teamID)\n\n# A tibble: 467 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 ahmedni01   2018     1 ARI    NL      153   516    61   121    33     5    16\n 2 ahmedni01   2019     1 ARI    NL      158   556    79   141    33     6    19\n 3 descada01   2018     1 ARI    NL      138   349    54    83    22     4    13\n 4 dysonja01   2019     1 ARI    NL      130   400    65    92    11     2     7\n 5 escobed01   2019     1 ARI    NL      158   636    94   171    29    10    35\n 6 goldspa01   2018     1 ARI    NL      158   593    95   172    35     5    33\n 7 jonesad01   2019     1 ARI    NL      137   485    66   126    25     1    16\n 8 kellyca02   2019     1 ARI    NL      111   314    46    77    19     0    18\n 9 marteke01   2018     1 ARI    NL      153   520    68   135    26    12    14\n10 marteke01   2019     1 ARI    NL      144   569    97   187    36     9    32\n# ℹ 457 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nRemember to read the pipe (|&gt;) as ‘then’. Here we would say:\n\nTake the batting tibble and then\nfilter the rows to only include those with games greater than 100 and those that played in 2018-2020 and then\narrange the rows by the team name\n\nWe can obtain a secondary arrangement by giving a second column\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(teamID, playerID)\n\n# A tibble: 467 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 ahmedni01   2018     1 ARI    NL      153   516    61   121    33     5    16\n 2 ahmedni01   2019     1 ARI    NL      158   556    79   141    33     6    19\n 3 descada01   2018     1 ARI    NL      138   349    54    83    22     4    13\n 4 dysonja01   2019     1 ARI    NL      130   400    65    92    11     2     7\n 5 escobed01   2019     1 ARI    NL      158   636    94   171    29    10    35\n 6 goldspa01   2018     1 ARI    NL      158   593    95   172    35     5    33\n 7 jonesad01   2019     1 ARI    NL      137   485    66   126    25     1    16\n 8 kellyca02   2019     1 ARI    NL      111   314    46    77    19     0    18\n 9 marteke01   2018     1 ARI    NL      153   520    68   135    26    12    14\n10 marteke01   2019     1 ARI    NL      144   569    97   187    36     9    32\n# ℹ 457 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nWe can reorder descending on a variable\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID)\n\n# A tibble: 467 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 adamsma01   2019     1 WAS    NL      111   310    42    70    14     0    20\n 2 difowi01    2018     1 WAS    NL      148   408    55    94    14     7     7\n 3 doziebr01   2019     1 WAS    NL      135   416    54    99    20     0    20\n 4 eatonad02   2019     1 WAS    NL      151   566   103   158    25     7    15\n 5 harpebr03   2018     1 WAS    NL      159   550   103   137    34     0    34\n 6 kendrho01   2019     1 WAS    NL      121   334    61   115    23     1    17\n 7 rendoan01   2018     1 WAS    NL      136   529    88   163    44     2    24\n 8 rendoan01   2019     1 WAS    NL      146   545   117   174    44     3    34\n 9 roblevi01   2019     1 WAS    NL      155   546    86   139    33     3    17\n10 sotoju01    2018     1 WAS    NL      116   414    77   121    25     1    22\n# ℹ 457 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Manipulating Data with `dplyr`"
    ]
  },
  {
    "objectID": "Notes/19-Manipulating_Data_with_dplyr.html#column-manipulations-with-dplyr",
    "href": "Notes/19-Manipulating_Data_with_dplyr.html#column-manipulations-with-dplyr",
    "title": "Manipulating Data with dplyr",
    "section": "Column Manipulations with dplyr",
    "text": "Column Manipulations with dplyr\nWe may want to subset our variables, rename them, or create new variables.\n\nselect() - Subset Columns\nWe call the subset of our variables selecting columns (or variables)\n\n\n\n\n\n\n\n\n\n\nTo return a single (probably simplified) column we looked at the following methods (one of which is in the tidyverse):\n\ndplyr::pull()\n$\n[ , ]\n\n\nWhen we want to look at pulling more than one column, select() is much better!\n\nSuppose we just wanted to look at the playerID, teamID, and hits type variables: H, X2B X3B, and HR of the players in our subset\nWe can add in a select() function to our chain (thanks again coherent ecosystem!).\n\nOne way is to simply list the columns you want:\n\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H, X2B, X3B, HR)\n\n# A tibble: 467 × 6\n   playerID  teamID     H   X2B   X3B    HR\n   &lt;chr&gt;     &lt;fct&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 adamsma01 WAS       70    14     0    20\n 2 difowi01  WAS       94    14     7     7\n 3 doziebr01 WAS       99    20     0    20\n 4 eatonad02 WAS      158    25     7    15\n 5 harpebr03 WAS      137    34     0    34\n 6 kendrho01 WAS      115    23     1    17\n 7 rendoan01 WAS      163    44     2    24\n 8 rendoan01 WAS      174    44     3    34\n 9 roblevi01 WAS      139    33     3    17\n10 sotoju01  WAS      121    25     1    22\n# ℹ 457 more rows\n\n\n\nWhere we really gain here is the ability to use helper functions when selecting columns!\n\n: to select all contiguous columns\n\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR)\n\n# A tibble: 467 × 6\n   playerID  teamID     H   X2B   X3B    HR\n   &lt;chr&gt;     &lt;fct&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 adamsma01 WAS       70    14     0    20\n 2 difowi01  WAS       94    14     7     7\n 3 doziebr01 WAS       99    20     0    20\n 4 eatonad02 WAS      158    25     7    15\n 5 harpebr03 WAS      137    34     0    34\n 6 kendrho01 WAS      115    23     1    17\n 7 rendoan01 WAS      163    44     2    24\n 8 rendoan01 WAS      174    44     3    34\n 9 roblevi01 WAS      139    33     3    17\n10 sotoju01  WAS      121    25     1    22\n# ℹ 457 more rows\n\n\n\nstarts_with() and ends_with() are also really useful\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(ends_with(\"ID\"), G, AB, H:HR)\n\n# A tibble: 467 × 10\n   playerID  yearID teamID lgID      G    AB     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 adamsma01   2019 WAS    NL      111   310    70    14     0    20\n 2 difowi01    2018 WAS    NL      148   408    94    14     7     7\n 3 doziebr01   2019 WAS    NL      135   416    99    20     0    20\n 4 eatonad02   2019 WAS    NL      151   566   158    25     7    15\n 5 harpebr03   2018 WAS    NL      159   550   137    34     0    34\n 6 kendrho01   2019 WAS    NL      121   334   115    23     1    17\n 7 rendoan01   2018 WAS    NL      136   529   163    44     2    24\n 8 rendoan01   2019 WAS    NL      146   545   174    44     3    34\n 9 roblevi01   2019 WAS    NL      155   546   139    33     3    17\n10 sotoju01    2018 WAS    NL      116   414   121    25     1    22\n# ℹ 457 more rows\n\n\n\nWe can combine those two as well using & and | operators\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(ends_with(\"ID\") | starts_with(\"X\"), G, AB, H, HR)\n\n# A tibble: 467 × 10\n   playerID  yearID teamID lgID    X2B   X3B     G    AB     H    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 adamsma01   2019 WAS    NL       14     0   111   310    70    20\n 2 difowi01    2018 WAS    NL       14     7   148   408    94     7\n 3 doziebr01   2019 WAS    NL       20     0   135   416    99    20\n 4 eatonad02   2019 WAS    NL       25     7   151   566   158    15\n 5 harpebr03   2018 WAS    NL       34     0   159   550   137    34\n 6 kendrho01   2019 WAS    NL       23     1   121   334   115    17\n 7 rendoan01   2018 WAS    NL       44     2   136   529   163    24\n 8 rendoan01   2019 WAS    NL       44     3   146   545   174    34\n 9 roblevi01   2019 WAS    NL       33     3   155   546   139    17\n10 sotoju01    2018 WAS    NL       25     1   116   414   121    22\n# ℹ 457 more rows\n\n\n\nIf our goal is really just to reorder the columns, we can use everything() after specifying the columns of interest\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, H:HR, everything())\n\n# A tibble: 467 × 22\n   playerID      H   X2B   X3B    HR yearID stint teamID lgID      G    AB     R\n   &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 adamsma01    70    14     0    20   2019     1 WAS    NL      111   310    42\n 2 difowi01     94    14     7     7   2018     1 WAS    NL      148   408    55\n 3 doziebr01    99    20     0    20   2019     1 WAS    NL      135   416    54\n 4 eatonad02   158    25     7    15   2019     1 WAS    NL      151   566   103\n 5 harpebr03   137    34     0    34   2018     1 WAS    NL      159   550   103\n 6 kendrho01   115    23     1    17   2019     1 WAS    NL      121   334    61\n 7 rendoan01   163    44     2    24   2018     1 WAS    NL      136   529    88\n 8 rendoan01   174    44     3    34   2019     1 WAS    NL      146   545   117\n 9 roblevi01   139    33     3    17   2019     1 WAS    NL      155   546    86\n10 sotoju01    121    25     1    22   2018     1 WAS    NL      116   414    77\n# ℹ 457 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nSee the help for select() for more information about selection features (these can usually be used in any tidyverse functions where you are selecting columns!)\n\n\n\nrename()\nWe’ve seen the use of colnames() or names() to rename columns. Those are great but aren’t easy to chain. rename() comes in handy in this case!\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\")\n\n# A tibble: 467 × 6\n   playerID  teamID     H Doubles Triples    HR\n   &lt;chr&gt;     &lt;fct&gt;  &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 adamsma01 WAS       70      14       0    20\n 2 difowi01  WAS       94      14       7     7\n 3 doziebr01 WAS       99      20       0    20\n 4 eatonad02 WAS      158      25       7    15\n 5 harpebr03 WAS      137      34       0    34\n 6 kendrho01 WAS      115      23       1    17\n 7 rendoan01 WAS      163      44       2    24\n 8 rendoan01 WAS      174      44       3    34\n 9 roblevi01 WAS      139      33       3    17\n10 sotoju01  WAS      121      25       1    22\n# ℹ 457 more rows\n\n\n\n\nCreating New Variables with dplyr\nOften we want to create new variables!\n\n\n\n\n\n\n\n\n\nThis can be accomplished using mutate(). This function allows us to create one or more variables and append them to our tibble.\n\nFor our dataset from above, suppose we wanted to create an “extra base hits” type column that is the sum of the doubles, triples, and home runs.\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  mutate(Extra_Base_Hits = Doubles + Triples + HR)\n\n# A tibble: 467 × 7\n   playerID  teamID     H Doubles Triples    HR Extra_Base_Hits\n   &lt;chr&gt;     &lt;fct&gt;  &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;           &lt;int&gt;\n 1 adamsma01 WAS       70      14       0    20              34\n 2 difowi01  WAS       94      14       7     7              28\n 3 doziebr01 WAS       99      20       0    20              40\n 4 eatonad02 WAS      158      25       7    15              47\n 5 harpebr03 WAS      137      34       0    34              68\n 6 kendrho01 WAS      115      23       1    17              41\n 7 rendoan01 WAS      163      44       2    24              70\n 8 rendoan01 WAS      174      44       3    34              81\n 9 roblevi01 WAS      139      33       3    17              53\n10 sotoju01  WAS      121      25       1    22              48\n# ℹ 457 more rows\n\n\n\nIf we want to add more than one variable, we just separate the variable definitions with a comma.\n\nLet’s add a Singles variable representing the number of hits minus the number of extra base hits\n\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  mutate(Extra_Base_Hits = Doubles + Triples + HR,\n         Singles = H - Extra_Base_Hits) |&gt;\n  select(playerID, teamID, Singles, Doubles:HR, H, Extra_Base_Hits)\n\n# A tibble: 467 × 8\n   playerID  teamID Singles Doubles Triples    HR     H Extra_Base_Hits\n   &lt;chr&gt;     &lt;fct&gt;    &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;           &lt;int&gt;\n 1 adamsma01 WAS         36      14       0    20    70              34\n 2 difowi01  WAS         66      14       7     7    94              28\n 3 doziebr01 WAS         59      20       0    20    99              40\n 4 eatonad02 WAS        111      25       7    15   158              47\n 5 harpebr03 WAS         69      34       0    34   137              68\n 6 kendrho01 WAS         74      23       1    17   115              41\n 7 rendoan01 WAS         93      44       2    24   163              70\n 8 rendoan01 WAS         93      44       3    34   174              81\n 9 roblevi01 WAS         86      33       3    17   139              53\n10 sotoju01  WAS         73      25       1    22   121              48\n# ℹ 457 more rows\n\n\n\nWe can of course use lots of functions when creating a new variable as well. Some common functions are log(), lead(), lag(), percent_rank(), cumsum(), etc. (see the help for mutate for a nice list).\n\nLet’s use percent_rank() to get a new column telling us where they rank for number of hits\n\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  mutate(Extra_Base_Hits = Doubles + Triples + HR,\n         Singles = H - Extra_Base_Hits,\n         H_Percentile = percent_rank(H)) |&gt;\n  select(playerID, teamID, H, H_Percentile, everything()) \n\n# A tibble: 467 × 9\n   playerID  teamID     H H_Percentile Doubles Triples    HR Extra_Base_Hits\n   &lt;chr&gt;     &lt;fct&gt;  &lt;int&gt;        &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;           &lt;int&gt;\n 1 adamsma01 WAS       70       0.0536      14       0    20              34\n 2 difowi01  WAS       94       0.223       14       7     7              28\n 3 doziebr01 WAS       99       0.266       20       0    20              40\n 4 eatonad02 WAS      158       0.830       25       7    15              47\n 5 harpebr03 WAS      137       0.665       34       0    34              68\n 6 kendrho01 WAS      115       0.436       23       1    17              41\n 7 rendoan01 WAS      163       0.865       44       2    24              70\n 8 rendoan01 WAS      174       0.923       44       3    34              81\n 9 roblevi01 WAS      139       0.687       33       3    17              53\n10 sotoju01  WAS      121       0.517       25       1    22              48\n# ℹ 457 more rows\n# ℹ 1 more variable: Singles &lt;int&gt;\n\n\n\nA common comparison we want to do is to take a particular value and compare it to its mean. Let’s add in a mean variable for hits as well.\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  mutate(Extra_Base_Hits = Doubles + Triples + HR,\n         Singles = H - Extra_Base_Hits,\n         H_Percentile = percent_rank(H),\n         H_Mean = mean(H)) |&gt;\n  select(playerID, teamID, H, H_Mean, H_Percentile, everything()) \n\n# A tibble: 467 × 10\n   playerID  teamID     H H_Mean H_Percentile Doubles Triples    HR\n   &lt;chr&gt;     &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;        &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 adamsma01 WAS       70   121.       0.0536      14       0    20\n 2 difowi01  WAS       94   121.       0.223       14       7     7\n 3 doziebr01 WAS       99   121.       0.266       20       0    20\n 4 eatonad02 WAS      158   121.       0.830       25       7    15\n 5 harpebr03 WAS      137   121.       0.665       34       0    34\n 6 kendrho01 WAS      115   121.       0.436       23       1    17\n 7 rendoan01 WAS      163   121.       0.865       44       2    24\n 8 rendoan01 WAS      174   121.       0.923       44       3    34\n 9 roblevi01 WAS      139   121.       0.687       33       3    17\n10 sotoju01  WAS      121   121.       0.517       25       1    22\n# ℹ 457 more rows\n# ℹ 2 more variables: Extra_Base_Hits &lt;int&gt;, Singles &lt;int&gt;\n\n\n\nUseful, but what if we want to show the mean by team? Easy to do in dplyr using group_by()!\nIf we add group_by() in our chain, any summary statistics created will honor those groups (ungroup() exists if you want to remove a grouping).\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  group_by(teamID)\n\n# A tibble: 467 × 6\n# Groups:   teamID [30]\n   playerID  teamID     H Doubles Triples    HR\n   &lt;chr&gt;     &lt;fct&gt;  &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 adamsma01 WAS       70      14       0    20\n 2 difowi01  WAS       94      14       7     7\n 3 doziebr01 WAS       99      20       0    20\n 4 eatonad02 WAS      158      25       7    15\n 5 harpebr03 WAS      137      34       0    34\n 6 kendrho01 WAS      115      23       1    17\n 7 rendoan01 WAS      163      44       2    24\n 8 rendoan01 WAS      174      44       3    34\n 9 roblevi01 WAS      139      33       3    17\n10 sotoju01  WAS      121      25       1    22\n# ℹ 457 more rows\n\n\n\nNotice there is now an additional attribute associated with this tibble!\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  group_by(teamID) |&gt;\n  attributes()\n\n$class\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n[235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n[253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n[271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n[289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n[307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n[325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n[343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n[361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n[379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n[397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n[415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n[433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n[451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n\n$names\n[1] \"playerID\" \"teamID\"   \"H\"        \"Doubles\"  \"Triples\"  \"HR\"      \n\n$groups\n# A tibble: 30 × 2\n   teamID       .rows\n   &lt;fct&gt;  &lt;list&lt;int&gt;&gt;\n 1 ARI           [14]\n 2 ATL           [17]\n 3 BAL           [12]\n 4 BOS           [16]\n 5 CHA           [14]\n 6 CHN           [19]\n 7 CIN           [17]\n 8 CLE           [16]\n 9 COL           [17]\n10 DET           [11]\n# ℹ 20 more rows\n\n\n\nLet’s find our mean relative to each team using a group_by() in our chain (this finds the percentile by teamID as well).\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  group_by(teamID) |&gt;\n  mutate(Extra_Base_Hits = Doubles + Triples + HR,\n         Singles = H - Extra_Base_Hits,\n         H_Percentile = percent_rank(H),\n         H_Mean = mean(H)) |&gt;\n  select(playerID, teamID, H, H_Mean, H_Percentile, everything()) |&gt;\n  print(n = 50)\n\n# A tibble: 467 × 10\n# Groups:   teamID [30]\n   playerID  teamID     H H_Mean H_Percentile Doubles Triples    HR\n   &lt;chr&gt;     &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;        &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 adamsma01 WAS       70   131.       0           14       0    20\n 2 difowi01  WAS       94   131.       0.154       14       7     7\n 3 doziebr01 WAS       99   131.       0.231       20       0    20\n 4 eatonad02 WAS      158   131.       0.769       25       7    15\n 5 harpebr03 WAS      137   131.       0.462       34       0    34\n 6 kendrho01 WAS      115   131.       0.308       23       1    17\n 7 rendoan01 WAS      163   131.       0.846       44       2    24\n 8 rendoan01 WAS      174   131.       0.923       44       3    34\n 9 roblevi01 WAS      139   131.       0.538       33       3    17\n10 sotoju01  WAS      121   131.       0.385       25       1    22\n11 sotoju01  WAS      153   131.       0.615       32       5    34\n12 taylomi02 WAS       80   131.       0.0769      22       3     6\n13 turnetr01 WAS      180   131.       1           27       6    19\n14 turnetr01 WAS      155   131.       0.692       37       5    19\n15 diazal02  TOR      111   103.       0.625       26       0    18\n16 drurybr01 TOR       91   103.       0.312       21       1    15\n17 galvifr01 TOR      120   103.       0.75        24       1    18\n18 grandcu01 TOR       74   103.       0.0625      21       1    11\n19 grichra01 TOR      104   103.       0.5         32       1    25\n20 grichra01 TOR      136   103.       1           29       5    31\n21 guerrvl02 TOR      126   103.       0.875       26       2    15\n22 hernate01 TOR      114   103.       0.688       29       7    22\n23 hernate01 TOR       96   103.       0.375       19       2    26\n24 janseda01 TOR       72   103.       0           12       1    13\n25 moralke01 TOR      103   103.       0.438       15       0    21\n26 pillake01 TOR      129   103.       0.938       40       2    15\n27 smoakju01 TOR      122   103.       0.812       34       0    25\n28 smoakju01 TOR       86   103.       0.25        16       0    22\n29 solarya01 TOR      106   103.       0.562       20       0    17\n30 tellero01 TOR       84   103.       0.188       19       0    21\n31 travide01 TOR       83   103.       0.125       14       3    11\n32 andruel01 TEX      165   113.       1           27       4    12\n33 beltrad01 TEX      118   113.       0.562       23       1    15\n34 chiriro01 TEX       80   113.       0.125       15       1    18\n35 choosh01  TEX      148   113.       0.875       30       1    21\n36 choosh01  TEX      149   113.       0.938       31       2    24\n37 deshide02 TEX       72   113.       0           14       1     2\n38 deshide02 TEX       89   113.       0.188       15       4     4\n39 forsylo01 TEX       72   113.       0           17       1     7\n40 gallojo01 TEX      103   113.       0.375       24       1    40\n41 guzmaro01 TEX       91   113.       0.25        18       2    16\n42 kineris01 TEX       93   113.       0.312       18       2     4\n43 mazarno01 TEX      126   113.       0.688       25       1    20\n44 mazarno01 TEX      115   113.       0.5         27       1    19\n45 odorro01  TEX      120   113.       0.625       23       2    18\n46 odorro01  TEX      107   113.       0.438       30       1    30\n47 profaju01 TEX      133   113.       0.75        35       6    20\n48 santada01 TEX      134   113.       0.812       23       6    28\n49 adamewi01 TBA      135   130.       0.4         25       1    20\n50 choiji01  TBA      107   130.       0.2         20       2    19\n# ℹ 417 more rows\n# ℹ 2 more variables: Extra_Base_Hits &lt;int&gt;, Singles &lt;int&gt;\n\n\n\nWe can get a secondary grouping too! Let’s group by year as well\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, yearID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  group_by(teamID, yearID) |&gt;\n  mutate(Extra_Base_Hits = Doubles + Triples + HR,\n         Singles = H - Extra_Base_Hits,\n         H_Percentile = percent_rank(H),\n         H_Mean = mean(H)) |&gt;\n  select(playerID, teamID, yearID, H, H_Mean, H_Percentile, everything())\n\n# A tibble: 467 × 11\n# Groups:   teamID, yearID [60]\n   playerID  teamID yearID     H H_Mean H_Percentile Doubles Triples    HR\n   &lt;chr&gt;     &lt;fct&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;        &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 adamsma01 WAS      2019    70   133.        0          14       0    20\n 2 difowi01  WAS      2018    94   129.        0.2        14       7     7\n 3 doziebr01 WAS      2019    99   133.        0.143      20       0    20\n 4 eatonad02 WAS      2019   158   133.        0.857      25       7    15\n 5 harpebr03 WAS      2018   137   129.        0.6        34       0    34\n 6 kendrho01 WAS      2019   115   133.        0.286      23       1    17\n 7 rendoan01 WAS      2018   163   129.        0.8        44       2    24\n 8 rendoan01 WAS      2019   174   133.        1          44       3    34\n 9 roblevi01 WAS      2019   139   133.        0.429      33       3    17\n10 sotoju01  WAS      2018   121   129.        0.4        25       1    22\n# ℹ 457 more rows\n# ℹ 2 more variables: Extra_Base_Hits &lt;int&gt;, Singles &lt;int&gt;\n\n\nWe are really able to do a lot quickly with these functions! Nice. One other commonly used function in mutate() is ifelse() or if_else() (the tidyverse version with slightly more restrictive functionality).\n\nLet’s add a new variable that compares a number of hits to the mean. If it is more than the mean we’ll say “Great”, if it is less than the mean we’ll say “Needs some work”.\n\nRecall ifelse() takes in a vector of conditions as the first argument. The second argument is what to do when TRUE and the third what to do when FALSE.\n\n\n\nbatting_tbl |&gt;\n  filter(G &gt; 100, yearID %in% c(2018, 2019, 2020)) |&gt;\n  arrange(desc(teamID), playerID) |&gt;\n  select(playerID, yearID, teamID, H:HR) |&gt;\n  rename(\"Doubles\" = \"X2B\", \"Triples\" = \"X3B\") |&gt;\n  group_by(teamID, yearID) |&gt;\n  mutate(Extra_Base_Hits = Doubles + Triples + HR,\n         Singles = H - Extra_Base_Hits,\n         H_Percentile = percent_rank(H),\n         H_Mean = mean(H),\n         Status = ifelse(H &gt; H_Mean, \n                         \"Great\", \n                         \"Needs some work\")) |&gt;\n  select(playerID, teamID, yearID, H, H_Mean, Status, H_Percentile, everything())\n\n# A tibble: 467 × 12\n# Groups:   teamID, yearID [60]\n   playerID teamID yearID     H H_Mean Status H_Percentile Doubles Triples    HR\n   &lt;chr&gt;    &lt;fct&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n 1 adamsma… WAS      2019    70   133. Needs…        0          14       0    20\n 2 difowi01 WAS      2018    94   129. Needs…        0.2        14       7     7\n 3 doziebr… WAS      2019    99   133. Needs…        0.143      20       0    20\n 4 eatonad… WAS      2019   158   133. Great         0.857      25       7    15\n 5 harpebr… WAS      2018   137   129. Great         0.6        34       0    34\n 6 kendrho… WAS      2019   115   133. Needs…        0.286      23       1    17\n 7 rendoan… WAS      2018   163   129. Great         0.8        44       2    24\n 8 rendoan… WAS      2019   174   133. Great         1          44       3    34\n 9 roblevi… WAS      2019   139   133. Great         0.429      33       3    17\n10 sotoju01 WAS      2018   121   129. Needs…        0.4        25       1    22\n# ℹ 457 more rows\n# ℹ 2 more variables: Extra_Base_Hits &lt;int&gt;, Singles &lt;int&gt;\n\n\n\n\nQuick R Video\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\nRecap!\ndplyr gives us a ton of functionality for doing common data manipulations\n\nas_tibble() - coerce a data frame to a tibble\nfilter() - subset rows\narrange() - reorder rows\nselect() - subset/reorder columns\nrename() - rename columns\nmutate() - add new variables to the tibble\n\nThe functionality of selecting columns described in the help for select() can be used in many places across the tidyverse and the functions group_by() and ifelse() are really useful as well!\n\ndplyr Cheat Sheet (PDF version on the right hand side of the page)\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Manipulating Data with `dplyr`"
    ]
  },
  {
    "objectID": "Notes/21-Connecting_to_Databases.html",
    "href": "Notes/21-Connecting_to_Databases.html",
    "title": "Databases and Basic SQL",
    "section": "",
    "text": "We’ve seen the use of flat files to store data (delimited data files). These files often have a file extension of .csv, .txt, or .dat.\nHowever, we often have multiple data sources around one problem. For instance, our Lahman package we’ve been using has information about Major League Baseball players’ batting, pitching, and personal information.\nThere are many data tables (essentially each could be a data frame). Below is information about the structure of these data tables and how they relate.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Databases and Basic SQL"
    ]
  },
  {
    "objectID": "Notes/21-Connecting_to_Databases.html#database---organized-collection-of-data",
    "href": "Notes/21-Connecting_to_Databases.html#database---organized-collection-of-data",
    "title": "Databases and Basic SQL",
    "section": "Database - Organized Collection of Data",
    "text": "Database - Organized Collection of Data\nFor most standard databases, we usually think of a bunch of 2D tables that are related by keys. We might refer to this as a relational database.\nConsider the Lahman diagram:\n\n\n\n\n\n\n\n\n\nEach rectangle represents one data table. We can see that the playerID variable is common across many of these tables. This is a variable that relates the tables to each other. A common task is then to combine separate data tables in order to do an analysis.\nCurrently, the Lahman data sets are not in a data base. They are simply stored in our package. We can put them into a database for some practice!\n\nDatabases Systems\nThere are many common types of relational databases management systems (RDBMS). Some of the more popular database types are:\n\nOracle\nMySQL\nSQL Server\nPostgreSQL\nSQLite\nAzure SQL\n\nMost of these RDBMS have their own Structured Query Language (SQL) that provides a language for getting information from tables in the database. The SQL languages have the same core functionality and syntax that is vital to learn as a data scientist!\n\n\nActions on Databases\nThere are a few common actions we often want to perform on a database (CRUD actions):\n\nCreate data (add rows or a table to the database)\nRead data (return data from a table in the database)\nUpdate data\nDelete data\nPlus: Provide access control, monitoring, tuning, and backup/recovery\n\nThe SQL code allows us to do these operations. Luckily, the logic for doing the querying is shared with the actions we do in dplyr! We’ll be able to query databases using dplyr commands and compare that to the equivalent SQL commands. The big idea is to get the logic down. Once you have that, it is just learning the syntax.\n\n\nDatabases in R: Connecting\nThe first step in dealing with a database is to connect yourR session to it so you can do your CRUD actions. To connect we use the DBI package. The dbConnect() function allows us to connect to a database. Here is some generic code for using that function:\n\nlibrary(DBI)\ncon &lt;- dbConnect(data_base_type_goes_here_usually_requires_a_package, \n  host = \"hostname.website\",\n  user = \"username\",\n  password = rstudioapi::askForPassword(\"DB password\")\n)\n\nThis code tellsR where the connection exists (host) and, if you need to login to gain access, the way in which you could specify your username and password.\nThe first argument specifies the type of database you are connecting to. Most commonly you’ll need to download an appropriate package and put something like the following:\n\nRSQLite::SQLite() for RSQLite\n\nRMySQL::MySQL() for RMySQL\n\nRPostgreSQL::PostgreSQL() for RPostgreSQL\n\nodbc::odbc() for Open Database Connectivity\n\nbigrquery::bigquery() for google’s bigQuery\n\n\n\nDatabases in R: Querying a Table\nOnce connect, you can use tbl() to reference a table in the database. Notice our first argument is the connection we made above.\n\nnew_data &lt;- tbl(con, \"name_of_table\")\n\nWe can then use SQL code or dplyr code (which actually calls code from a package called dbplyr) to query things.\n\n\nDatabases in R: Ending Your Connection\nWhen done working, it is good practice to disconnect from the database via the dbDisconnect() function.\n\ndbDisconnect(con)\n\n\n\nDatabases Example\nA Lahman RSQLite exists at https://www4.stat.ncsu.edu/~online/datasets/lahman.db. We’ll do some practice with this! Make sure you install the RSQLite package if you don’t have that already.\nYou’ll need to download the .db file and place it in a place you know (perhaps your project directory or the directory your .qmd file lives in.)\nThen you can run this code to connect to the database!\n\nlibrary(DBI)\ncon &lt;- dbConnect(RSQLite::SQLite(), \"data/lahman.db\")\n\nFirst, let’s list out the tables with DBI::dbListTables()\n\ndbListTables(con)\n\n [1] \"AllstarFull\"         \"Appearances\"         \"AwardsManagers\"     \n [4] \"AwardsPlayers\"       \"AwardsShareManagers\" \"AwardsSharePlayers\" \n [7] \"Batting\"             \"BattingPost\"         \"CollegePlaying\"     \n[10] \"Fielding\"            \"FieldingOF\"          \"FieldingOFsplit\"    \n[13] \"FieldingPost\"        \"HallOfFame\"          \"HomeGames\"          \n[16] \"LahmanData\"          \"Managers\"            \"ManagersHalf\"       \n[19] \"Parks\"               \"People\"              \"Pitching\"           \n[22] \"PitchingPost\"        \"Salaries\"            \"Schools\"            \n[25] \"SeriesPost\"          \"Teams\"               \"TeamsFranchises\"    \n[28] \"TeamsHalf\"           \"battingLabels\"       \"fieldingLabels\"     \n[31] \"pitchingLabels\"     \n\n\nGreat, now we can access one of these tables with the dplyr::tbl() function. From there, we can use all our usual dplyr code in place of SQL syntax!\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ntbl(con, \"Pitching\")\n\n# Source:   table&lt;`Pitching`&gt; [?? x 30]\n# Database: sqlite 3.47.1 [C:\\Users\\esmeyer2\\Documents\\repos\\ST-558-Data-Science-for-Statisticians\\Notes\\data\\lahman.db]\n   playerID  yearID stint teamID lgID      W     L     G    GS    CG   SHO    SV\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 bechtge01   1871     1 PH1    NA        1     2     3     3     2     0     0\n 2 brainas01   1871     1 WS3    NA       12    15    30    30    30     0     0\n 3 fergubo01   1871     1 NY2    NA        0     0     1     0     0     0     0\n 4 fishech01   1871     1 RC1    NA        4    16    24    24    22     1     0\n 5 fleetfr01   1871     1 NY2    NA        0     1     1     1     1     0     0\n 6 flowedi01   1871     1 TRO    NA        0     0     1     0     0     0     0\n 7 mackde01    1871     1 RC1    NA        0     1     3     1     1     0     0\n 8 mathebo01   1871     1 FW1    NA        6    11    19    19    19     1     0\n 9 mcbridi01   1871     1 PH1    NA       18     5    25    25    25     0     0\n10 mcmuljo01   1871     1 TRO    NA       12    15    29    29    28     0     0\n# ℹ more rows\n# ℹ 18 more variables: IPouts &lt;int&gt;, H &lt;int&gt;, ER &lt;int&gt;, HR &lt;int&gt;, BB &lt;int&gt;,\n#   SO &lt;int&gt;, BAOpp &lt;dbl&gt;, ERA &lt;dbl&gt;, IBB &lt;int&gt;, WP &lt;int&gt;, HBP &lt;int&gt;, BK &lt;int&gt;,\n#   BFP &lt;int&gt;, GF &lt;int&gt;, R &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\ntbl(con, \"Pitching\") |&gt;\n  select(ends_with(\"ID\")) |&gt;\n  filter(yearID == 2010) \n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.47.1 [C:\\Users\\esmeyer2\\Documents\\repos\\ST-558-Data-Science-for-Statisticians\\Notes\\data\\lahman.db]\n   playerID  yearID teamID lgID \n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 aardsda01   2010 SEA    AL   \n 2 abadfe01    2010 HOU    NL   \n 3 accarje01   2010 TOR    AL   \n 4 aceveal01   2010 NYA    AL   \n 5 acostma01   2010 NYN    NL   \n 6 adamsmi03   2010 SDN    NL   \n 7 affelje01   2010 SFN    NL   \n 8 albaljo01   2010 NYA    AL   \n 9 alberma01   2010 BAL    AL   \n10 ambrihe01   2010 CLE    AL   \n# ℹ more rows\n\n\nNotice the number of rows isn’t actually calcuated here! This is called lazy evaluation. Until we store the result in an object or do some calculation that requires all of the rows, it won’t do the computation.\nHow dplry works with a database\n\nIt never pulls data intoR unless you explicitly ask for it\nIt delays doing any work until the last possible moment - it collects together everything you want to do and then sends it to the database in one step (you can add collect() if you want all the data anyway)\n\n\ntbl(con, \"Pitching\") |&gt;\n  select(ends_with(\"ID\")) |&gt;\n  filter(yearID == 2010) |&gt;\n  collect()\n\n# A tibble: 684 × 4\n   playerID  yearID teamID lgID \n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 aardsda01   2010 SEA    AL   \n 2 abadfe01    2010 HOU    NL   \n 3 accarje01   2010 TOR    AL   \n 4 aceveal01   2010 NYA    AL   \n 5 acostma01   2010 NYN    NL   \n 6 adamsmi03   2010 SDN    NL   \n 7 affelje01   2010 SFN    NL   \n 8 albaljo01   2010 NYA    AL   \n 9 alberma01   2010 BAL    AL   \n10 ambrihe01   2010 CLE    AL   \n# ℹ 674 more rows\n\n\nWe can actually get out some SQL code from our dplyr code if we use show_query()\n\ntbl(con, \"Pitching\") |&gt;\n  select(ends_with(\"ID\")) |&gt;\n  filter(yearID == 2010) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT `playerID`, `yearID`, `teamID`, `lgID`\nFROM `Pitching`\nWHERE (`yearID` = 2010.0)\n\n\nThat’s cool! If you have the logic of dplyr down you can just use that but also learn SQL syntax along the way!\nYou can actually write straight SQL code as well (if you know that):\n\ntbl(con, sql(\n\"SELECT `playerID`, `yearID`, `teamID`, `lgID`\nFROM `Pitching`\nWHERE (`yearID` = 2010.0)\")\n)\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.47.1 [C:\\Users\\esmeyer2\\Documents\\repos\\ST-558-Data-Science-for-Statisticians\\Notes\\data\\lahman.db]\n   playerID  yearID teamID lgID \n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 aardsda01   2010 SEA    AL   \n 2 abadfe01    2010 HOU    NL   \n 3 accarje01   2010 TOR    AL   \n 4 aceveal01   2010 NYA    AL   \n 5 acostma01   2010 NYN    NL   \n 6 adamsmi03   2010 SDN    NL   \n 7 affelje01   2010 SFN    NL   \n 8 albaljo01   2010 NYA    AL   \n 9 alberma01   2010 BAL    AL   \n10 ambrihe01   2010 CLE    AL   \n# ℹ more rows\n\n\nThere are a ton of online tutorials to learn SQL. It is highly recommended if you are looking for a job!\nWe’ll cover the basics of Joins (methods to combine more than one table (data frame)) shortly.\nWe should disconnect from our database now that we are done!\n\ndbDisconnect(con)\n\n\n\nQuick R Video\nPlease pop this video out and watch it in the full panopto player!\n\n\nLink to repo with files from video\n\n\n\nRecap!\n\nDatabases are commonly used to store lots of data\n\nThe data, the DBMS, and the applications associated with them are often simply called a database\n\nCreate data, Read data, Update data, Delete data (CRUD)\nDBI package has functionality to connect to many types of databases in R\ntbl() and common dplyr functions work for querying\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Databases and Basic SQL"
    ]
  },
  {
    "objectID": "Notes/23_5-Week3_4_em.html",
    "href": "Notes/23_5-Week3_4_em.html",
    "title": "Week 4 Overview",
    "section": "",
    "text": "This wraps up the content for week 3. Now we require some practice! You should head back to our Moodle site to check out your assessment for this week.\nWeek 4 we take on exploring and finding relationships in our data. Anytime we read data into R (or another language), we should do some basic data visualization and get to know our data. These are the basics steps to an exploratory data analysis (EDA). This week we’ll develop the skills to do this!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Week 4 Overview"
    ]
  },
  {
    "objectID": "Notes/23_5-Week3_4_em.html#week-4-additional-readingslearning-materials",
    "href": "Notes/23_5-Week3_4_em.html#week-4-additional-readingslearning-materials",
    "title": "Week 4 Overview",
    "section": "Week 4 Additional Readings/Learning Materials",
    "text": "Week 4 Additional Readings/Learning Materials\n\nEDA\n\nChapters 1, 9, 10, and 11 of R 4 Data Science\n(Optional) Useful site with EDA process example\n(Optional) Chapters 4 and 5 of Introduction to Modern Statistics\n\n\n\nFactors and missing values\n\nChapters 16 and 18 of R 4 Data Science",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Week 4 Overview"
    ]
  },
  {
    "objectID": "Notes/23_5-Week3_4_em.html#week-4-learning-objectives",
    "href": "Notes/23_5-Week3_4_em.html#week-4-learning-objectives",
    "title": "Week 4 Overview",
    "section": "Week 4 Learning Objectives",
    "text": "Week 4 Learning Objectives\nUpon completion of this week, students will be able to: (CO is the corresponding course learning objective this helps build toward)\n\nSummarizing Data\n\n   utilize R to construct basic numeric summaries such as the mean, median, standard deviation, variance, quantiles, correlation, and contingency tables (CO 5)\na.    interpret and pull out relevant pieces of n-way contingency tables created using the table function in R b.    calculate common summary statistics for each level of a categorical variable (or combinations of levels of multiple categorical variables)\n   create graphical summaries using base R and the ggplot2 package (CO 5)\na.    explain the idea behind the way plots are created using the ggplot2 package in R b.    customize ggplot graphical summaries in R including, but not limited to, changes to axes and title labels, the type of plot, the appearance of points, the addition of lines, the color, shape, and size of points and lines, and the addition of a legend c.    create multiple plots using the faceting capabilities of the ggplot2 package d.    describe and create the most commonly used plots for categorical data\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Week 4 Overview"
    ]
  },
  {
    "objectID": "Notes/25-Summarizing_Categorical_Variables.html",
    "href": "Notes/25-Summarizing_Categorical_Variables.html",
    "title": "Creating Contingency Tables",
    "section": "",
    "text": "Now that we know how to get our raw data into R, we are ready to do the fun stuff - investigating our data!\nWe discussed the main steps of an EDA and covered the most common data validation and basic manipulations for the data. The next few sets of notes dive into how to find summarize our data. Recall, how we summarize our data depends on the type of data we have!\nIn either situation, we want to describe each variable’s distribution, perhaps comparing across different subgroups!\nLet’s start with summaries of strictly categorical data (or numeric variables with only a few values).",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Creating Contingency Tables"
    ]
  },
  {
    "objectID": "Notes/25-Summarizing_Categorical_Variables.html#categorical-data-summaries",
    "href": "Notes/25-Summarizing_Categorical_Variables.html#categorical-data-summaries",
    "title": "Creating Contingency Tables",
    "section": "Categorical Data Summaries",
    "text": "Categorical Data Summaries\nTo summarize categorical variables numerically, we use contingency tables.\nTo do so visually, we use bar plots.\nFirst, let’s read in the appendicitis data from the previous lecture.\n\nlibrary(tidyverse)\nlibrary(readxl)\napp_data &lt;- read_excel(\"data/app_data.xlsx\", sheet = 1)\napp_data &lt;- app_data |&gt;\n  mutate(BMI = as.numeric(BMI),\n         US_Number = as.character(US_Number),\n         SexF = factor(Sex, levels = c(\"female\", \"male\"), labels = c(\"Female\", \"Male\")),\n         DiagnosisF = as.factor(Diagnosis),\n         SeverityF = as.factor(Severity))\napp_data\n\n# A tibble: 782 × 61\n     Age   BMI Sex    Height Weight Length_of_Stay Management   Severity     \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        \n 1 12.7   16.9 female    148   37                3 conservative uncomplicated\n 2 14.1   31.9 male      147   69.5              2 conservative uncomplicated\n 3 14.1   23.3 female    163   62                4 conservative uncomplicated\n 4 16.4   20.6 female    165   56                3 conservative uncomplicated\n 5 11.1   16.9 female    163   45                3 conservative uncomplicated\n 6 11.0   30.7 male      121   45                3 conservative uncomplicated\n 7  8.98  19.4 female    140   38.5              3 conservative uncomplicated\n 8  7.06  NA   female     NA   21.5              2 conservative uncomplicated\n 9  7.9   15.7 male      131   26.7              3 conservative uncomplicated\n10 14.3   14.9 male      174   45.5              3 conservative uncomplicated\n# ℹ 772 more rows\n# ℹ 53 more variables: Diagnosis_Presumptive &lt;chr&gt;, Diagnosis &lt;chr&gt;,\n#   Alvarado_Score &lt;dbl&gt;, Paedriatic_Appendicitis_Score &lt;dbl&gt;,\n#   Appendix_on_US &lt;chr&gt;, Appendix_Diameter &lt;dbl&gt;, Migratory_Pain &lt;chr&gt;,\n#   Lower_Right_Abd_Pain &lt;chr&gt;, Contralateral_Rebound_Tenderness &lt;chr&gt;,\n#   Coughing_Pain &lt;chr&gt;, Nausea &lt;chr&gt;, Loss_of_Appetite &lt;chr&gt;,\n#   Body_Temperature &lt;dbl&gt;, WBC_Count &lt;dbl&gt;, Neutrophil_Percentage &lt;dbl&gt;, …\n\n\nLet’s go!\n\nContingency Tables\nWe can use Base R or the tidyverse.\n\nVia Base R\nHonestly, the easiest way to make contingency tables is through BaseR’s table() function.\nFrom the help\ntable(...,\n      exclude = if (useNA == \"no\") c(NA, NaN),\n      useNA = c(\"no\", \"ifany\", \"always\"),\n      dnn = list.names(...), deparse.level = 1)\nwhere ... is\n\none or more objects which can be interpreted as factors (including numbers or character strings), or a list (such as a data frame) whose components can be so interpreted.\n\nOk, so we can just pass it the vectors we want or we could pass it a data frame (which remember, is just a list of equal length vectors!).\nLet’s create some contingency tables for the SexF, DiagnosisF, and SeverityF variables.\n\ntable(app_data$SexF)\n\n\nFemale   Male \n   377    403 \n\n\nWe can include NA if we want to via the useNA argument:\n\ntable(app_data$SexF, useNA = \"always\")\n\n\nFemale   Male   &lt;NA&gt; \n   377    403      2 \n\n\nWe can create a two-way table (two-way for two variables) by adding the second variable in:\n\ntable(app_data$SexF, app_data$DiagnosisF)\n\n        \n         appendicitis no appendicitis\n  Female          200             176\n  Male            262             141\n\n\nWhat is returned from when we create a table? An array! (homogenous data structure - 1D array is a vector, 2D is a matrix)\nThat means we can subset them if want to! Let’s return the conditional one-way table of Sex based on only those that had appendicitis:\n\ntwo_way_sex_diag &lt;- table(app_data$SexF, app_data$DiagnosisF)\ntwo_way_sex_diag[,1]\n\nFemale   Male \n   200    262 \n\n\nNice! Things do get more complicated if we add in a third variable as it is tough to display that info compactly.\n\ntable(app_data$SexF, app_data$DiagnosisF, app_data$SeverityF)\n\n, ,  = complicated\n\n        \n         appendicitis no appendicitis\n  Female           55               1\n  Male             63               0\n\n, ,  = uncomplicated\n\n        \n         appendicitis no appendicitis\n  Female          145             175\n  Male            199             141\n\n\nIf you look at the output you see , , = complicated. This is R hinting at how to access this 3D array!\nWe can return the conditional two-way table of Sex and Diagnosis for only those with an uncomplicated situation:\n\nthree_way &lt;- table(app_data$SexF, app_data$DiagnosisF, app_data$SeverityF)\nthree_way[, , \"uncomplicated\"]\n\n        \n         appendicitis no appendicitis\n  Female          145             175\n  Male            199             141\n\n#or\nthree_way[, , 2]\n\n        \n         appendicitis no appendicitis\n  Female          145             175\n  Male            199             141\n\n\nWe can also get a one-way table conditional on two of the variables. Here is the one-way table for sex for only those with an uncomplicated situation and no appendicitis:\n\nthree_way[, 2, 2]\n\nFemale   Male \n   175    141 \n\n\nLastly, just note that you can supply a data frame instead of the individual vectors.\n\ntable(app_data[, c(\"SexF\", \"DiagnosisF\")])\n\n        DiagnosisF\nSexF     appendicitis no appendicitis\n  Female          200             176\n  Male            262             141\n\n\n\n\nVia the tidyverse\nOk, great. But we might want to stay in the tidyverse. We can use the dplyr::summarize() function to compute summaries on a tibble. This generally outputs a tibble with fewer rows than the original (as we are summarizing the variables to view them in a more compact form). We often use group_by() to set a grouping variable. Any summary done will respect the groupings!\nAny of the common summarization functions you can think of are likely permissible in summarize(). The one for counting values is simply n(). Let’s recreate all of our above tables under the tidyverse method.\nOne-way table:\n\napp_data |&gt;\n  group_by(SexF) |&gt;\n  summarize(count = n())\n\n# A tibble: 3 × 2\n  SexF   count\n  &lt;fct&gt;  &lt;int&gt;\n1 Female   377\n2 Male     403\n3 &lt;NA&gt;       2\n\n\nNotice that NA values are included by default (probably a good thing). We can remove those with tidyr::drop_na().\n\napp_data |&gt;\n  drop_na(SexF) |&gt;\n  group_by(SexF) |&gt;\n  summarize(count = n())\n\n# A tibble: 2 × 2\n  SexF   count\n  &lt;fct&gt;  &lt;int&gt;\n1 Female   377\n2 Male     403\n\n\nTwo-way table: Simply add another grouping variable. The summarize() function respects these groups when counting!\n\napp_data |&gt;\n  drop_na(SexF, DiagnosisF) |&gt;\n  group_by(SexF, DiagnosisF) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'SexF'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   SexF [2]\n  SexF   DiagnosisF      count\n  &lt;fct&gt;  &lt;fct&gt;           &lt;int&gt;\n1 Female appendicitis      200\n2 Female no appendicitis   176\n3 Male   appendicitis      262\n4 Male   no appendicitis   141\n\n\nNice. But that isn’t in the best way for viewing (i.e. a wider format would be more compact for displaying). Let’s use tidyr::pivot_wider() to fix that!\n\napp_data |&gt;\n  drop_na(SexF, DiagnosisF) |&gt;\n  group_by(SexF, DiagnosisF) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = DiagnosisF, values_from = count)\n\n`summarise()` has grouped output by 'SexF'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   SexF [2]\n  SexF   appendicitis `no appendicitis`\n  &lt;fct&gt;         &lt;int&gt;             &lt;int&gt;\n1 Female          200               176\n2 Male            262               141\n\n\nThree-way table: Again, just add more grouping variables!\n\napp_data |&gt;\n  drop_na(SexF, DiagnosisF, SeverityF) |&gt;\n  group_by(SexF, DiagnosisF, SeverityF) |&gt;\n  summarize(count = n())\n\n`summarise()` has grouped output by 'SexF', 'DiagnosisF'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 7 × 4\n# Groups:   SexF, DiagnosisF [4]\n  SexF   DiagnosisF      SeverityF     count\n  &lt;fct&gt;  &lt;fct&gt;           &lt;fct&gt;         &lt;int&gt;\n1 Female appendicitis    complicated      55\n2 Female appendicitis    uncomplicated   145\n3 Female no appendicitis complicated       1\n4 Female no appendicitis uncomplicated   175\n5 Male   appendicitis    complicated      63\n6 Male   appendicitis    uncomplicated   199\n7 Male   no appendicitis uncomplicated   141\n\n\nWe can also pivot this, although there is no great way to get all the info there. We’ll just move the severity variable across the top.\n\napp_data |&gt;\n  drop_na(SexF, DiagnosisF, SeverityF) |&gt;\n  group_by(SexF, DiagnosisF, SeverityF) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = SeverityF, values_from = count)\n\n`summarise()` has grouped output by 'SexF', 'DiagnosisF'. You can override\nusing the `.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   SexF, DiagnosisF [4]\n  SexF   DiagnosisF      complicated uncomplicated\n  &lt;fct&gt;  &lt;fct&gt;                 &lt;int&gt;         &lt;int&gt;\n1 Female appendicitis             55           145\n2 Female no appendicitis           1           175\n3 Male   appendicitis             63           199\n4 Male   no appendicitis          NA           141\n\n\n\n\n\nRecap!\nContingency tables summarize the distribution of one or more categorical variables. We can create them using\n\ntable() - returns an array of counts\ngroup_by() along with summarize() and the n() function\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Creating Contingency Tables"
    ]
  },
  {
    "objectID": "Notes/27-Numerical_Variable_Summaries.html",
    "href": "Notes/27-Numerical_Variable_Summaries.html",
    "title": "Numerical Variable Summaries",
    "section": "",
    "text": "We now know how to summarize categorical data and we’ve learned the basics of ggplot2. Now we’re ready to investigate how to summarize numeric variables. Recall:\nAs before, our goal is to describe the distribution of the variable. We talked about this briefly:\nFirst, let’s read in the appendicitis data from the previous lecture.\nlibrary(tidyverse)\nlibrary(readxl)\napp_data &lt;- read_excel(\"data/app_data.xlsx\", sheet = 1)\napp_data &lt;- app_data |&gt;\n  mutate(BMI = as.numeric(BMI),\n         US_Number = as.character(US_Number),\n         SexF = factor(Sex, levels = c(\"female\", \"male\"), labels = c(\"Female\", \"Male\")),\n         DiagnosisF = as.factor(Diagnosis),\n         SeverityF = as.factor(Severity))\napp_data\n\n# A tibble: 782 × 61\n     Age   BMI Sex    Height Weight Length_of_Stay Management   Severity     \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        \n 1 12.7   16.9 female    148   37                3 conservative uncomplicated\n 2 14.1   31.9 male      147   69.5              2 conservative uncomplicated\n 3 14.1   23.3 female    163   62                4 conservative uncomplicated\n 4 16.4   20.6 female    165   56                3 conservative uncomplicated\n 5 11.1   16.9 female    163   45                3 conservative uncomplicated\n 6 11.0   30.7 male      121   45                3 conservative uncomplicated\n 7  8.98  19.4 female    140   38.5              3 conservative uncomplicated\n 8  7.06  NA   female     NA   21.5              2 conservative uncomplicated\n 9  7.9   15.7 male      131   26.7              3 conservative uncomplicated\n10 14.3   14.9 male      174   45.5              3 conservative uncomplicated\n# ℹ 772 more rows\n# ℹ 53 more variables: Diagnosis_Presumptive &lt;chr&gt;, Diagnosis &lt;chr&gt;,\n#   Alvarado_Score &lt;dbl&gt;, Paedriatic_Appendicitis_Score &lt;dbl&gt;,\n#   Appendix_on_US &lt;chr&gt;, Appendix_Diameter &lt;dbl&gt;, Migratory_Pain &lt;chr&gt;,\n#   Lower_Right_Abd_Pain &lt;chr&gt;, Contralateral_Rebound_Tenderness &lt;chr&gt;,\n#   Coughing_Pain &lt;chr&gt;, Nausea &lt;chr&gt;, Loss_of_Appetite &lt;chr&gt;,\n#   Body_Temperature &lt;dbl&gt;, WBC_Count &lt;dbl&gt;, Neutrophil_Percentage &lt;dbl&gt;, …\nLet’s dig in!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Numerical Variable Summaries"
    ]
  },
  {
    "objectID": "Notes/27-Numerical_Variable_Summaries.html#numerical-summaries",
    "href": "Notes/27-Numerical_Variable_Summaries.html#numerical-summaries",
    "title": "Numerical Variable Summaries",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\nWe’ll utilize the summarize() function along with group_by() to find most of our numerical summaries.\nAs we discussed, we can’t really describe the entire distribution with a single number so we try to summarize different aspects of the distribution. In particular, center and spread.\n\nMeasures of Center\nWe can find the mean and median via the mean() and median() function.\n\napp_data |&gt;\n  summarize(mean_BMI = mean(BMI, na.rm = TRUE), med_BMI = median(BMI, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  mean_BMI med_BMI\n     &lt;dbl&gt;   &lt;dbl&gt;\n1     18.9    18.1\n\n\nWe can try to get fancy and do it for all numeric columns. Recall we did this earlier with across() and where(is.numeric):\n\napp_data |&gt;\n  summarize(across(where(is.numeric), \n                   list(\"mean\" = mean, \"median\" = median), \n                   .names = \"{.fn}_{.col}\"))\n\n# A tibble: 1 × 34\n  mean_Age median_Age mean_BMI median_BMI mean_Height median_Height mean_Weight\n     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1       NA         NA       NA         NA          NA            NA          NA\n# ℹ 27 more variables: median_Weight &lt;dbl&gt;, mean_Length_of_Stay &lt;dbl&gt;,\n#   median_Length_of_Stay &lt;dbl&gt;, mean_Alvarado_Score &lt;dbl&gt;,\n#   median_Alvarado_Score &lt;dbl&gt;, mean_Paedriatic_Appendicitis_Score &lt;dbl&gt;,\n#   median_Paedriatic_Appendicitis_Score &lt;dbl&gt;, mean_Appendix_Diameter &lt;dbl&gt;,\n#   median_Appendix_Diameter &lt;dbl&gt;, mean_Body_Temperature &lt;dbl&gt;,\n#   median_Body_Temperature &lt;dbl&gt;, mean_WBC_Count &lt;dbl&gt;,\n#   median_WBC_Count &lt;dbl&gt;, mean_Neutrophil_Percentage &lt;dbl&gt;, …\n\n\nOh, darn. That’s right, we have missing values. We can remove those just for a particular column instead of removing all the rows (as we did with drop_na()). This is a bit more complicated but we can specify some additional arguments of the mean and median function in our named list.\n\napp_data |&gt;\n  summarize(across(where(is.numeric), \n                   list(\"mean\" = ~ mean(.x, na.rm = TRUE), \"median\" = ~ median(.x, na.rm = TRUE)), \n                   .names = \"{.fn}_{.col}\"))\n\n# A tibble: 1 × 34\n  mean_Age median_Age mean_BMI median_BMI mean_Height median_Height mean_Weight\n     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1     11.3       11.4     18.9       18.1        148.          150.        43.2\n# ℹ 27 more variables: median_Weight &lt;dbl&gt;, mean_Length_of_Stay &lt;dbl&gt;,\n#   median_Length_of_Stay &lt;dbl&gt;, mean_Alvarado_Score &lt;dbl&gt;,\n#   median_Alvarado_Score &lt;dbl&gt;, mean_Paedriatic_Appendicitis_Score &lt;dbl&gt;,\n#   median_Paedriatic_Appendicitis_Score &lt;dbl&gt;, mean_Appendix_Diameter &lt;dbl&gt;,\n#   median_Appendix_Diameter &lt;dbl&gt;, mean_Body_Temperature &lt;dbl&gt;,\n#   median_Body_Temperature &lt;dbl&gt;, mean_WBC_Count &lt;dbl&gt;,\n#   median_WBC_Count &lt;dbl&gt;, mean_Neutrophil_Percentage &lt;dbl&gt;, …\n\n\nThe ~ is a quick way to write a lambda or anonymous function. Essentially, we are inline doing something like this\n\nmy_fun &lt;- function(x) {mean(x, na.rm = TRUE)}\n\nBut a lambda function is a shorthand for this where we don’t need to give the function a name (since we aren’t planning on using it again anyway).\nOf course we want these kinds of statistics across groups so we can compare them. We saw how to do this with group_by()\n\napp_data |&gt;\n  group_by(Diagnosis, Sex) |&gt;\n  drop_na(Diagnosis, Sex) |&gt;\n  summarize(mean_BMI = mean(BMI, na.rm = TRUE), med_BMI = median(BMI, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Diagnosis'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   Diagnosis [2]\n  Diagnosis       Sex    mean_BMI med_BMI\n  &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 appendicitis    female     18.6    17.8\n2 appendicitis    male       18.3    17.5\n3 no appendicitis female     20.2    20.0\n4 no appendicitis male       18.7    17.2\n\n\nWe can do this similar thing with the fancier version too!\n\napp_data |&gt;\n  group_by(Diagnosis, Sex) |&gt;\n  drop_na(Diagnosis, Sex) |&gt;\n  summarize(across(where(is.numeric), \n                   list(\"mean\" = ~ mean(.x, na.rm = TRUE), \"median\" = ~ median(.x, na.rm = TRUE)), \n                   .names = \"{.fn}_{.col}\"))\n\n`summarise()` has grouped output by 'Diagnosis'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 36\n# Groups:   Diagnosis [2]\n  Diagnosis       Sex    mean_Age median_Age mean_BMI median_BMI mean_Height\n  &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 appendicitis    female     11.7       11.9     18.6       17.8        148.\n2 appendicitis    male       10.6       11       18.3       17.5        146.\n3 no appendicitis female     12.5       12.5     20.2       20.0        152.\n4 no appendicitis male       10.7       10.9     18.7       17.2        147.\n# ℹ 29 more variables: median_Height &lt;dbl&gt;, mean_Weight &lt;dbl&gt;,\n#   median_Weight &lt;dbl&gt;, mean_Length_of_Stay &lt;dbl&gt;,\n#   median_Length_of_Stay &lt;dbl&gt;, mean_Alvarado_Score &lt;dbl&gt;,\n#   median_Alvarado_Score &lt;dbl&gt;, mean_Paedriatic_Appendicitis_Score &lt;dbl&gt;,\n#   median_Paedriatic_Appendicitis_Score &lt;dbl&gt;, mean_Appendix_Diameter &lt;dbl&gt;,\n#   median_Appendix_Diameter &lt;dbl&gt;, mean_Body_Temperature &lt;dbl&gt;,\n#   median_Body_Temperature &lt;dbl&gt;, mean_WBC_Count &lt;dbl&gt;, …\n\n\nGreat, now we have an easy way to compare the centers of the distribution for each of these numeric variables!\n\n\nMeasures of Spread\nSame idea here but we can use the sd() and IQR() functions.\n\napp_data |&gt;\n  group_by(Diagnosis, Sex) |&gt;\n  drop_na(Diagnosis, Sex) |&gt;\n  summarize(sd_BIM = sd(BMI, na.rm = TRUE), IQR_BMI = IQR(BMI, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Diagnosis'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   Diagnosis [2]\n  Diagnosis       Sex    sd_BIM IQR_BMI\n  &lt;chr&gt;           &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 appendicitis    female   4.33    4.62\n2 appendicitis    male     4.03    5.22\n3 no appendicitis female   4.53    5.75\n4 no appendicitis male     4.60    5.07\n\n\n\n\nMeasures of Association Between Two Numeric Variables\nWe can find the linear associations between two numeric variables with cor().\n\napp_data |&gt;\n  group_by(Diagnosis, Sex) |&gt;\n  drop_na(Diagnosis, Sex) |&gt;\n  summarize(correlation = cor(BMI, Age))\n\n`summarise()` has grouped output by 'Diagnosis'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diagnosis [2]\n  Diagnosis       Sex    correlation\n  &lt;chr&gt;           &lt;chr&gt;        &lt;dbl&gt;\n1 appendicitis    female          NA\n2 appendicitis    male            NA\n3 no appendicitis female          NA\n4 no appendicitis male            NA\n\n\nOh yeah, missing values. Unfortunately, BaseR isn’t that consistent. To deal with missing values appropriately, we can look at the help.\nuse\n\nan optional character string giving a method for computing covariances in the presence of missing values. This must be (an abbreviation of) one of the strings “everything”, “all.obs”, “complete.obs”, “na.or.complete”, or “pairwise.complete.obs”.\n\n\napp_data |&gt;\n  group_by(Diagnosis, Sex) |&gt;\n  drop_na(Diagnosis, Sex) |&gt;\n  summarize(correlation = cor(BMI, Age, use = \"pairwise.complete.obs\"))\n\n`summarise()` has grouped output by 'Diagnosis'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diagnosis [2]\n  Diagnosis       Sex    correlation\n  &lt;chr&gt;           &lt;chr&gt;        &lt;dbl&gt;\n1 appendicitis    female       0.556\n2 appendicitis    male         0.462\n3 no appendicitis female       0.413\n4 no appendicitis male         0.422\n\n\nGreat - we can do all our basic numerical summaries!\n\n\nRecap!\nWe tend to describe the center and spread of a numeric variable’s distribution. Often we want to compare across groups and that can be done with group_by().\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Numerical Variable Summaries"
    ]
  },
  {
    "objectID": "Notes/28_5-Week_5_overview_em.html",
    "href": "Notes/28_5-Week_5_overview_em.html",
    "title": "Week 5 Overview",
    "section": "",
    "text": "There is no new material this week! You have an exam!\nRemember that this exam needs to be proctored. Contact your instructor asap if you have any questions.",
    "crumbs": [
      "Home",
      "Topic 2: Tidyverse & EDA",
      "Week 5 Overview"
    ]
  },
  {
    "objectID": "Notes/29-Big_Recap_Landing.html",
    "href": "Notes/29-Big_Recap_Landing.html",
    "title": "Recap and Direction!",
    "section": "",
    "text": "The video below recaps the most important points covered so far and looks ahead at what we’ll cover in the rest of the course.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Recap and Direction!"
    ]
  },
  {
    "objectID": "Notes/29-Big_Recap_Landing.html#notes",
    "href": "Notes/29-Big_Recap_Landing.html#notes",
    "title": "Recap and Direction!",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Recap and Direction!"
    ]
  },
  {
    "objectID": "Notes/31-purrr_Landing.html",
    "href": "Notes/31-purrr_Landing.html",
    "title": "purrr Package",
    "section": "",
    "text": "The video below discusses the purrr package. The functions from this package are the tidyverse version of the apply family of functions. In some ways they are easier to use and in some ways harder!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "`purrr` Package"
    ]
  },
  {
    "objectID": "Notes/31-purrr_Landing.html#notes",
    "href": "Notes/31-purrr_Landing.html#notes",
    "title": "purrr Package",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "`purrr` Package"
    ]
  },
  {
    "objectID": "Notes/23-Querying_APIs_Landing.html",
    "href": "Notes/23-Querying_APIs_Landing.html",
    "title": "Querying APIs",
    "section": "",
    "text": "The video below discusses how we can obtain data from an Application Program Interface (API). This is a common way to query and obtain data!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Querying APIs"
    ]
  },
  {
    "objectID": "Notes/23-Querying_APIs_Landing.html#notes",
    "href": "Notes/23-Querying_APIs_Landing.html#notes",
    "title": "Querying APIs",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Better Programming",
      "Querying APIs"
    ]
  },
  {
    "objectID": "Notes/34-RShiny_Tutorials_I.html",
    "href": "Notes/34-RShiny_Tutorials_I.html",
    "title": "Tutorials to Complete Part I",
    "section": "",
    "text": "Now that we have the basics of R Shiny, please head over to the “Shiny Basics” tutorials and complete the following three:\n\nWelcome to Shiny\nBuild a user interface\nAdd control widgets\n\nThen head back here for the next content!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Tutorials to Complete Part I"
    ]
  },
  {
    "objectID": "Notes/36-RShiny_Tutorials_II.html",
    "href": "Notes/36-RShiny_Tutorials_II.html",
    "title": "Tutorials to Complete Part II",
    "section": "",
    "text": "We’re now ready to head over to the “Shiny Basics” tutorials and finish the remaining four lessons:\n\nDisplay reactive output\nUse R scripts and data\nUse reactive expressions\nShare your apps\n\nThen head back here for the next content!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Tutorials to Complete Part II"
    ]
  },
  {
    "objectID": "Notes/38-RShiny_Tutorials_III.html",
    "href": "Notes/38-RShiny_Tutorials_III.html",
    "title": "Tutorials to Complete Part III",
    "section": "",
    "text": "Let’s build an app! Head over to the “Build an App” tutorial and complete the entire lesson (‘Hello Shiny!’, ‘Reactive Flow’, ‘Reactivity Essentials’, and ‘Customizing UI’.\nThen head back here for the next content!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Tutorials to Complete Part III"
    ]
  },
  {
    "objectID": "Notes/40-RShiny_UI_Layouts.html",
    "href": "Notes/40-RShiny_UI_Layouts.html",
    "title": "Flexible UI Layouts & Dashboards",
    "section": "",
    "text": "Hopefully you are seeing the usefulness of shiny apps! We build a user interface to define what the user sees and a server to run our R code. We’ve also learned how to dynamically update our UI! This lesson takes customizing the UI further by looking at how we can create our own layout and also use some packages to change the way our UI is set up.",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Flexible UI Layouts & Dashboards"
    ]
  },
  {
    "objectID": "Notes/40-RShiny_UI_Layouts.html#custom-ui-layout-with-fluidrow-and-column",
    "href": "Notes/40-RShiny_UI_Layouts.html#custom-ui-layout-with-fluidrow-and-column",
    "title": "Flexible UI Layouts & Dashboards",
    "section": "Custom UI Layout with fluidRow() and column()",
    "text": "Custom UI Layout with fluidRow() and column()\nWhen using the base shiny functions to create our UI we generally start with a fluidPage().\n\nlibrary(shiny)\nui &lt;- fluidPage()\n\nshiny uses a popular bootstrap framework for creating its dynamic content. Essentially, you can build rows and each row has a width of 12. We can create a row with fluidRow().\n\nlibrary(shiny)\nui &lt;- fluidPage(\n  fluidRow()\n)\n\nYou can create columns using the column() function. This takes a width argument and an offset (essentially a buffer)\n\nui &lt;- fluidPage(\n  fluidRow(\n    column(width = 3, offset = 1, \"content!--------------------------------------------------\"),\n    column(width = 8, \"more content!--------------------------------------------------------------------\")\n  )\n)\n\n\n\n\n\n\n\n\n\n\nAgain, the columns should sum to 12 in total width for an ‘area’! This setup can allow us to create a custom layout!\n\nlibrary(shiny)\nui &lt;- fluidPage(#using shinyUI so it will show here but you'd save this as ui instead!\n  fluidRow(\n    column(2,\"fluidRow with three columns. Width 2-------------------------------------------------------------------\"),\n    column(6,\"2nd column. Width 6------------------------------------------------------------------------------------------------------------------\"),\n    column(4,\"3rd column. Width 4-------------------------------------------------------------\")),\n  fluidRow(tags$hr()),\n  fluidRow(\n    column(6,\"2nd fluidRow with two columns, each of width 6------------------------------------------------------------------------------\"),\n    column(6,\n           fluidRow(\"The 2nd column has its own fluidRow! This row should have columns that sum to 12 again! In this case there are no columns so it defaults to fill all 12 columns\"),\n           fluidRow(\n             column(3,\"A second fluidRow below the first. Of width 3------------------------------------\"),\n             column(9,\"Width 9!--------------------------------------------------------------------------------\")\n           ))\n  )\n)",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Flexible UI Layouts & Dashboards"
    ]
  },
  {
    "objectID": "Notes/40-RShiny_UI_Layouts.html#using-tabs",
    "href": "Notes/40-RShiny_UI_Layouts.html#using-tabs",
    "title": "Flexible UI Layouts & Dashboards",
    "section": "Using Tabs",
    "text": "Using Tabs\nTo create tabs that can allow you to easily change what the user sees, we can use shiny::tabsetPanel() with shiny::tabPanel().\n\ntabsetPanel(\n  tabPanel(\"Title1\", \"contents! As usual, separate items by commas and you can put widgets, outputs, and HTML content\"),\n  tabPanel(\"Title2\", \"contents\")\n)\n\n\n\n\n\n\n\n\n\n\nOften we want a larger menu to change between pages. There are a number of other ways to do this as well. We can get a sidebar menu using the shinydashboard package! You’ll likely need to install this package. This package also comes with its own tabItems() and tabItem() functions that create a menu with tabs on it.\n\nlibrary(shinydashboard)\n\nWarning: package 'shinydashboard' was built under R version 4.4.3\n\n\n\nAttaching package: 'shinydashboard'\n\n\nThe following object is masked from 'package:graphics':\n\n    box\n\nui &lt;- dashboardPage(\n  dashboardHeader(title=\"Regression Activity\"),\n    \n    dashboardSidebar(    \n      sidebarMenu(\n        menuItem(\"Correlation Exploration\", tabName = \"correlation\", icon = icon(\"archive\")),\n        menuItem(\"Simple Linear Regression\", tabName = \"slr\", icon = icon(\"laptop\"))\n        )\n      ),\n    \n    dashboardBody(\n      tabItems(\n        tabItem(tabName = \"correlation\",\n                titlePanel(\"Correlation Exploration\"),\n                sliderInput(\"slidey\", \"My slider\", min = 0, max = 10, value = 5)\n        ),\n        tabItem(tabName = \"slr\",\n                titlePanel(\"Simple Linear Regression\"),\n                numericInput(\"number\", \"My Numeric Input\", min = 0, max = 10, value = 5)\n        )\n      )\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nshinydashboard also comes with functions similar to tabsetPanel() and tabPanel(), which I prefer to use. These functions are tabBox() and tabItem().\n\nfluidRow(\n  tabBox(\n    id = \"first_plots\",\n    tabPanel(title = \"Normal Approximation\",\n             plotOutput(\"norm_approx\")),\n    tabPanel(title = \"Map View\", \n             leaflet::leafletOutput(\"map_plot\")\n             ),\n    width = 12 #set just like column! \n    )\n)\n\n\n\n\n\n\n\n\n\n\nI like these functions because it makes it easy to do an action when a tab is selected by the user. We can use observeEvent() to look for changes in the tab id.\n\nobserveEvent(input$first_plots, {\n      if(input$first_plots == \"Map View\"){\n        ...\n      }\n})\n\n\nRecap\nWe can easily create our own UI layout with fluidPage(), fluidRow(), and column(). These columns must add to 12.\nThere are tabs that can be used from shiny or from shinydashboard (and other packages!).\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Flexible UI Layouts & Dashboards"
    ]
  },
  {
    "objectID": "Notes/42-RShiny_Debugging_Other_Useful_Things_Landing.html",
    "href": "Notes/42-RShiny_Debugging_Other_Useful_Things_Landing.html",
    "title": "Debugging & Other Useful Things",
    "section": "",
    "text": "The video below discusses how to debug shiny apps and some other really useful material.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Debugging & Other Useful Things"
    ]
  },
  {
    "objectID": "Notes/42-RShiny_Debugging_Other_Useful_Things_Landing.html#notes",
    "href": "Notes/42-RShiny_Debugging_Other_Useful_Things_Landing.html#notes",
    "title": "Debugging & Other Useful Things",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\napp.R file created in the video\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: R Shiny Dashboards",
      "Debugging & Other Useful Things"
    ]
  },
  {
    "objectID": "Notes/44.5-Week8.html",
    "href": "Notes/44.5-Week8.html",
    "title": "Week 8 Overview",
    "section": "",
    "text": "More coming soon!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Week 8 Overview"
    ]
  },
  {
    "objectID": "Notes/45-Prediction_Testing_Training.html",
    "href": "Notes/45-Prediction_Testing_Training.html",
    "title": "Prediction & Training/Test Sets",
    "section": "",
    "text": "In this course we focus on the predictive modeling paradigm. In the end, we’ll often fit different families of models (say some multiple linear regression models, a tree based model, and a random forest model) to a given data set. We’ll then judge those models using some model metric to determine which model is best at predicting!\nLet’s break down this process into a few steps.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Prediction & Training/Test Sets"
    ]
  },
  {
    "objectID": "Notes/45-Prediction_Testing_Training.html#predictive-modeling-idea",
    "href": "Notes/45-Prediction_Testing_Training.html#predictive-modeling-idea",
    "title": "Prediction & Training/Test Sets",
    "section": "Predictive Modeling Idea",
    "text": "Predictive Modeling Idea\nFirst we’ll choose a form for a model. The types of models to consider often depends on the subject matter at hand. In this course, we’ll cover a few general types (or families) of models.\nOnce we’ve chosen a model type, we fit the model using some algorithm. Usually, we can write this fitting process in terms of minimizing some loss function.\nWe then need to determine the quality of predictions made by the model. We use a model metric to do this. Quite often, the loss function and model metric are the same, but this isn’t always the case!\nFor numeric response, the most common model metric is mean squared error (MSE) or root mean squared error (RMSE). For a categorical reponse, the most commmon model metrics are accuracy and log-loss (discussed in detail later).",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Prediction & Training/Test Sets"
    ]
  },
  {
    "objectID": "Notes/45-Prediction_Testing_Training.html#training-vs-test-sets",
    "href": "Notes/45-Prediction_Testing_Training.html#training-vs-test-sets",
    "title": "Prediction & Training/Test Sets",
    "section": "Training vs Test Sets",
    "text": "Training vs Test Sets\nIdeally we want our model to predict well for observations it has yet to see. We want to avoid overfitting to the data we train our model on.\nThe evaluation of predictions over the observations used to fit or train the model is called the training (set) error\n\nLet yiy_i denote an observed value and ŷi\\hat{y}_i denote our prediction for that observation. If RMSE was our metric:\n\nTraining RMSE=1# of obs used to fit model∑obs used to fit model(yi−ŷi)2\\mbox{Training RMSE} = \\sqrt{\\frac{1}{\\mbox{# of obs used to fit model}}\\sum_{\\mbox{obs used to fit model}}(y_i-\\hat{y}_i)^2}\n\nIf we only consider this error, we’ll have no idea how the model will fare on data it hasn’t seen!\n\nOne method to obtain a better idea about model performance is to randomly split the data into a training set and test set.\n\nOn the training set we can fit (or train) our models\nWe can then predict for the test set observations and judge effectiveness with our metric\n\n\n\n\n\n\n\n\n\n\n\nExample of Fitting and Evaluating Models\nConsider our data set on motorcycle sale prices\n\nlibrary(tidyverse)\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv\")\nbike_data &lt;- bike_data |&gt; \n  mutate(log_selling_price = log(selling_price), \n         log_km_driven = log(km_driven)) |&gt;\n  select(log_km_driven, log_selling_price, everything())\nbike_data\n\n# A tibble: 1,061 × 9\n   log_km_driven log_selling_price name    selling_price  year seller_type owner\n           &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n 1          5.86             12.1  Royal …        175000  2019 Individual  1st …\n 2          8.64             10.7  Honda …         45000  2017 Individual  1st …\n 3          9.39             11.9  Royal …        150000  2018 Individual  1st …\n 4         10.0              11.1  Yamaha…         65000  2015 Individual  1st …\n 5          9.95              9.90 Yamaha…         20000  2011 Individual  2nd …\n 6         11.0               9.80 Honda …         18000  2010 Individual  1st …\n 7          9.74             11.3  Honda …         78500  2018 Individual  1st …\n 8         10.6              12.1  Royal …        180000  2008 Individual  2nd …\n 9         10.4              10.3  Hero H…         30000  2010 Individual  1st …\n10         10.6              10.8  Bajaj …         50000  2016 Individual  1st …\n# ℹ 1,051 more rows\n# ℹ 2 more variables: km_driven &lt;dbl&gt;, ex_showroom_price &lt;dbl&gt;\n\n\nHere our response variable is the log_selling_price = ln(selling_price). We could consider the family of multiple linear regression (MLR) models with differing predictors (x variables).\n\nThe basic MLR model with pp predictors models the average response variable given the predictors (that’s what E(Y|x1,x2,...,xp)E(Y|x_1,x_2,...,x_p) represents, the average or expected YY, given (that’s what the vertical bar means) the values of x1x_1, x2x_2, …, xpx_p) as a linear function (linear in the parameter terms).\n\nE(Y|x1,x2,...,xp)=β0+β1x1+β2x2+...+βpxpE(Y|x_1, x_2, ..., x_p) = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_px_p\n\nβ0\\beta_0 is the intercept and each βj\\beta_j is a slope term associated with a predictor variable.\nAs with SLR, we could model the error term with a Normal distribution if we were interested in making inference on the parameters\n\nThese basic models find coefficients that minimize the sum of squared residuals (i.e. uses squared error loss to fit the model).\n\nMLR modeling fitting criterion (the ‘hats’ just imply the parameters are estimates rather than the ‘true’ underlying values):\n\nminβ̂′s∑i=1n(yi−(β̂0+β̂1x1i+...+β̂pxpi))2min_{\\hat{\\beta}'s}\\sum_{i=1}^{n}(y_i-(\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+...+\\hat{\\beta}_px_{pi}))^2\nThis turns out to be equivalent to doing maximum likelihood estimation with the iid error Normal, constant variance, assumption!\n\nConsider three competing MLR models:\n\nModel 1: log_selling_price = intercept + slope*year + Error\\mbox{Model 1: log_selling_price = intercept + slope*year + Error}\nModel 2: log_selling_price = intercept + slope*log_km_driven + Error\\mbox{Model 2: log_selling_price = intercept + slope*log_km_driven + Error}\nModel 3: log_selling_price = intercept + slope*log_km_driven + slope*year + Error\\mbox{Model 3: log_selling_price = intercept + slope*log_km_driven + slope*year + Error}\n\n\nWe can split the data randomly into a training set and a testing set. There are a lot of ways to do this. We’ll use the rsample::initial_split() function.\n\nWe commonly use an 80/20 or 70/30 training/test split. The proportion used in this split really depends on the amount of data you have and your subject matter expertise. More data in the test set means a better estimate of the model’s performance. However, less data in the training set means a more variable model (bigger changes in predictions from data set to data set).\nLet’s split our bike_data into a training and test set.\n\nUse initial_split() to create an initial object\nUse training() and testing() on that object to create the two data sets (note the number of observations in each set below!)\n\n\n\nlibrary(tidymodels)\nset.seed(10)\nbike_split &lt;- initial_split(bike_data, prop = 0.7)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_train\n\n# A tibble: 742 × 9\n   log_km_driven log_selling_price name    selling_price  year seller_type owner\n           &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n 1         10.8              10.3  Bajaj …         30000  2012 Individual  1st …\n 2          8.95             10.6  Honda …         40000  2015 Individual  1st …\n 3          9.99              9.80 Bajaj …         18000  2005 Individual  1st …\n 4         10.2              10.5  Hero H…         35000  2017 Individual  1st …\n 5         10.8              11.4  Royal …         85000  2013 Individual  1st …\n 6          9.88             10.3  Bajaj …         30000  2008 Individual  1st …\n 7         10.5              10.5  Hero C…         35000  2014 Individual  1st …\n 8          9.68              9.90 Bajaj …         20000  2009 Individual  1st …\n 9         11.1              10.1  Hero H…         25000  2008 Individual  3rd …\n10          8.94             12.2  Bajaj …        200000  2019 Individual  1st …\n# ℹ 732 more rows\n# ℹ 2 more variables: km_driven &lt;dbl&gt;, ex_showroom_price &lt;dbl&gt;\n\nbike_test\n\n# A tibble: 319 × 9\n   log_km_driven log_selling_price name    selling_price  year seller_type owner\n           &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n 1          8.64             10.7  Honda …         45000  2017 Individual  1st …\n 2          9.39             11.9  Royal …        150000  2018 Individual  1st …\n 3          7.03             12.8  Yamaha…        365000  2019 Individual  1st …\n 4          7.44             12.1  Jawa 42        185000  2020 Individual  1st …\n 5         10.9              10.1  Suzuki…         25000  2012 Individual  1st …\n 6         11.0               9.62 Hero P…         15000  2008 Individual  1st …\n 7          7.60             12.1  Jawa S…        180000  2019 Individual  1st …\n 8         10.1              10.6  Honda …         42000  2017 Individual  1st …\n 9          9.21              9.95 Hero H…         21000  2009 Individual  1st …\n10          9.95             10.7  Hero G…         45000  2018 Individual  1st …\n# ℹ 309 more rows\n# ℹ 2 more variables: km_driven &lt;dbl&gt;, ex_showroom_price &lt;dbl&gt;\n\n\nWe can fit or train these models on the training set. Recall we use lm() to easily fit an MLR model via formula notation. With formula notation we put our response variable on the left and our model for the predictors on the right. The model can include interactions, non-linear terms, etc. If we just want ‘main effects’ we separate predictors with + on the right hand side (we’ll cover this in more detail later!)\n\nLet’s fit our three models and save them as objects.\n\n\nreg1 &lt;- lm(log_selling_price ~ year, data = bike_train)\ncoef(reg1)\n\n  (Intercept)          year \n-186.17235057    0.09777273 \n\nreg2 &lt;- lm(log_selling_price ~ log_km_driven, data = bike_train)\ncoef(reg2)\n\n  (Intercept) log_km_driven \n   14.6228627    -0.3899342 \n\nreg3 &lt;- lm(log_selling_price ~ year + log_km_driven, data = bike_train)\ncoef(reg3)\n\n  (Intercept)          year log_km_driven \n-131.66741960    0.07191291   -0.24274055 \n\n\nNow we have the fitted models. Want to use them to predict the response\n\nModel 1: log_selling_pricê=−186.1724+0.0978*year\\mbox{Model 1: } \\widehat{\\mbox{log_selling_price}} = -186.1724 + 0.0978*\\mbox{year}\nModel 2: log_selling_pricê=14.6229−0.3899*log_km_driven\\mbox{Model 2: } \\widehat{\\mbox{log_selling_price}} = 14.6229 -0.3899*\\mbox{log_km_driven}\nModel 3: log_selling_pricê=−131.6674+0.0719*year−0.2427*log_km_driven\\mbox{Model 3: } \\widehat{\\mbox{log_selling_price}} = -131.6674 + 0.0719*\\mbox{year}-0.2427*\\mbox{log_km_driven}\n\nTo get predictions from our model, we use the predict() function and specify the newdata we want to predict for as a data frame with column names matching our predictors in our models. If we don’t specify any newdata, it returns the predictions made on the training data.\n\nWe can see the first few predictions on the training data from the first model via the code below\n\n\n#year values the predictions are for\nbike_train$year |&gt; head()\n\n[1] 2012 2015 2005 2017 2013 2008\n\n\n\n#predictions made with our first model\npredict(reg1) |&gt; head()\n\n        1         2         3         4         5         6 \n10.546377 10.839696  9.861968 11.035241 10.644150 10.155287 \n\n\nLet’s use RMSE as our metric. Although not how we want to compare our models, we can obtain the training RMSE easily with predict(). Let’s code it up ourselves and also yardstick::rmse_vec() to find it (this is the tidymodels way).\n\n#our own calculation for training RMSE\nsqrt(mean((bike_train$log_selling_price - predict(reg1))^2))\n\n[1] 0.5378694\n\n\n\nNow we can supply the actual responses and the model predictions to yardstick::rmse_vec()\n\n\n#using yardstick\nyardstick::rmse_vec(bike_train$log_selling_price, predict(reg1))\n\n[1] 0.5378694\n\n#second and third models\n#using yardstick\nrmse_vec(bike_train$log_selling_price, predict(reg2))\n\n[1] 0.5669127\n\n#using yardstick\nrmse_vec(bike_train$log_selling_price, predict(reg3))\n\n[1] 0.4924924\n\n\n\nThese values represent a measure of quality of prediction by these models (as judged by our metric RMSE).\nThis estimate of RMSE for the predictions is too optimistic compared to how the model would perform with new data!\nReally, what we want to compare is how the models do on data they weren’t trained on.\nWe want to find this type of metric on the test set. That means we want to use the truth from the test set (yiy_i for the test set) and compare that to predictions made for that test set observation (ŷi\\hat{y}_i).\n\nTest Set RMSE=1ntest∑i=1ntest(yi−ŷi)2\\mbox{Test Set RMSE} = \\sqrt{\\frac{1}{n_{test}}\\sum_{i=1}^{n_{test}}(y_i-\\hat{y}_i)^2}\nTo do this in R we need to tell predict() about the newdata being the test set (bike_test). As this is a data frame with columns for our predictors, we can just pass the entire data frame for ease and predict() uses appropriate/needed columns to find our predictions.\n\n#look at a few observations and predictions\nbike_test |&gt;\n  select(log_km_driven, log_selling_price) |&gt;\n  mutate(model_1_preds = predict(reg1, newdata = bike_test))\n\n# A tibble: 319 × 3\n   log_km_driven log_selling_price model_1_preds\n           &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1          8.64             10.7           11.0\n 2          9.39             11.9           11.1\n 3          7.03             12.8           11.2\n 4          7.44             12.1           11.3\n 5         10.9              10.1           10.5\n 6         11.0               9.62          10.2\n 7          7.60             12.1           11.2\n 8         10.1              10.6           11.0\n 9          9.21              9.95          10.3\n10          9.95             10.7           11.1\n# ℹ 309 more rows\n\n\n\nNow let’s find the test set error for each model\n\n\n#obtain the test set RMSE for each model\nrmse_vec(bike_test$log_selling_price, predict(reg1, newdata = bike_test))\n\n[1] 0.5746992\n\nrmse_vec(bike_test$log_selling_price, predict(reg2, newdata = bike_test))\n\n[1] 0.6548019\n\nrmse_vec(bike_test$log_selling_price, predict(reg3, newdata = bike_test))\n\n[1] 0.554596\n\n\nWe see that our third model with both year and log_km_driven gives a better (by our metric) set of predictions!\nWhen choosing a model, if the RMSE values were ‘close’, we’d want to consider the interpretability of the model (and perhaps the assumptions required by each model if we wanted to do inference too!)\n\nNote: we can do this with yardstick::rmse() if our predictions are in the data frame. rmse() takes in the data as the first argument, the truth column as the second, and the estimate (or predictions) as the third.\n\n\n#look at a few observations and predictions\nbike_test |&gt;\n  select(log_selling_price, log_km_driven) |&gt;\n  mutate(model_1_preds = predict(reg1, newdata = bike_test)) |&gt;\n  rmse(truth = log_selling_price, estimate = model_1_preds)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.575\n\n\n\n\nRecap\nWe generally need to go through a few steps when training and testing our model(s):\n\nChoose form of model\nFit model to data using some algorithm\n\nUsually can be written as a problem where we minimize some loss function\n\nEvaluate the model using a metric\n\nRMSE very common for a numeric response\n\nIdeally we want our model to predict well for observations it has yet to see!\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Prediction & Training/Test Sets"
    ]
  },
  {
    "objectID": "Notes/47-Multiple_Linear_Regression_Landing.html",
    "href": "Notes/47-Multiple_Linear_Regression_Landing.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The video below discusses the widely used multiple linear regression model.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "Notes/47-Multiple_Linear_Regression_Landing.html#notes",
    "href": "Notes/47-Multiple_Linear_Regression_Landing.html#notes",
    "title": "Multiple Linear Regression",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "Notes/49-tidymodels_Tutorial.html",
    "href": "Notes/49-tidymodels_Tutorial.html",
    "title": "tidymodels Tutorial",
    "section": "",
    "text": "Now that we have the basics of how the tidymodels package works, let’s work through the tidymodels.org tutorial.\nPlease read through the five part tutorial they have to introduce people to tidymodels.\nThen head back here for the next content!\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "`tidymodels` Tutorial"
    ]
  },
  {
    "objectID": "Notes/51-Modeling_Recap_Landing.html",
    "href": "Notes/51-Modeling_Recap_Landing.html",
    "title": "Modeling Recap",
    "section": "",
    "text": "The video below recaps the predictive modeling paradigm. This is a pretty big concept so it never hurts to focus on the big picture ideas again!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Modeling Recap"
    ]
  },
  {
    "objectID": "Notes/51-Modeling_Recap_Landing.html#notes",
    "href": "Notes/51-Modeling_Recap_Landing.html#notes",
    "title": "Modeling Recap",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Modeling Recap"
    ]
  },
  {
    "objectID": "Notes/52-Logistic_Regression_Models_Landing.html",
    "href": "Notes/52-Logistic_Regression_Models_Landing.html",
    "title": "Logistic Regression Models",
    "section": "",
    "text": "The video below introduces the logistic regression model. This model is very useful for modeling binary responses (there are extensions to response variables with more than two values).\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Logistic Regression Models"
    ]
  },
  {
    "objectID": "Notes/52-Logistic_Regression_Models_Landing.html#notes",
    "href": "Notes/52-Logistic_Regression_Models_Landing.html#notes",
    "title": "Logistic Regression Models",
    "section": "Notes",
    "text": "Notes\nNote: The code on slide 29 of the video differs slightly from the code in the notes below. An older version of the tidymodels package was used in the video and that code no longer works. The code in the slides attached is the way to do this now.\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Logistic Regression Models"
    ]
  },
  {
    "objectID": "Notes/54-Ensemble_Trees_Bagging_Random_Forests_Landing.html",
    "href": "Notes/54-Ensemble_Trees_Bagging_Random_Forests_Landing.html",
    "title": "Ensemble Trees",
    "section": "",
    "text": "The video below discusses two ensemble tree methods: bagging and random forests. These models fit many trees using bootstrap resamples and then combines the predictions in some way.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Ensemble Trees"
    ]
  },
  {
    "objectID": "Notes/54-Ensemble_Trees_Bagging_Random_Forests_Landing.html#notes",
    "href": "Notes/54-Ensemble_Trees_Bagging_Random_Forests_Landing.html#notes",
    "title": "Ensemble Trees",
    "section": "Notes",
    "text": "Notes\nNote: The code on slide 21 (same issues as before) and slides 19 & 31 (added metric =) of the video differs slightly from the code in the notes below. An older version of the tidymodels package was used in the video and that code no longer works. The code in the slides attached is the way to do this now.\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Predictive Modeling",
      "Ensemble Trees"
    ]
  },
  {
    "objectID": "Notes/55-Creating_API_Landing.html",
    "href": "Notes/55-Creating_API_Landing.html",
    "title": "Creating an API in R",
    "section": "",
    "text": "The video below discusses how to create your own API in R using the plumber package!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Creating an API in `R`"
    ]
  },
  {
    "objectID": "Notes/55-Creating_API_Landing.html#notes",
    "href": "Notes/55-Creating_API_Landing.html#notes",
    "title": "Creating an API in R",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\nmyAPI.R file\nrunAPI.R file\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Creating an API in `R`"
    ]
  },
  {
    "objectID": "Notes/57-Building_Docker_Landing.html",
    "href": "Notes/57-Building_Docker_Landing.html",
    "title": "Building a Docker Image",
    "section": "",
    "text": "The video below looks at how to build a basic Docker image that will run our API.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Building a Docker Image"
    ]
  },
  {
    "objectID": "Notes/57-Building_Docker_Landing.html#notes",
    "href": "Notes/57-Building_Docker_Landing.html#notes",
    "title": "Building a Docker Image",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\nmyAPI.R file\nDockerfile\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 6: APIs & Docker",
      "Building a Docker Image"
    ]
  }
]